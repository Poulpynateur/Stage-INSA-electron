[{"authors":"Shannon Hall, Nature magazine","pub_date":"June 12, 2019","abstract":"The helicopter fell like a stone. It dropped by more than 1,500 metres over Maryland, twisting slightly as the ground grew rapidly closer. Although this was all according to plan, that didn’t settle James Garvin’s nerves. Nor did the realization that his seat belt wasn’t fully fastened—a moment that sent his heart rate skyrocketing.\n\nThen, a mere 6 metres above the ground, the ride got even wilder when the pilots pulled the aircraft out of the fall and climbed skywards, only to fall again. The helicopter dropped ten times that day. And each time, Garvin pointed a camera towards the ground through the open door in an attempt to measure the topography of a rock quarry below—from massive boulders to smooth sheets of sand. Although his interests were hardly terrestrial.\n\nGarvin, the chief scientist at NASA’s Goddard Space Flight Center in Greenbelt, Maryland, is the principal investigator on a proposed mission to Venus that would drop a probe through its atmosphere. That’s why he hired two pilots in August 2016 to plunge a helicopter towards the ground while he tested what a Venus probe might be able to photograph. The harrowing ride was worth it: researchers would love to get their hands on pictures of Venus with so much detail that the scenery would become familiar. “These images would be like you landing in your backyard,” he says.\n\nGarvin is not the only scientist preparing such a daring mission. Nearly every space agency around the globe is currently sketching a proposal to explore our long-neglected neighbour. The Indian Space Research Organisation (ISRO) will be first to lift off when it launches an orbiter to Venus in 2023. The United States could follow close behind. Garvin and his colleagues are one of a handful of groups that will soon propose missions to NASA that, if selected, would take off in 2025. The European Space Agency (ESA) is currently considering a proposal to send an orbiter to Venus in 2032. And the Russian space agency Roscosmos is working in collaboration with the United States to send a daring mission to the planet any time from 2026 to 2033, which would include an orbiter, a lander that would send back short-term readings and a research station that would survive for much longer.\n\nThe new-found interest stands in stark contrast to the fact that nations have long overlooked Venus in favour of chasing Mars, asteroids and other planets. Over the past 65 years, for example, NASA has sent 11 orbiters and 8 landers to Mars, but just 2 orbiters to Venus—and none since 1994. This has not been for lack of scientific interest. Since the mid-1990s, U.S. scientists alone have submitted nearly 30 Venus proposals to NASA. None has been approved.\n\nBut momentum is building to explore Venus, in part because scientists say it could hold the secret to understanding what makes a planet habitable. Once Earth’s twin, today Venus is a hellish abode where surface temperatures reach more than 400 °C, atmospheric pressures slam down with enough force to crush heavy machinery and clouds of sulfuric acid blow through the sky. If researchers could decipher why conditions on Venus turned so deadly, that would help them to assess whether life might exist on some of the thousand-plus rocky worlds that astronomers are discovering throughout the Galaxy.\n\nAs the scientific justification has grown for exploring Venus, planetary scientists are dreaming up new ways to study the planet and are building technology in the laboratory that can survive the horrendous conditions on its surface. And with India leading the way, there might soon be a parade of probes heading towards the second rock from the Sun.\n\n“It might be the start of a new decade of Venus,” says Thomas Widemann, a planetary scientist at the Paris Observatory.\n\nDouble trouble\n\nWhen humanity initially reached towards the stars, it ventured to Venus. Our neighbour was the target of the first successful interplanetary probe (United States, 1962); the first planet on which a mission crashed (Soviet Union, 1965); and the first alien world to host a successful landing (Soviet Union, 1970). It was during this space race to Venus that scientists discovered a torrid and toxic world. That could explain why interest in Venus dwindled. Scientists quickly realized that this planet would not be a home for future human exploration, nor an outlet on which to search for life. It would be downright difficult to study at all, even for short amounts of time.\n\nAnd yet in so many ways—size, density, chemical make-up—Venus is Earth’s double. Recent research has even suggested that it might have looked like Earth for three billion years, with vast oceans that could have been friendly to life. “That’s what sets my imagination on fire,” says Darby Dyar, a planetary scientist at Mount Holyoke College in South Hadley, Massachusetts. “If that’s the case, there was plenty of time for evolution to kick into action.”\n\nThat could mean that Venus was (somewhat surprisingly) the first habitable planet in the Solar System—a place where life was just as likely to arise as it was on Earth. That alone is a reason to return to the former ocean world. “Why are we investing so much time looking for life on Mars when it only had liquid water for 400 million years?” Dyar asks. “And then there’s Venus with three billion years of water and no one loves her.”\n\nYet there’s no question that something went terribly wrong. Although Earth and Venus began in a similar fashion, the two have wandered down drastically different evolutionary paths—diverging perhaps as recently as 715 million years ago. That might seem like a reason not to visit, but scientists now argue that it makes the planet even more intriguing. If researchers could only understand what caused Venus to undergo such a deadly metamorphosis, they might gain a better understanding of what caused Earth to become such a haven for life.\n\n“Venus plays a key role in understanding ourselves here—how life evolved on our own planet,” says Adriana Ocampo, science programme manager at NASA headquarters in Washington DC.\n\nIt is a crucial question now that astronomers have uncovered thousands of planets outside our Solar System—many of which are rocky worlds that orbit their stars at distances similar to those of Venus and Earth from the Sun. That means that many of these worlds could be Venus-like. “There is growing realization within the exoplanet community that Venus is the best analogue in the Solar System for many of the rocky exoplanets we have found,” says Laura Schaefer, an astronomer at Stanford University in California who studies exoplanets.\n\nOff the radar\n\nWith such a tantalizing question left unanswered, it’s easy to see why ISRO’s return to Venus has created so much excitement. “I’m thrilled that ISRO is doing this,” Dyar says. “I’m thrilled that the international community is taking note of Venus and proposing missions. That’s fantastic.”\n\nAlthough the ISRO mission is enveloped in a cloud of secrecy (Nature e-mailed and called project scientists dozens of times, to no avail), it’s clear that the agency plans to send an orbiter smothered in instruments. When ISRO announced the mission late last year, it published a list of a dozen instruments proposed by Indian scientists that have already been chosen—providing a sneak peek of the mission. Of those sensors, two will map the planet using radar, which is arguably the best method to peer through Venus’s dense clouds and trace its surface from orbit.\n\nThat said, ISRO is a relatively young space agency with a limited number of successful landings on the Moon and Mars. And, similar to programmes from other fledgling agencies, India’s first Venus mission might be a proof of concept that is less focused on science than on engineering. But given that even basic information on Venus is lacking, any small step will contribute to science.\n\nOne such contribution might be new maps of Venus’s surface features—a major step up, scientifically. The last mission to map the planet’s topography was NASA’s Magellan orbiter, which launched 30 years ago. Although those radar maps remain the foundation of Venusian geoscience today, they show topographic details at a horizontal resolution of just 10–20 kilometres per pixel, on average (the image resolution can be two orders of magnitude higher). With such limited topographic data, researchers have a blurry view of Venus’s geology—but the available maps do hint that plate tectonics might be kicking into action today.\n\nThat is particularly tantalizing, because many scientists think that tectonic activity is a crucial ingredient for life. Tectonic plates—those interlocking slabs of Earth’s crust that fit together like puzzle pieces—constantly move about, with some slipping below others and diving into the planet’s interior in a process called subduction. Over millions of years, that process has kept Earth from growing too hot or cold by cycling heat-trapping carbon dioxide between the atmosphere and the deep Earth. It acts as a natural thermostat, which might mean that fidgety planets are more likely to host life.\n\nAs such, scientists are eager to decipher the conditions that allow plate tectonics to arise. That is why Suzanne Smrekar, a planetary scientist at NASA’s Jet Propulsion Laboratory in Pasadena, California, has her eye on Venus—especially some spots that look eerily similar to locations on Earth where subduction is happening now. Scientists agree that subduction is the first step in the path towards plate tectonics, and yet there are no clear signs of large moving plates on Venus—at least not in the decades-old maps produced by Magellan. The San Andreas fault, which forms the tectonic boundary between Earth’s Pacific Plate and North American Plate, for example, varies in width from metres to a kilometre—too narrow to show up in Magellan topographic data.\n\nBut future maps might uncloak such tectonic features. Smrekar is the principal investigator on a potential mission, known as VERITAS, that she and her team will soon propose to NASA. The geophysical mission would use radar to map Venus’s topography in higher resolution than before—increasing the accuracy from roughly 15 kilometres to 250 metres—and allowing scientists to uncover features as small as the San Andreas fault for the first time.\n\nAlthough scientists don’t know what they will find, it is possible that they will uncover evidence for past plate tectonics. Such a discovery would explain why Venus preserved an Earth-like environment for billions of years, Smrekar says—that natural thermostat would have kept CO2 in check. And it would explain how Venus turned hellish. When plate tectonics ceased, CO2 levels would have increased in the atmosphere and trapped so much heat that the oceans vaporized.\n\nBut that is only one possible finding. Some scientists are keen to study the planet’s atmosphere, which holds another, equally tantalizing set of secrets.\n\nThe probe that Garvin is proposing, called DAVINCI, would drop through the atmosphere to measure the brew of toxic compounds. The isotopes of noble gases, particularly xenon, could give scientists a window into the planet’s volcanic history and reveal whether Venus started with as much water as Earth did. “Venus’s atmosphere is this lurking laboratory for telling us about its history,” Garvin says. “And really, most of the measurements in the atmosphere are woefully incomplete.” In addition, the probe would take images of the surface—thanks to Garvin’s terrifying helicopter flights—until the last few seconds before it hits.\n\nBoth VERITAS and DAVINCI will enter NASA’s competition on 1 July for future Discovery missions—a line of low-cost planetary probes that each cost just US$500 million. And rumour has it they’re not alone. There could be as many as five Venus missions (including a balloon) among the dozens of proposals to study various objects in space. NASA’s last Discovery competition, in 2015, for example, considered 27 proposals—from probes that would explore asteroids, moons and planets across the Solar System to telescopes that would image its outer reaches—before choosing two missions that would fly.\n\nAt the end of this year, the administration will select a few missions for further study, and it will pick the final project in two years’ time. Both Smrekar and Garvin are hopeful that each of their missions will be selected, in part because they proposed similar missions in the last Discovery competition, and both were chosen for further study, along with three others. If one of the Venus missions is successful, it will launch in the mid-2020s.\n\nEven after that time frame, Venus might remain a hub for interplanetary activity. ESA recently picked a Venus probe called EnVision, along with two other finalists, as a mission that could fly as soon as 2032. Like VERITAS, EnVision is an orbiter. But unlike VERITAS, which would map the entire planet to a resolution of 15–30 metres, EnVision will analyse small portions of the planet with a resolution as high as 1 metre. At that level of accuracy, scientists might be able to see the landers that the Soviet Union left behind.\n\nThey could even pick out the type of rock that the landers are resting on. This is possible because astronomers in the early 1990s found that certain wavelengths of light can pass through the CO2 haze that hides the Venusian surface. An orbiter carrying a spectrometer tuned to these transparent ‘windows’ in the light spectrum could analyse the composition of the planet’s surface from above the clouds. That’s an exciting prospect, especially if scientists could spot granite.\n\nLike basalt, granite forms when molten magma cools and hardens. But unlike basalt, the recipe for granite typically requires copious amounts of water—which happens on Earth when water-rich oceanic crust subducts below another plate. So if Venus is found to be rich in granite, it probably once overflowed with liquid water.\n\nAnd that might be the best hint yet that the planet was formerly a pale blue dot vastly similar to Earth today—another clue in their diverging stories.\n\nThe problem is that there are only five narrow spectral windows in Venus’s atmosphere that are actually transparent. With such little information, scientists weren’t sure whether they would be able to differentiate between granite and basalt. So Jörn Helbert, a planetary scientist at the Institute of Planetary Research in Berlin, subjected both types of rock to Venus-like conditions and imaged them through those narrow frequency bands. His experiment suggested that the two rock spectra look radically different from one another, and that future missions could make use of the windows. He and his colleagues built an instrument to use this trick to map any granite on the Venusian surface. It would fly on both VERITAS and EnVision.\n\nWithin reach\n\nTo truly understand the surface, a number of scientists want to actually land a craft on our toxic twin—a feat that has not been achieved for 35 years. Although the Soviet Union sent several landers to Venus, the ones that survived quickly succumbed to the planet’s harsh environment: the longest-lasting one persevered for a mere 127 minutes.\n\nBut scientists hope to break that record, and have already designed technology that can last not just minutes, but months. A team at NASA’s Glenn Research Center in Cleveland, Ohio, is building a station that should survive for at least 60 days. Instead of using its bulk to absorb heat or countering the conditions with refrigeration, the lander would use simple electronics made of silicon carbide (a hybrid of silicon and carbon commonly used in sandpaper and fake diamonds) that can withstand the Venusian environment. “That’s the real game changer for Venus exploration,” says Philip Neudeck, an electronics engineer at the Glenn Research Center.\n\nThe team has already tested the circuits in a Venus simulation chamber—a 14-tonne stainless-steel tank that can imitate the temperature, pressure and specific chemistry of the Venusian surface. The researchers have used those results to design a stationary surface probe called LLISSE (Long-Lived In-Situ Solar System Explorer), which should be ready for flight by the mid-2020s and will be offered to other countries. “Any mission to Venus is welcome to use LLISSE,” says electronics engineer Gary Hunter, also at the Glenn Research Center. He and the team were careful to design a lander that would be only as large as a toaster—making it both small and light enough that it can hitch a ride on a number of future missions.\n\nDespite its small size, LLISSE would be able to record temperature, pressure, wind speed, wind direction, the amount of solar energy at the surface and a few specific chemicals in the low atmosphere. And it would do so for months, providing crucial input for models of the Venusian atmosphere. “Imagine if one tried to say one knew the weather on Earth by going outside for 127 minutes,” Hunter says. That is the current record for any weather data on Venus.\n\nAlready, scientists at Roscosmos are eager to use this new technology. In a joint proposal with NASA, they are working on a mission known as Venera-Dolgozhivuschaya (where the latter means long-lasting), or Venera-D for short. Such a mission would comprise a menagerie of components—an orbiter, a lander and a long-lived station. The lander would include a number of advanced instruments but would last for only a few hours; the long-lived station would be simpler in design but continue taking measurements for months. The station is likely to be NASA’s LLISSE.\n\nAt least, that’s the baseline architecture—but the mission could include even more. This year, the Venera-D team released a report that covered a number of potential additions, including a balloon that could explore the cloudy atmosphere. And that opens up the possibility of searching for life on Venus. All the other missions proposed so far aim to assess whether Venus was habitable in the past. But a balloon might be able to look for life in the only environment where it might survive today: the skies.\n\n“You can imagine that there’s somewhere in between the hot hostile surface and the cold vacuum of outer space where there are conditions—like Goldilocks’—that are just right for life,” Dyar says. Not only would that layer have a pleasant temperature, but it could also have nutrients, liquid water and energy from the Sun. If life ever existed on the planet, it might have been carried up to the clouds and survived there after the surface turned toxic.\n\nBut even without a balloon, the three main components of the Venera-D mission would provide excellent science, argues Ocampo. “It would be a breakthrough mission in the understanding of Venusian science,” she says. “We haven’t had a mission similar to this before.”\n\nUnfortunately, Venera-D has not yet been selected, and many scientists have expressed some concern over the fact that it has long been discussed and yet still does not have the appropriate funding. But Ludmila Zasova, the lead scientist on the Venera-D mission at the Space Research Institute in Moscow, hopes that might change this year.\n\nIt’s not the only big ambitious mission in the works. Some US teams plan to submit Venus projects to NASA’s New Frontiers programme, which is capped at $1 billion, and to the Flagship mission programme, which costs even more. Because Venus proposals have done well in past competitions (often falling just behind the selected proposals), scientists think there is a good chance that they will now rise to the top.\n\nWith every space agency eyeing our neighbour, Venus is likely to receive a fleet of visitors over the next few decades. And although they all plan to address the question of habitability in one way or another, Garvin is convinced that whatever they find, it will be “beyond our wildest dreams”. Perhaps they will prove that Venus was formerly an ocean world. Or maybe they’ll discover that it’s tectonically active today. “We need to find out,” he says. “Because she’s waiting to tell us something and I would hate to miss the boat.”\n\nThis article is reproduced with permission and was first published on June 5, 2019.","title":"Venus, Earth’s Evil Twin, Beckons Space Agencies","origin":"Space","image":"https://static.scientificamerican.com/sciam/cache/file/BB10F71B-933E-4506-AA7E2EF71A62E9D3_source.jpg?w=590&h=800&20A4CDBB-CE37-46C6-AE226EFA35678EF5","link":"https://www.scientificamerican.com/article/venus-earths-evil-twin-beckons-space-agencies/"},{"authors":"Barry Nerhus","pub_date":"June 12, 2019","abstract":"Save the whales. The polar bears. The honeybees. You’ve undoubtedly encountered one or all of these conservation campaigns over the course of your lifetime. You’ll probably encounter many more as our earth’s wildlife continues to hurdle towards mass extinction at an alarming rate. Approximately one eighth of our planet’s plant and animal species are currently at risk. What may seem like a small fraction on its own actually works out to around one million endangered or vulnerable species when extrapolated. That’s no small figure.\n\nSomething must be done! That’s what you're supposed to say after reading a statistic like the one above. Did it spur you to action, or did it merely evoke a feeling of slight disappointment? Perhaps you, like many others, believe that something is already being done. If so, you’re right. Something is already being done—but it’s not enough. Current conservation efforts are no match for the rapid rate of habitat deterioration plaguing our ecosystems. Only partnerships with community-based conservation are capable of inciting the necessary amount of change in the recommended amount of time. \n\nGoverning bodies have done a great job drafting and implementing legislation designed to preserve critical habitats and endangered species. There are at least seven major conservation laws regulating the U.S. alone. Process typically slows progress, however, and there is plenty of red tape between written legislation and actual restoration. Even with policies in place, sometimes we’re too late. Add to that the fact that regulations are only as effective as the individuals that know and follow them, and it’s easy to see how our efforts are minimal in the grand scheme of things. \n\nThere are five major obstacles impeding the progress of most major conservation efforts today: \n\n\n\tLack of enforcement\n\tUnclear goals\n\tUnderfunding\n\tGeneralized strategy\n\tCommunity exclusion, ignorance, or apathy\n\n\nWhile each of these obstacles is significant in its own right, community exclusion is the only one with the potential to minimize the impact of the others when corrected. Including communities in current restoration practices and involving them in future ones instills a certain sense of responsibility and accountability, both of which are crucial to effective habit restoration strategies.\n\nCommunity-based restoration is, essentially, a sustainable conservation program. Rather than relying on present-day law and public opinion to assign importance, it incorporates the value of conservation efforts into the backbone of a community.  When executed correctly, habitat restoration becomes a critical consideration of the legal, academic and economic infrastructures in a given location.  Let’s take a look at what community involvement in each of these areas might look like.\n\nLegal. Federal legislation can only do so much when it comes to environmental conservation and restoration. Overarching policies provide a nice framework, yes, but they fail to consider the nuances and public sentiment of a given location. When legislation becomes local, however, communities are able to provide critical insight that may otherwise be overlooked. Local citizens tend to know more about which area practices might be hindering progress, who is involved in those practices and which alternatives might provide a viable solution. \n\nFurthermore, individuals are far more likely to be aware of the rules policing their own community. It’s difficult for a federal agency to enforce national acts at the municipal level, so it’s of the utmost importance that communities make local enforcement a priority. Some habitats require public assistance in order to be considered fully restored, while others must be left alone entirely. Regulations drafted and enforced at the grassroots level ensures that each diverse habitat in the community receives the proper level of care. \n\nAcademic. One of the most important factors influencing the success of a community-based restoration program is academic involvement. Numerous colleges offer ecology and conservation programs as a part of their curriculum, including the University of California, Irvine, where I teach courses essential to the Masters in Conservation and Restoration Science program; but the important information is limited to the number of students signed up to take such courses. To speed up habitat restoration, it is essential that conservation education moves beyond the walls of higher education and into the syllabi of primary and secondary schools. \n\nYoung people are the future stewards of the earth, so it’s only logical that they understand the current state of their environment as well as the impact of their actions upon it. When educators emphasize the importance of community ecology at an early age, habitat restoration becomes less of a burden and more of an ingrained attitude. \n\nEconomic. Money is often cited as a chief impediment of habitat restoration efforts. Lack of funds translates to lack of action in most cases, but it doesn’t have to. Changing the financial mindset of a community can actually help stimulate conservation economy. Say, for example, that an environment focuses on providing attractive wages and incentives for conservation-related positions within their borders. Not only is such a community encouraging ecologically responsible citizens to become part of its population, but it is putting money into the hands of the individuals that are most likely to do good with it. \n\nUrging local institutions to clearly outline conservation career paths at the educational and professional levels serves a similar purpose. The educational programs attract eco-smart students to the community and the professional pathways offer opportunities for them to stay. The more a community invests in habitat restoration personnel and advocates, the more citizens it acquires who are willing to invest in the community in return. \n\nHabitat restoration is not a fast process, nor will it ever be. We are capable of destroying in minutes what it took nature eons to create. If we’re going to succeed in reversing the damage, if we’re going to truly make a difference, we must move restoration out of the hands of individuals and into the hearts of communities.","title":"Habitat Restoration Isn't Just for Professionals","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/1ABC1BDC-C04B-4AAA-803041FC1DF59FD0_source.jpg?w=590&h=800&904E6AEE-6317-45C9-A4BE7F12856EA9EF","link":"https://blogs.scientificamerican.com/observations/habitat-restoration-isnt-just-for-professionals/"},{"authors":"Steve Mirsky","pub_date":"June 12, 2019","abstract":"At the third Scientific American “Science Meets Congress” event, “Solving the Plastic Waste Problem”, experts examined the question of biodegradability.","title":"A Biodegradable Label Doesn't Make It So","origin":"Environment 60-Second ScienceSubscribe:Apple iTunesRSS","link":"https://www.scientificamerican.com/podcast/episode/a-biodegradable-label-doesnt-make-it-so/"},{"authors":"Emily Willingham","pub_date":"June 12, 2019","abstract":"A repeated warning about cannabis use in the post-hippie era is to be aware that what you’re smoking or ingesting isn’t your grandparents’ cannabis. Humans have worked hard in recent decades to produce strains with powerful doses of tetrahydrocannabinol, or THC, the psychoactive component of the plant. The result is a surprisingly intense high that famously sent New York Times columnist Maureen Dowd into a spiral of paranoia after she ingested a cannabis-laced chocolate edible one fateful evening in Colorado.\n\nA superstrong strain of pot might overpower unwitting baby boomers, but all of this might just be a case of déjà vu. Today’s herb could be somewhat like cannabis that people cultivated about 2,500 years ago in Central Asia, a plant also bearing high levels of THC.  Research published June 12 in Science Advances offers the first evidence that humans around that time not only used cannabis for the high it offers but also selected strains for their THC-packed psychoactive power, burning them in mortuary rituals. The find represents some of the oldest documented use of cannabis for its mind-altering effects. “I am impressed by this new and important discovery,” says Tengwen Long, an assistant professor and environmental archaeologist at the University of Nottingham, Nimbo campus, who was not involved in the study. “It offers us very important data concerning humans’ close interaction with the plant.”\n\nThis earliest example of humans burning cannabis in a purposeful way was uncovered as researchers worked on solving the mystery of ancient wooden burners found at a cemetery site in the Pamir mountain range in eastern China. The burners, dating to about 500 years B.C., were charred. Samples of the char revealed compounds that develop when people living in the region burned cannabis.  Furthermore, the estimated levels of THC in these samples were higher than wild cannabis would normally produce, suggesting intentional cultivation of the plant for its psychoactive powers.\n\nSuch a discovery adds to growing indications of an association among many cultures of cannabis with the afterworld and death, says Mark Merlin, professor of botany at the University of Hawaii, who was not involved in conducting this study. Cannabis was probably used for “medicinal and spiritual reasons,” he says, “which are not really separated by most traditional societies.”\n\nAlthough the work shows the earliest chemical evidence of cannabis smoking, says study author Yimin Yang, professor of archaeometry at the University of Chinese Academy of Sciences, it doesn’t necessarily mean that the Pamirs are where the practice first gathered steam. Yang’s co-author Robert Spengler, laboratory director at the Max Planck Institute for the Science of Human History, agrees. Despite a cluster of archaeological finds from the Pamirs and other ancient cultures in the mountain foothill areas of Asia, Spengler urges researchers to continue the quest for earlier examples of cannabis use elsewhere.\n\nMore cases may inevitably turn up, says Barney Warf, professor of geography at the University of Kansas, who was not involved in the work. Warf called the most recent finding a “very reasonable and thorough study [that] makes a useful contribution documenting early psychoactive cannabis use. But I would not be surprised if future efforts uncover even earlier cases.” \n\nOther regions where evidence of ancient cannabis use has emerged include the lands of the ancient Scythians in Siberia and similarly old sites in China, where hints of recreational use have been discovered, says study author Hongen Jiang, professor of archaeobotany at the University of the Chinese Academy of Sciences. Merlin points to the discovery in Japan of seeds that are about 10,000 years old. “This was a key plant in Eurasia,” he says.\n\nA practice involving cannabis and death rites that may have disseminated far and wide along trade routes such as the Silk Road could have met its end with the rise of monotheism, Merlin says. “The spread of the major monotheistic religions pretty much wiped out shamanism, but there are vestiges,” he notes.\n\nThis sense of an ancient diversity of human ritualistic use lends context to the attention cannabis receives in contemporary headlines, says Spengler. “It is important to continually illustrate that the human interaction with this plant goes deeper back in time,” he says.\n\nIn the meantime, Long sees the findings reported in this study as “unambiguous” in confirming early use of cannabis deliberately as a psychoactive substance. Merlin agrees that all signs point to intentional use of the plant for this purpose, but it’s also possible that ancient people burned it to mask the smell of death and decay associated with mortuary rites, he says.\n\nStill, Merlin doesn’t think that’s really the case, because of the link of cannabis to the burials of shamans, suggesting shamans might continue their practices in the afterlife. Merlin himself has seen a another example of ancient cannabis use, although without confirmation of burning. “It was almost two pounds of cannabis next to a shaman’s head, practically still green,” he says, expressing awe of the find in tombs in Turpan, northwest China. “I observed this 2,500-year-old shaman, and he had mortar that had been caked in cannabis with him. That was a remarkable find.”","title":"Using Marijuana to Get High Dates Back Millennia","origin":"Behavior & Society","image":"https://static.scientificamerican.com/sciam/cache/file/953837EE-3C36-48FD-B21B73F0E9FB73CA_source.jpg?w=590&h=800&8A98FFAF-972F-41B6-BC73EAC6F009C46A","link":"https://www.scientificamerican.com/article/using-marijuana-to-get-high-dates-back-millennia/"},{"authors":"Kate Marvel","pub_date":"June 12, 2019","abstract":"I left California in the middle of a drought. The hills framing the 280 from the city to Palo Alto turned brown in the summer, as they always did, and then stayed brown through the winter. The pleasant seventy-degree air began to feel oppressive, the glorious blue sky a source of inchoate guilt. I lived in Oakland, by a cemetery on a hill. The winding paths past angel-topped mausolea and modest age-blacked gravestones were framed by a line of sweet gums, peppered with cypress, redwood, and olive. The smell—a peppery sweetness, pine without Christmas—is what I remember when I think of home.\n\nI now live in New York City, where trees are considered suspicious, not allowed to congregate in groups outside the designated parks. The few on my street are framed by small cages to keep the local dogs away. The effect is pathetic, almost comic, a tiny prison for a tall defiant thing. The largest tree I have seen in New York is dead and in the natural history museum. It is an enormous stump of a California sequoia, each ring a record of a life that began twenty generations before the birth of the logger who ended it.\n\nThe trees in New York, and the city too, are fed by moisture carried from the Gulf of Mexico or the Atlantic. There is no dry season, just a scatter of rain and snow spread evenly on top of the year. California rainfall, though, beats with the erratic rhythm of the tropical Pacific. There, the long spine of South America dredges cold water from the depths of the ocean, upwelling to form a permanent pool of cold water that extends into the west, pushed there by the trade winds. The cold water sloshes back and forth, the winds that drive it pulse to their own beat, and the result of the dance is, in some years, a phenomenon of warmer-than-usual water: the Christ Child, El Niño. When he visits, California can expect a rainy winter. When the opposite conditions prevail—La Niña—it’s drought.\n\nThere are other beautiful places that cycle through drought and floods with some regularity. In the eastern part of Australia, things are reversed. It is El Niño who dries, and La Niña who is associated with the rains. In the Mediterranean, it is a different cycle altogether. It has been like this for thousands of years.\n\nFor as long as we have been able to comprehensively observe the planet, we have been changing it. Satellites have recorded trends since the late 70s. But we have been increasing the level of atmospheric carbon dioxide since well before that. As scientists, we want to attribute the trends we measure to natural cycles like El Niño or externally imposed changes. But it can be difficult to tell if what we are seeing is unusual, because everything we have ever seen is unusual.\n\nTrees have seen more than any of us. If you drill a very small bore into a tree, you can extract a thin rod striped with each year’s rings. If you do this again, to another tree, and then another, and then to thousands all over the world, you can weave them together into a drought atlas, a record of good and bad years, wet and dry soils. Scientists have built these for half the world: North America and Mexico, Europe and the Levant, monsoon Asia and eastern Australia. The map is a record of wet and dry years that stretches backward in time. The trees remember years before world wars and depressions, before industrial and political revolutions, before even the first contact between the invaders and the New World.\n\nThe collected rings of thousands of trees show a pattern of drying and moistening that ebbs and flows with time. El Niño and La Niña arrive with erratic regularity, air and water slosh back and forth, and the trees record the consequences. At the beginning of the twentieth century, though, a faint fixed pattern becomes discernible among the randomness, a quiet but strengthening note against a background symphony. Some regions—California, the Mediterranean, Australia—dry out. It is a small, almost imperceptible-to-humans drying, but it is a pattern that no natural cycle can reproduce.\n\nThis pattern is similar to one that I generate on my computer in a small office in New York. In climate models, one can remove the effects of randomness by re-starting time over and over again, creating a universe of alternate worlds that might have existed but did not. The average of these never-worlds smooths out the kinks and ripples of randomness. If something is left over, it must be something common to all the realities that could have been.\n\nWhat is present in all of these worlds is carbon dioxide, which humans were even then slowly but steadily increasing. Carbon dioxide heats the planet, and warmer air is thirstier air. Even if there are no changes to rainfall, some areas will sink into drought under the atmosphere’s relentless demand for moisture.\n\nAnd this is the pattern that emerges from the trees. It is unusual in the context of everything that has come before, recorded for long posterity in the tree rings. It is a signature of us.\n\nThere are few issues in science that interest me less than the question of when the Anthropocene began. I am not a rock, and I do not need to find proof of human activity in the geological record to be concerned about our impact. But our fingerprints stretch further back into the past than we might have realized. And the harmony between the predictions of my machine-world and the real world recorded in the flesh of tree gives me reason to listen to what those machines tell me for the future.\n\nFrom my office in New York, I can look at these future projections and see California dancing from dry to wet and back again, until there is no again and it settles in to permanent drought. If the trees survive us, they may live to tell of a time where the grass turned brown, the map turned brown, and it was a long time before it was ever green again.","title":"Creeping Toward Permanent Drought","origin":"Hot Planet","image":"https://static.scientificamerican.com/blogs/cache/file/FFDB502F-729D-4DC3-A06AF2C0F5B7AB2D_source.jpg?w=590&h=800&48E2A878-4E8F-47A1-956305FED6E62E3B","link":"https://blogs.scientificamerican.com/hot-planet/creeping-toward-permanent-drought/"},{"authors":"Shannon Hall","pub_date":"June 12, 2019","abstract":"The sea sloshing beneath the icy surface of Jupiter’s moon Europa just might be the best incubator for extraterrestrial life in our solar system. And yet it is concealed by the moon’s frozen outer shell—presenting a challenge for astrobiologists who would love nothing more than to peer inside. Luckily they can catch a partial glimpse by analyzing the flavor of the surface. And the results are salty.  \n\nA new study published this week in Science Advances suggests that sodium chloride—the stuff of table salt—exists on Europa’s surface. Because the exterior is essentially formed from frozen seawater, the finding suggests that Europa’s hidden sea is drenched in table salt—a crucial fact for constraining the possibilities for life on the alien world.\n\nNot that scientists have tasted a slice of the distant moon. To analyze Europa’s composition, astronomers study the light emanating from its surface, splitting it into a rainbow-like spectrum to search for any telltale absorption or emission lines that reveal the world’s chemistry. There is just one problem: Ordinary table salt is white and thus gives off a featureless spectrum. But harsh radiation—which exists at Europa’s surface in abundance—just might add a dash of color. That much was realized in 2015 when two NASA planetary scientists Kevin Hand and Robert Carlson published a study suggesting the yellowish-brown gunk on Europa might be table salt baked by radiation. To reach that conclusion, Hand and Carlson re-created the conditions on Europa within vacuum chambers—or as Hand calls them, “stainless steel shiny objects that are humming and whizzing.” Next, they placed table salt into those chambers, lowered the pressures and temperatures to simulate Europa’s surface, and blasted the samples with an electron gun to simulate the intense radiation.\n\nMany of the electrons were then captured in empty spaces within the salt’s crystalline structure, transforming the salt’s color into a yellowish brown that bore an uncanny resemblance to that of the mysterious material filling the fissures and fractures crisscrossing Europa’s surface. The parallel caused Hand and Carlson to speculate that the gunk just might be irradiated table salt. But a match in color is not proof. Instead, proof could arrive in the form of very distinct spectroscopic features—particularly an absorption line at 450 nanometers within the visible portion of the electromagnetic spectrum. “One of the upshots of this irradiation is that it can help make the previously invisible, visible,” Hand says.\n\nSo, Samantha Trumbo, a graduate student at the California Institute of Technology, her advisor Michael Brown and Hand (now also at Caltech) turned the Hubble Space Telescope toward the distant moon in search of that very feature. And after four observations that ran from May to August 2017, they spotted it. “The match between the lab data and the telescopic data was just beautiful,” Hand says. It proves that the surface of Europa is indeed covered with sodium chloride—at least within a region called Tara Regio that harbors young ice thought to have erupted from the ocean relatively recently. “You’d be able to taste it,” Trumbo says. “If you licked the surface of Europa, at least in this spot, it would taste salty.”\n\nReggie Hudson, an astrochemist at the Cosmic Ice Laboratory at NASA’s Goddard Space Flight Center who was not involved in the latest study, is impressed by the full picture. “It’s a very nice synthesis of lab work, Hubble data, radiation chemistry and planetary science,” he says.\n\nFor years, astronomers have argued that another type of salt, magnesium sulfate, was prevalent on Europa’s surface. Under that thinking, the moon’s subsurface ocean might have resembled Epsom salts mixed into a bath. But this new work suggests instead the ocean is more akin to table salt mixed with tap water—like the solution you might create to soothe a sore throat or what you might find in the Earth’s oceans. “It really is a shift from what we’ve thought for the past 20 years or so,” Trumbo says. “If this sodium chloride… is really representative of the ocean composition, then the ocean, at least salt-wise, would be more similar to what we see on Earth.”\n\nAnd that could boost the chance for life to arise and evolve. Even so, scientists cannot yet say what quantities of salt exist beneath the ice. Europa’s seas could ultimately prove to be too salty for life as we know it to exist there at all.\n\nThen again, a more equitable mix of salt and water could allow life there to thrive—especially if that ocean is as active as our own. On Earth, water gets pulled into the seafloor around the flanks of hydrothermal vents—seafloor springs that spew water at superhot temperatures—before being jettisoned back out from the vents themselves. In that process magnesium is captured within the rocks, whereas sodium and chlorine escape. In theory, Europa’s table-salt–laced surface could hint that the moon’s seawater is similarly cycled and even point toward hydrothermal vents—a tantalizing prospect given that these vents on Earth are usually overrun with life. But Hand and Hudson both caution against making such a bold leap. “I’m not 100 percent convinced that sodium chloride on Europa’s surface means hydrothermal activity down under, but the two things are not mutually exclusive,” Hudson says.\n\nAt the very least, the finding shows that Tara Regio is a good spot to study further with NASA’s Europa Clipper mission—a probe set to launch in the 2020s that will orbit Jupiter and fly past Europa 45 times. Trumbo, in particular, would like to search for any organics that might have spilled onto the surface along with the salt. And, just maybe, scientists could get close enough to Europa’s surface to (remotely) taste it, via a robotic lander. “As someone who has studied Europa’s surface chemistry for about 20 years, I’d have to say that nothing beats a lander for getting up close and personal,” Hudson says. And Hand similarly wants to be within reach.\n\n“This is excellent work, but I doubt that it is the last word on the subject,” Hudson says. “As with a lot of good scientific research, it doesn’t close the door, it opens it to future studies.”","title":"Water on Europa—with a Pinch of Salt","origin":"Space","image":"https://static.scientificamerican.com/sciam/cache/file/F9571C44-9516-46A8-8C9A9B1E78FAF40E_source.jpg?w=590&h=800&8AF1653A-953F-4539-8E44A1632FF29ADA","link":"https://www.scientificamerican.com/article/water-on-europa-with-a-pinch-of-salt/"},{"authors":"Maxine Joselow, E&E News","pub_date":"June 12, 2019","abstract":"An independent panel of experts is poised to scrutinize one of the Trump administration’s most consequential environmental rollbacks.\n\nEPA’s Science Advisory Board voted last week to review the science behind the proposed overhaul of Obama-era clean car standards. The 45-member board, which is tasked with advising EPA on a range of scientific matters, will zero in on the technical analysis underpinning the proposal (E&E News PM, June 6).\n\nCritics of the Trump EPA say the review stands to highlight errors in that analysis that could inform future lawsuits against the administration.\n\n“We anticipate this review will paint a more complete picture ... of the current proposal’s inconsistencies and faulty conclusions,” said Robbie Diamond, president and CEO of Securing America’s Future Energy.\n\nJeff Alson, a former staffer in EPA’s Office of Transportation and Air Quality, said the administration’s analysis “doesn’t pass the laugh test.”\n\n“I think it was clearly based on politics, not science,” he said.\n\nThe rollback, formally known as the “Safer Affordable Fuel-Efficient (SAFE) Vehicles Rule,” is aimed at significantly weakening Obama-era greenhouse gas and fuel economy standards for light-duty vehicles.\n\nPresident Obama made the standards a key plank of his climate agenda. He envisioned getting cars to travel an average of 54.5 mpg by 2025.\n\nBut EPA and the National Highway Traffic Safety Administration are proposing to freeze fuel economy requirements at an average of 37 mpg.\n\nThe two agencies are also proposing to withdraw California’s Clean Air Act waiver for greenhouse gases, which allows the state to set tougher tailpipe pollution rules than the federal government.\n\nThe scientific board’s review will serve as an important check on the Trump EPA, said Trish Koman, a professor at the University of Michigan School of Public Health.\n\n“EPA is a science-based agency, and it has very large public health responsibilities. So the law requires, and the public expects, EPA to obtain independent science advice,” said Koman, who worked as an EPA scientist for 22 years before leaving the agency in 2012.\n\n“The agency, as an institution, really needs to have procedures in place so they can do a rigorous look at the analysis and provide the best information to decisionmakers and the public,” she added.\n\n‘What is going on there?’\n\nThe SAB meeting last week took place in downtown Washington, D.C., and spanned two days.\n\nDuring the first day, the board grappled with how to approach the clean cars rollback and, in particular, the role California plays.\n\nThe proposal sets up a clash with the California Air Resources Board, which has historically issued tougher tailpipe pollution rules than the federal government. Thirteen states have adopted those tougher requirements.\n\nAn SAB work group chaired by Alison Cullen, a public health expert at the University of Washington, recommended that the full board review the proposal only if EPA and CARB fail to reach agreement on “harmonized” tailpipe standards applying to the entire country.\n\n“If they don’t harmonize the rule between CARB and EPA, then I think it would be interesting for SAB to consider what are the differences,” Cullen said. “Why are they not able to harmonize? And [are] aspects of the science being differently emphasized? What is going on there?”\n\nThe board also confronted the rollback’s effect on greenhouse gas emissions and climate change.\n\nAn analysis by the Rhodium Group, an economic consulting firm, found that the proposal would lead to an additional 32 million to 114 million metric tons of carbon dioxide emissions.\n\nBut John Christy, a professor of atmospheric sciences at the University of Alabama, Huntsville, argued that the proposal would not affect emissions at all. EPA Administrator Andrew Wheeler appointed Christy, who rejects mainstream climate science, to the board in January (E&E News PM, Jan. 31).\n\nThe rollback “will have zero effect on the climate,” Christy said at the meeting. “So this is going to be a policy issue. There’s no science in terms of its impact on the climate.”\n\nSAB Chairman Michael Honeycutt proposed that the board defer action on the rollback until its next meeting. But that remark prompted grumbles of opposition from several members. The board then agreed by a show of hands to conduct the review.\n\n‘Foot-dragging’\n\nThe SAB faces a severely compressed time window for its review. The Trump administration is seeking to finalize the rollback this month, although experts have questioned whether it can meet this self-imposed deadline (Climatewire, June 3).\n\n“If the administration is going to proceed on the time frame it mentioned, it’s going to be very hard for the SAB to do an analysis of the scope and complexity that’s needed,” said Chris Frey, a professor of environmental engineering at North Carolina State University.\n\nFrey, who served on the SAB from 2012 to 2018, accused Wheeler of using delay tactics to foster the time crunch. The board sent EPA a notice of its intent to review the rollback last June; Wheeler responded 10 months later.\n\n“It shouldn’t take 10 months for the administrator to respond,” Frey said. “In my opinion, the administrator is foot-dragging to let the clock run out and hope the SAB doesn’t get its act together to do anything.”\n\nEPA spokesman Michael Abboud pushed back.\n\n“In his first few months on the job, Administrator Wheeler has strived to engage with the SAB on a more regular basis, which is why he provided his April letter to the board and appeared before the Board last week to take questions,” Abboud said in an email to E&E News.\n\n“Administrator Wheeler will continue to work with the SAB and try to improve relations that the previous administration took for granted,” he said.\n\nLawsuits on the horizon\n\nStill, the board’s review could ultimately provide ammunition for future lawsuits.\n\n“It would be informative to future rulemaking efforts to have an SAB opinion on record. And it may also be informative to litigants,” Frey said.\n\nCalifornia Attorney General Xavier Becerra (D), a vocal foe of President Trump, has promised to sue once the rollback is finalized. So have a litany of environmental groups.\n\n“We all know that there will be immediate litigation as soon as the rollback is completed,” said Alson, the former EPA staffer. “If the Science Advisory Board were to go on record with concerns about how the science and technical analysis were carried out, I would think that would be exactly the type of information judges would be interested in.”\n\nThe SAB could help California and green groups pinpoint specific errors in the administration’s technical analysis, Alson said.\n\nIn particular, the administration has argued that the rollback will prevent 12,700 deaths on the nation’s roads. To justify this claim, it pointed to something called the “scrappage model,” which predicts when people will “scrap” an older car and purchase a newer model.\n\nBut critics say the administration manipulated the scrappage model to get its desired result.\n\n“The analysis underlying your proposal simply makes no sense,” CARB Chairwoman Mary Nichols testified at a hearing in Fresno, Calif., last year, adding, “[These] claims are not only absurd on their face; they are not supportable by fact.”\n\nA group of 11 prominent researchers published a paper in Science magazine last year accusing the administration of misrepresenting their findings in its technical analysis. They say the administration not only manipulated their work on the scrappage model but also on fleet size, compliance costs and other factors (Climatewire, Dec. 7, 2018).\n\n“There were just so many assumptions made in that analysis that are not defensible,” Alson said. “The paper in Science magazine was just one example.”\n\nReporter Sean Reilly contributed.\n\nReprinted from Climatewire with permission from E&E News. E&E provides daily coverage of essential energy and environmental news atwww.eenews.net.","title":"EPA’s Science Advisory Board to Scrutinize Clean Car Rollback","origin":"Policy & Ethics","image":"https://static.scientificamerican.com/sciam/cache/file/F825449D-BE08-4D1F-8A5E7E8D5117D428_source.jpg?w=590&h=800&5B988E11-293F-445D-B875A54814374100","link":"https://www.scientificamerican.com/article/epas-science-advisory-board-to-scrutinize-clean-car-rollback/"},{"authors":"Michelle Doose","pub_date":"June 11, 2019","abstract":"We live in the age of tech-savviness where I can hail a ride or order food at the touch of a button. But when I move across the country and transfer my medical care to a new provider, I’m expected to bring paper copies of all of my medical records and succinctly describe my detailed and complex medical history in 30 seconds.\n\nThat can be especially hard for cancer survivors, like me, who often have long, complex medical histories. And it’s something we’d do well to remember and address. \n\nThe seminal 2006 Institute of Medicine (IOM) report “From Cancer Patient to Cancer Survivor—Lost in Transition,” first brought to light the unmet needs of patients as they transitioned from active cancer treatment to the post-treatment “survivorship” phase. It also acknowledged the complex, unique health care needs of cancer patients and survivors that often require the involvement of many medical providers—cancer specialists, primary care providers, and specialists like cardiologists. But it’s been over a decade since this landmark report was published. Sadly, to this day, we still do not deliver comprehensive, coordinated care to every patient.\n\nThat’s a bigger problem than you might think.\n\nThis year alone, 1.7 million adults and 11,000 children (ages 0–14) in the United States will be diagnosed with cancer. Up to half (or more) may have at least one other chronic health condition at cancer diagnosis, like hypertension or diabetes, which requires even more organization of patient care. Another 16.9 million people are cancer survivors, including half a million survivors of childhood cancer.\n\nCancer survivors face unique health care needs because of the negative health effects brought on by chemotherapy, radiation, surgery and/or other therapies. These effects can manifest during, shortly after or many years after treatment. In fact, two thirds of adult survivors of childhood cancer are at risk of developing a chronic condition as a result of cancer treatment.\n\nPhysicians cannot take away the risk of potential health problems from the lifesaving treatments needed to cure their patients of cancer. But patients deserve to receive high-quality care to be as healthy as possible.\n\nThe IOM report recommends that patients receive a comprehensive care summary with a follow-up plan and coordinated, interdisciplinary survivorship care. A simple one-to-two–page survivorship care plan can outline the treatments received and the tests and follow-up care needed to screen for recurrence and other health effects from treatments. Yet, many patients do not report receiving a survivorship care plan to help them coordinate their care.\n\nToo often, patients are lost after their treatment ends and have many lingering questions. Which doctor should I see to make sure the cancer has not returned? Who is supposed to help control chronic pain or depression or other effects from cancer treatment? And who should I see for ongoing preventive care or when I have a cold?\n\nThese are questions that I’ve asked myself. As a survivor of childhood cancer, I struggled to get the care I needed when I aged out of pediatric oncology to adult medicine, when health insurance coverage changed with a new job and when new providers were added to my care team. The last time I transferred care to a new primary care provider, the doctor asked me “So, everything is okay with your cancer history?” That’s not an adequate question. I wasn’t asked for a survivorship care plan, which treatments I received, or if I had any ongoing effects from treatment. As much as cancer survivors may not want to talk about their cancer history (and relive the experience), we do need to make sure our medical providers are aware of our medical history and on top of current survivorship guidelines.\n\nAnd let’s not forget the role of health insurance and out-of-pocket costs that cause patients to forgo or skip necessary medical care. “This can cause further frustration to both the provider and patient when receiving denials for a needed treatment or labs and imaging for surveillance,” Sonia Morales a hematology/oncology fellow at Children’s Hospital of Orange County, explained to me. “Oftentimes the general population do not need mammograms nor bone Dexa [dual energy x-ray absorptiometry] scans until they reach an older age. However, many times cancer survivors are in need of these services as prior treatments can cause secondary late effects such as another cancer or osteoporosis.”\n\nThe financial effects from cancer can also be long-lasting, including stress, loss of income, debt, bankruptcy and change in job status or career goals. And all of this is even harder for racial/ethnic minorities and individuals with fewer economic resources who still have poorer cancer care and cancer outcomes than their white peers.\n\nOf course, recent tech advances have made it to the health care industry too. Electronic health records, or EHRs, are getting more and more common, giving providers a tool to record and track patients’ health information and giving patients access to a digital copy of their information. Yet too often patients cannot get their EHRs from one medical provider to communicate with another medical specialist. This can be true even in the same health care system. And for cancer survivors, this may mean medical information located on different apps and websites for each provider.\n\nIt’s time we focus on helping patients thrive during and after cancer with coordinated care focused on each patient’s physical health as well as their psychological, social and financial well-being.\n\nAnd to accomplish that, innovative thinking will be required. New models of care delivery, like patient-centered medical homes and accountable care organizations, are focused on reshaping the health system by organizing the delivery and coordination of patient care activities. These include sharing EHRs with all providers, co-locating providers and services in the same physical location, linking patients to community services and resources to meet their social needs (like food and housing), as well as, telemedicine and patient navigation. These new models also hold health systems and providers accountable for the health care outcomes of their patients. Investing more time and energy into these, and other ideas and innovations, is an important step in supporting our nation’s cancer survivors.\n\nHealth care is complex. But care coordination for cancer survivors would help millions get the care that they need and deserve. And it can serve as a model for optimal care delivery for patients with other health conditions, so nobody will be asked to recite their medical history in 30 seconds again.","title":"Cancer Survivors Deserve Coordinated Care","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/2CF0F1FA-BB0C-4187-8D2340671A2D2639_source.jpg?w=590&h=800&4A6BCD1C-1C40-4C92-8C553AE67E1B0F33","link":"https://blogs.scientificamerican.com/observations/cancer-survivors-deserve-coordinated-care/"},{"authors":"Scott Pakin, Patrick Coles","pub_date":"June 10, 2019","abstract":"By now, most people have heard that quantum computing is a revolutionary technology that leverages the bizarre characteristics of quantum mechanics to solve certain problems faster than regular computers can. Those problems range from the worlds of mathematics to retail business, and physics to finance. If we get quantum technology right, the benefits should lift the entire economy and enhance U.S. competitiveness.\n\nThe promise of quantum computing was first recognized in the 1980s yet remains unfulfilled. Quantum computers are exceedingly difficult to engineer, build, and program. As a result, they are crippled by errors in the form of noise, faults, and loss of quantum coherence, which is crucial to their operation and yet falls apart before any nontrivial program has a chance to run to completion.\n\nThis loss of coherence (called decoherence), caused by vibrations, temperature fluctuations, electromagnetic waves and other interactions with the outside environment, ultimately destroys the exotic quantum properties of the computer. Given the current pervasiveness of decoherence and other errors, contemporary quantum computers are unlikely to return correct answers for programs of even modest execution time.  \n\nWhile competing technologies and competing architectures are attacking these problems, no existing hardware platform can maintain coherence and provide the robust error correction required for large-scale computation. A breakthrough is probably several years away.\n\nThe billion-dollar question in the meantime is, how do we get useful results out of a computer that becomes unusably unreliable before completing a typical computation?\n\nAnswers are coming from intense investigation across a number of fronts, with researchers in industry, academia and the national laboratories pursuing a variety of methods for reducing errors. One approach is to guess what an error-free computation would look like based on the results of computations with various noise levels. A completely different approach, hybrid quantum-classical algorithms, runs only the most performance-critical sections of a program on a quantum computer, with the bulk of the program running on a more robust classical computer. These strategies and others are proving to be useful for dealing with the noisy environment of today’s quantum computers.\n\nWhile classical computers are also affected by various sources of errors, these errors can be corrected with a modest amount of extra storage and logic. Quantum error­correction schemes do exist but consume such a large number of qubits (quantum bits) that relatively few qubits remain for actual computation. That reduces the size of the computing task to a tiny fraction of what could run on defect­free hardware.\n\nTo put in perspective the importance of being stingy with qubit consumption, today’s state-of-the-art gate-based quantum computers, which use logic gates analogous to those forming the digital circuits found in the computer, smartphone, or tablet you’re reading this article on, boast a mere 50 qubits. That is just a tiny fraction of the number of classical bits your device has available to it, typically hundreds of billions.\n\nTAMING DEFECTS TO GET SOMETHING DONE\n\nThe trouble is, quantum mechanics challenges our intuition. So we struggle to figure out the best algorithms for performing meaningful tasks. To help overcome these problems, our team at Los Alamos National Laboratory is developing a method to invent and optimize algorithms that perform useful tasks on noisy quantum computers.\n\nAlgorithms are the lists of operations that tell a computer to do something, analogous to a cooking recipe. Compared to classical algorithms, the quantum kind are best kept as short as possible and, we have found, best tailored to the particular defects and noise regime of a given hardware device. That enables the algorithm to execute more processing steps within the constrained time frame before decoherence reduces the likelihood of a correct result to nearly zero.\n\nIn our interdisciplinary work on quantum computing at Los Alamos, funded by the Laboratory Directed Research and Development program, we are pursuing a key step in getting algorithms to run effectively. The main idea is to reduce the number of gates in an attempt to finish execution before decoherence and other sources of errors have a chance to unacceptably reduce the likelihood of success. \n\nWe use machine learning to translate, or compile, a quantum circuit into an optimally short equivalent that is specific to a particular quantum computer. Until recently, we have employed machine-learning methods on classical computers to search for shortened versions of quantum programs. Now, in a recent breakthrough, we have devised an approach that uses currently available quantum computers to compile their own quantum algorithms. That will avoid the massive computational overhead required to simulate quantum dynamics on classical computers.\n\nBecause this approach yields shorter algorithms than the state of the art, they consequently reduce the effects of noise. This machine-learning approach can also compensate for errors in a manner specific to the algorithm and hardware platform. It might find, for instance, that one qubit is less noisy than another, so the algorithm preferentially uses better qubits. In that situation, the machine learning creates a general algorithm to compute the assigned task on that computer using the fewest computational resources and the fewest logic gates. Thus optimized, the algorithm can run longer.  \n\nThis method, which has worked in a limited setting on quantum computers now available to the public on the cloud, also takes advantage of quantum computers’ superior ability to scale-up algorithms for large problems on the larger quantum computers envisioned for the future.\n\nNew work with quantum algorithms will give both experts and nonexperts the tools to perform calculations on a quantum computer. Application developers can begin to take advantage of quantum computing’s potential for accelerating execution speed beyond the limits of conventional computing. These advances may bring us all several steps closer to having robust, reliable large-scale quantum computers to solve complex real-world problems that bring even the fastest classical computers to their knees.","title":"The Problem with Quantum Computers","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/C2459FB7-AF87-4937-A8414AC0E54113B7_source.jpg?w=590&h=800&0D70F61B-5A2F-419F-9E02D49B94B36F5C","link":"https://blogs.scientificamerican.com/observations/the-problem-with-quantum-computers/"},{"authors":"Anastasia Cifuentes","pub_date":"June 7, 2019","abstract":"Back in 2013, Nisha Dua was scrambling to reinvent a celebrity gossip site called Cambio. With little money and tasked with reimagining a brand whose client base was 75 percent 13- to 24-year-old girls, Dua called upon five teenage girls to help her rework the entire site. The girls were immersed in a six-week internship, where they would spend time with engineers; UI/UX experts; and sales and marketing personnel. These five girls would end up driving the development of the new Cambio, creating a site for girls and by girls.\n\n “Many of the girls came in thinking, I want to do this because I want to blog about the Kardashians,” Dua told me, speaking from her co-working space in New York’s Meatpacking District. “But they left saying, ‘actually I want to be in data analytics, so I can understand how data drives audience insights.’” That experience, which began as a brand experiment, evolved into her founding in 2014 of Built By Girls—an organization that prepares female and non-binary high school and college students for careers powered by technology. Its mission is to give hundreds of thousands of girls what those five interns at Cambio had.\n\n“I think there’s no question that you can only be what you see,” said Dua, who was raised by Indian parents in Australia. “When I see Mindy Kaling on TV, that changes my perspective of what a young Indian woman can be—when she speaks confidently and unapologetically about her body, her business, her accomplishments, that is what I ingest as a viewer. Despite coming from an Indian family, there was never a gendered difference applied to us in terms of success. I was frankly never part of the women’s groups at work, because I grew up expected to be as successful as my two big brothers. It wasn’t until I started investing in women that my perspective completely changed.”\n\n“The first time I ever went to The Wing,” she continued, referring to a social club and co-working space for women, “I must say I was a little bit nervous.” Dua continued, “because I was like, I’m not sure I’m a cool girl, and I think this might be a place for cool girls.” Dua’s company BBG Ventures (separate from Built By Girls) is an early- stage venture fund that invests in consumer start-ups founded by at least one woman; it was an early investor in The Wing. “I had never in my life felt this experience of being in a room full of women where there’s no element of competition, every woman in the room is there to be who she is, to work hard, to play hard, and to build together—it’s magical.” “You feel like there is space for you [that] is when you personally can shift, and when a group can shift.”\n\nThe Built By Girls platform, Wave, works by algorithmically pairing female advisors with advisees. “We want to set it up so that people can have an organic and authentic connection once they meet in person, but they can be guided by our tools to have a valuable conversation,” explained product manager Corie Miller. The algorithm takes 13 different data points into account—including location, previous matches, whether the advisee has been at a company before, interests, obsessions, industry, and tech medium (such as software or hardware).\n\nUsing a corpus of keywords to determine overlap between possible matches, the algorithm uses a Monte Carlo tree search to produce 200 simulations. The optimal match in the entire universe of matches, or the match that has the most overlap, is selected. The algorithm can match approximately 2,000 users in under five hours, according to Miller. “We’re using tech in order to ... forge this new type of community, and this new type of network that’s super meaningful, not just ... click a button, and we’re friends,” Miller added.\n\nI myself joined Built By Girls in 2016, after I was recommended by a female founder. One of my advisees, Neha Somineni, was introduced to the program by the president of her coding club. Somineni and I spent three months together, to brainstorm about a virtual reality app she was building, which would allow people to remotely experience famous destinations throughout the world. Our meetings were always spirited, and we’d go beyond the allotted time.\n\nDuring our time together, I introduced Neha to the founder of an image recognition start-up, a systems engineer, a VR game designer and my business partner—all of them men. Neha would meet with three other advisors throughout the program, and go on to study biochemistry at Stony Brook University, with a possible minor in nanotechnology. She would also join WISE, the Women in Science and Engineering club at the university. Like Dua, Neha said, “I never really associated myself personally with a woman’s group.” “After a while, I started to realize it’s good to have someone who is going through the same things as you.”  \n\n“It’s not just who you know, it’s knowing what they know,” explained Dua, about the platform’s goals. “I think women really enjoy context and being able to relate skills to real-world problems.” “And you need a way to break into the network—if you’re a young girl in Compton, who is maybe a single mother, you might never have the opportunity to crack into the club of a New York start-up, or Uber or Amazon,” some of the Built By Girls partners. “We are the older sister for these girls,” Dua added, and Built By Girls is arming them with practical skills that no one teaches you in the classroom.\n\nAll of the advisees are generation Z. Many are first-generation immigrants who are striving to be activists in their communities. These girls know technology is a big part of moving ahead, but being a coder is not the end goal, explained Dua. “There is this amorphous thing called STEM, but what are the skills, the traits, the passions that align with those different careers in STEM?” asked Dua. These girls are asking, “How do I harness the power of tech to change the world?” \n\n“Exposure is so important,” said Charlize Yiwen Wang, an advisee I mentored in 2018. Built By Girls is “a better way of obtaining information.” When I met Wang, she was on the cusp of graduating college from a double major in computer science and applied mathematics. She now works at an enterprise software company in San Francisco called Workday, where she develops features for the application team. “Startup culture doesn’t sound as unreachable as before.” She confided that without the BBG experience, she could have easily have turned into a “coding monkey.”\n\nThe appetite is just as real on the other side, for advisors. Dua was quick to correct me on my use of the words mentors and mentees. “It is very intentionally not a mentorship program,” she clarified. Built By Girls is promoting something more organic; not every professional is ready to be a mentor, but they do have advice and expertise to share, Dua explained. “A trend we see is toward individual citizenship.” There is a desire in advisors to provide help that they didn’t have as young people.\n\nIn the last two years, Built By Girls has matched over 7,000 pairs, and it plans to match 15,000 by the end of the year. “Girls are demanding this like crazy,” shared Dua, and the team is struggling to keep up with a wait-list of hundreds of girls looking to be matched with an advisor. The organization is available in 47 states and plans to go global.\n\n At least 20 percent of advisors on the platform are male, something that’s critical to the platform’s success, according to Dua. “Our general approach is to define people as individuals, with different interests and different traits. It helps us see past gender,” Dua proclaimed, perhaps similar to the way she did when she entered The Wing for the first time or the way Somineni did in joining WISE. By connecting women in different phases of their education and careers, Built By Girls is helping to mold a new generation of people powered by technology, gender aside.","title":"Built by Girls","origin":"Voices","image":"https://static.scientificamerican.com/blogs/cache/file/69DB5495-D879-4672-A8AD77F393B65092_source.jpg?w=590&h=800&27064118-5D8F-4F96-84EBA0178EA4D7E1","link":"https://blogs.scientificamerican.com/voices/built-by-girls/"},{"authors":"Robert Langer","pub_date":"June 12, 2019","abstract":"When we think of medical breakthroughs, our minds often turn to new drugs and treatments. However, innovations in biomaterials and medicine delivery processes are just as revolutionary and important as they pave the way for such advances.\n\nAt the Massachusetts Institute of Technology, we are working at this interface of biotechnology and materials science. Take something like a burn. We can effectively treat someone with an antibiotic ointment. However, a severe burn may require more advanced treatment, such as building a skin cell polymer scaffold that helps a patient regenerate their skin. That is where our work comes in. And it’s an example of how treatments are getting ever more personal to each patient.\n\nOne major focus for our team is developing polymers to deliver drugs continuously and at controlled rates for prolonged periods of time. In our research, we have created biomaterials and processes for everything from controlled drug release to tissue regeneration, and worked on anticancer drugs, gene therapy agents and vaccines alike.\n\nThis means a better response to some of the limitations of human body, and particularly its central nervous system. When we take a drug orally, it can wreak havoc on the body as it carries medicine throughout our system. By engineering polymers that offer smart delivery systems, we can target specific parts of the body. This limits exposure and therefore adverse effects, offering more effective and precise treatment.\n\nA good example of this is treating brain disorders. Drugs taken orally—such as L-DOPA, a dopamine precursor used to treat Parkinson’s disease, and Prozac, used to boost serotonin levels in patients with depression—can interact with neurotransmitters. These drugs have side effects because they act throughout the brain. What we can do instead is deliver medicine through a miniaturized cannula—a thin tube used to deliver medicine—that is connected to small pumps implanted under the skin. This technique can deliver tiny doses (on order of hundreds of nanoliters) into brain regions as small as one cubic millimeter, giving us much more control over where medication is dispensed, according to an individual patient’s needs. Research has demonstrated that this technology can control motor function. Down the line, it may help us treat tumors or neurological disorders such as Parkinson’s disease or Alzheimer’s.\n\nAnother exciting application of biomedical and technological innovation is telemedicine, or medical treatment from a distance. One example of this involves creating miniaturized wireless devices to treat chronic diseases or conditions that require frequent injections. Edible microchips could respond to wireless transmissions, and release drugs at sequential intervals into someone’s body to treat diseases. In theory, one could have a “pharmacy on a chip” that could dispense multiple drugs in regular and accurate doses.\n\nRight now, we are starting to see more funding of personalized treatments. This includes personalized cancer vaccines, which use messenger RNA to make vaccines against a person’s own particular cancer. These treatments harness a patient’s own cells to fight disease and foster the development of vaccines against a particular cancer, rather than a one-size-fits all approach. Almost all RNA therapies use nanoparticles for delivery, making technological development key for delivering medication at the right scale.\n\nThis treatment is part of a broader interest in immunotherapy and cellular-based therapies, where I expect innovation will occur in the next 30 years. The goal here is harnessing the immune system to more effectively treat diseases. This means working at a small scale to achieve more effective results.\n\nThat being said, the fruits of these labors do take some time. Regardless of the research and innovation at hand, clinical trials and funding can have significant and unpredictable effects on timelines for medical innovation and adoption.\n\nThis makes it even more crucial that we encourage innovation and support scientists and engineers. They need the freedom to explore, learn and create. They must feel that there are no limits to what they can do, that they are doing something important; and we need to raise the profile of innovative new technologies so that other teams of scientists can develop further treatments. Awards like the Millennium Technology Prize help facilitate this; in fact, since I received the prize for my research, some of the greatest strides in personalized medicine have been made by teams from across the globe. \n\nFor most diseases and illnesses, we’re still at a stage where finding the right treatment is a process of trial and error; most patients are prescribed treatments because they have been proven to work for the majority, and if these don’t work then it becomes a long and costly process of finding the one that is effective. But, as the developments in the areas above show, scientists and engineers are beginning to make great strides in making medicine more personal.\n\nNominations for the 2020 Millennium Technology Prize opened on April 1, 2019 and are accepted until July 31, 2019. They can be received from organizations such as universities, research institutes, academies of science and technology, companies and industries, and can be awarded to an individual or to a team of any nationality. More information is at millenniumprize.fi.","title":"How Nanotech Powers Precision Medicine","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/2CB98507-38A4-49B8-9A8DBD4743DB283E_source.jpg?w=590&h=800&B2A4B2CA-B92D-4D62-A1A06DA95155F016","link":"https://blogs.scientificamerican.com/observations/how-nanotech-powers-precision-medicine/"},{"authors":"Evelyn Lamb","pub_date":"June 5, 2019","abstract":"No disrespect to \"why was six afraid of seven,\" but \"base 10\" is the funniest math joke. \n\nI have made the mistake of unironically writing the phrase \"base 10\" before, and I recently cringed and also laughed at an old post of mine that used the phrase “base 60” over and over again. I almost caught myself writing “base 8” the other day. In the unfortunately unfindable-on-YouTube words of Mitch Hedberg (who was talking about Ritz crackers, but it works for numeral bases too) if you write “base 60” or “base 8,” you have no faith in the product itself. \n\nEvery base is base 10. That’s the whole point of the positional numeral system! In the base ten system, the farthest right place is the ones place, the place one to the left is the tens place, one to the left of that is the hundreds (ten squared) place, and so on. In base two, the farthest right place is for ones, the next one to the left is for the twos, the one to the left of that is for fours (two squared), and so on. In base two, the number two is 10. \n\nThat’s how bases work in general. In base sixteen, the number sixteen is 10. In base twenty, the number twenty is 10. In base sixty, the number sixty is 10. So “base 10” means absolutely nothing, and I find that hilarious.\n\nOn shows like Star Trek where some kind of universal translator is a given, I always wonder how the translation programs deal with the fact that every species of alien would probably refer to themselves and their planet with words that would most naturally translate to the English words “humans” or “people” and “Earth” or “world.” (I recognize this is definitely not the least believable thing about universal translators.) The way modern mathematicians have defined a positional numeral system means that if the base we are using is not already understood, there is no way to communicate it using digits.\n\nBase 10 is a very funny joke, but in order to facilitate communication, I am hereby dedicating myself to the cause of always spelling out numbers when I need to explain what base I am using. The English language also has base ten built into the way we form numbers—sixty seven is six tens and seven, for example—but at least the written forms of numbers avoid ambiguity because there is no different way to speak or write numbers to communicate that they are in a base other than ten. (I almost typed 10. Gah!)\n\nWe are so used to 10 representing the number ten that we don’t even recognize “base 10” as a joke, but it is. To me, that is what makes it the funniest math joke.","title":"The Funniest Math Joke","origin":"Roots of Unity","image":"https://static.scientificamerican.com/blogs/cache/file/A4A22EE4-ED6D-4B70-9C8C43F54970DFE6_source.jpg?w=590&h=800&3FCF02F5-9B5F-4141-B1059135FE6F0BF6","link":"https://blogs.scientificamerican.com/roots-of-unity/the-funniest-math-joke/"},{"authors":"Erin Daksha-Talati Paquette","pub_date":"June 7, 2019","abstract":"As a woman practicing medicine, raised by a first-generation immigrant father and Hispanic mother, I fit the image of an underrepresented minority. Yet my education and position belie that stereotype.\n\nAs a young girl, I remember walking in our small town in Maryland watching my Indian father’s expression harden and eyes dim as he held back from reacting to racially directed comments—shouted as we walked by—urging him to return to his “home country.”\n\nI didn’t understand at the time what racism meant or the traumatic impact that repeated experiences could have on health. Lately I have understood it all too well.\n\nThe recent shootings at a California synagogue and a New Zealand mosque, as well as acts of hatred at the university where I teach and work, demonstrate the ongoing prevalence of racism in places of religious worship and institutions of higher education—places where tolerance is generally promoted.\n\nThese events remind us of the collective responsibility we have to acknowledge and address racism.\n\nRecognition of widespread health inequities in minority populations is growing, as is awareness that disparities in the adult population are mirrored in the pediatric population. As a pediatrician, I see firsthand that poverty is a key driver of poor health.\n\nI have seen families struggle to keep children with chronic medical illness living at home because of their socioeconomic state. Often these children need home nursing support, but their family’s only housing options are limited to areas where agencies cannot get nurses to travel. These children come into the hospital sicker because they cannot get the support they need at home and can get stuck in the hospital when we are not able to find sufficient nursing support for them to be able to be at home with their families.\n\nEmerging research addresses how socioeconomic status can impact individuals at the level of their genes and thereby influence their health. Although it is often assumed minority race and ethnicity are associated with lower socioeconomic status, in reality, minority race and ethnicity are independently linked to worse health.\n\nSome researchers have reported that race makes one susceptible to worse health outcomes, but others have cautioned against the risk of treating race as being biologically related to poor health. Instead many of us have advocated for evaluating practices and policies that may change how certain minority racial or ethnic groups gain access to the highest quality care. This is an important difference because while we can’t change a person’s race, we can change practices and policies that make certain racial groups less likely to get the best health outcomes.\n\nIndeed, for many reasons including economics, geography, insurance and more, some patients do not have access to the highest performing health care institutions. That lack of access can influence health outcomes for racial and ethnic minorities.\n\nEven within institutions, racial and ethnic disparities may exist, related both to health outcomes and to patient experience. For instance, at several different hospitals, I have seen how the lack of doctors and nurses from racial and ethnic minorities can lead to racial and cultural barriers between health care providers and families. These barriers often place a strain on communication, which can impact how families make decisions for their child.\n\nMedical schools have increasingly recognized the need to educate students on social determinants of health—a broad term used to encapsulate key socioeconomic factors impacting health such as economic stability, access to education and health care, housing and food security.\n\nI am currently working with a Health Resources and Services Administration–supported national collaborative on efforts to identify and promote best practices for educating future health providers about social determinants of health and potential avenues to address them. This is an important first step towards a needed change in mindset from thinking about individual patient symptoms to including the broader social context in which our patients live.\n\nAcknowledging that implicit racial bias and overt racism contribute to poor health, experts who were part of a study to determine the most important topics to teach about social determinants of health called for curricula to include racism, discrimination and stigma as core elements. Education about racism and bias is important because unconscious bias may lead us to treat individuals of certain racial groups differently. Although unconscious biases often cannot be changed, awareness of our biases can lead us to exercise more care in how we act towards groups that are especially vulnerable to them.\n\nThe education of future medical providers is surely an important step in creating a future health care workforce that is sensitive to the impact of racism on health. Health institutions are moving this education into communities of practice to ensure that providers at all levels—including those who are training the future generation of health professionals—have knowledge of the impact of racism on health outcomes, can recognize the role of implicit bias as a subtle form of racism, and appreciate the importance of a diverse workforce.\n\nTraining at multiple levels through medical schools and hospitals has the potential to reach large numbers of health providers. However, training only in medical schools and hospitals is not enough. For meaningful change in health outcomes related to racism at the community level, efforts must extend beyond health care institutions.\n\nImplicit biases are present in young children as early as preschool. Yet, recent research also suggests that diversity in the school environment can have a positive influence on students’ health. Using health education as a model, schools across the educational continuum can be important partners in addressing bias and promoting tolerance to create the needed cultural shift over time.\n\nCorporate codes of conduct and professionalism need to include bias training for employees and leadership.\n\nPromoting recognition of the ill effects of bias and racism as adverse experiences—like recent efforts to implement a statewide trauma-informed awareness day—is an important step towards the development of a policy mindset that is sensitive to the health impacts of such experiences, particularly in young children.\n\nEven if we are not the direct recipients of overt racism, everyone must contribute to the discussion to change the social context in order to raise a healthier next generation.\n\nI understand now what I didn’t as a child: that racism is bad for everyone’s health.","title":"Racism Is Literally Bad for Our Health","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/E625B989-8A02-4D90-ADC840D70A939905_source.jpg?w=590&h=800&9C95111C-B5CF-4E6B-AAFDCF56DF9B787F","link":"https://blogs.scientificamerican.com/observations/racism-is-literally-bad-for-our-health/"},{"authors":"Amanda Baker","pub_date":"June 5, 2019","abstract":"School is winding down and eyes everywhere are turning to summer – whether days will be spent at camp, with family, on a new job, traveling, or sticking close to home. The months seem full of possibility. But flash forward to changing leaves and all of us will be asking, what did I even do this summer?\n\nSetting summer goals can help fight back against future fall regrets, especially if you take the time to get specific. Rather than planning to “learn guitar,” pick a song. Identify potential resources, whether books from the library, informal lessons from your neighbor, or how-to videos on the internet. Finally, don’t just say “this summer;” name some dates to check in and an ultimate deadline to check it off.\n\nThere are more ideas than summer days, but here are a few to get you started on a list of your own: \n\nGet Started on a New Skill\n\n\n\tLearn how to make three types of bread from scratch\n\tLearn how to crochet with 4 different stitch types\n\tGrow a new type of vegetable or flower\n\tSolve an old problem in a new programming language\n\tDraw or paint a picture to give away using a new type of paint or paper\n\tPlan out the route for a family trip or bike ride using a map in advance\n\n\nSet a Conservation Goal\n\n\n\tCut down your carbon footprint with shorter showers, bike rides, or shifting thermostats\n\tTry to reduce the amount of plastic you use in a week by half\n\tTrack your family’s food waste and compare new shopping plans\n\n\nFind New Ways to Help\n\n\n\tPick five neighbors to help by the end of the summer and find out what they need\n\tTake charge of three new things – mowing the lawn, cooking Wednesday dinner, vacuuming – to play a bigger role in your household\n\tLook for ten opportunities to help friends or family with something they find hard\n\n\nFinish Something\n\n\n\tWrite a story from start to finish and share it with someone\n\tPlan to walk at least a hundred miles, and then track your progress\n\tFinish that puzzle, model, or painting that you have been avoiding\n\tWrite and record a video about a local invasive species and share it with your neighborhood\n\tCreate and complete a reading list with new kinds of books – one fiction, one non-fiction, one by an author younger than 25, one translated from another language, one set in the past, one set in the future, and one picked by someone you just met\n\n\nSet Some Learning Goals\n\n\n\tI want to be able to name every plant in my yard\n\tI want know how to use every tool in my mom’s toolbox\n\tI want to understand all of the steps to design and build a birdhouse from scratch\n\tI want to be able to write a code for a robot that can start, stop, turn, and sense a wall\n\tI want to be able to give a five-minute presentation in front of people without any notes\n\n\nMake your list, make a calendar, and set some goals. Check-ins along the way will help you to see your progress and to re-assess plans that might have been too ambitious to start. Come September, you will be surprised at just how many new things you managed to do.","title":"Making the Most of Your Summer Break","origin":"Budding Scientist","image":"https://static.scientificamerican.com/blogs/cache/file/07834DAB-CDB9-4A93-B4B892ED7CD3AF69_source.jpg?w=590&h=800&AB898F1E-ADB5-45D7-B12EDC3FF049E8BC","link":"https://blogs.scientificamerican.com/budding-scientist/making-the-most-of-your-summer-break/"},{"authors":"Sara Whitlock","pub_date":"June 11, 2019","abstract":"It’s no secret that scientists like to argue—publishing competing theories, shouting ideas at conferences and even debating one another on Twitter. These arguments are useful for weeding out bad ideas. But every now and then, a worthwhile idea is discarded.\n\nThat’s what happened to an idea proposed almost a century ago by Soviet scientist Aleksandr Oparin about how life first formed on Earth. Now his idea is being resurrected, and arguments in favor of it are winning.\n\nOparin predicted that the first life forms were free-floating liquid clusters in a primordial soup of life’s building blocks: methane, ammonia and a mix of other organic substances suspended in water. He suggested these clusters spontaneously separated from their surroundings, like oil droplets in water, and grabbed building blocks as they passed by—making contact between key ingredients happen more easily inside the droplet than would happen outside. Extra contact between ingredients, he proposed, made chemical reactions possible that were necessary for life to form.\n\nOparin went on to perform basic chemical reactions inside test-tube droplets that would confirm his theory. His experiments showed that close proximity between ingredients inside the droplets could form a short sequence of RNA, which is one of the molecules that stores information in modern cells (similar to DNA). His idea was elegant and simple; it makes sense that crowded building blocks are more likely to interact than spread out ones, just like how people are more likely to talk on a crowded bus than when they can sit alone on an empty one.\n\nAnd where cells in bacteria, plants and our bodies have membranes or cell walls that define the inside and outside of the cell, Oparin’s clusters separate from their surroundings without boundaries. The droplets are more viscous than their environment—meaning they have the thick, gooey consistency of oil while their surroundings are like water. Different textures mean that those oil-like droplets favor interacting with each other over interacting with their watery surroundings. Primitive organisms that separated from their surroundings like this needed to evolve fewer parts than modern cells with membranes—an advantageous step in the formation of life.  \n\nUnfortunately, politics and a limited understanding of biology prevented the scientific community from embracing Oparin’s droplet ideas, no matter how elegant they were. At first, his ideas were published in Russian and didn’t get much attention outside his home country. Once they were translated, scientists around the world began to debate his work. But as he was a Soviet scientist, much of what he proposed was couched in Marxist philosophy, leading Western scientists to reject his ideas during the Cold War. And it was hard to understand how Oparin’s clusters would lead to evolution of a modern cell; parts of the droplets he used contained ingredients that aren’t present in our cells today. So, biologists forgot Oparin.\n\nFast-forward to 2018: two collaborating scientists, Jared Schrader of Wayne State University and Seth Childers of the University of Pittsburgh, noticed something interesting inside their favorite bacteria. The protein they were studying formed clusters inside the body of the bacteria—and they conducted experiments showing that these clusters are spontaneously separating liquid droplets like the clusters Oparin proposed as the first life forms.   \n\nSoon after, across the world in Germany, researchers at the Max Planck Institute of Biochemistry found more clusters. They were studying a different type of bacteria that performs carbon fixation—the process of taking carbon dioxide from the atmosphere and converting it into oxygen and sugar molecules that fuel the bacteria. When these researchers mixed carbon-fixing protein parts, they were surprised by what they found. The mixture clumped together, and they found out that the clumps were liquid droplets that spontaneously separate, just like Oparin’s clusters and those that Childers and Schrader found only a few months before.\n\nThese discoveries bring Oparin’s predictions back into modern debates about the origins of life. It’s easier to envision a free-floating droplet as the ancestor of life when modern cells use droplets inside their bodies to organize important reactions.\n\nFinding droplets inside bacterial cells is especially exciting for understanding how life began. Although the droplets researchers found in bacteria aren’t the first examples of droplets in modern cells—a Princeton scientist named Clifford Brangwynne saw them in worm cells a few years earlier— it stands to reason that those inside bacteria are closer relatives of a free-floating ancestral droplet than any in the cells of more complicated animals and plants. \n\nBut even bacterial droplets are many steps of evolution beyond Oparin’s free-floating ancestors. To fully understand early droplets, scientists need to study a large number of droplets across a variety of species. In doing so, they hope to identify features that are shared by all modern droplets. These might be inherited from a common, free-floating ancestor. For now, finding more droplets to compare shouldn’t be too challenging: the metabolic processes leading to droplet formation are performed by many types of bacteria, all of which could form droplets for scientists to compare.\n\nIt may turn out that Oparin was right—that somehow, a century before biological evidence supported his idea, he predicted a fundamental step in the evolution of life. The scientific community of Oparin’s day had reason to question his ideas because evidence was lacking. But the political agendas of Western scientists ensured he vanished from debates about the origin of life for years. Today, with new evidence and a more tolerant political climate, Oparin has a seat at the table once again. But before the scientific community fully confirms his ideas, the debate will continue.","title":"In Search of Life's Beginnings","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/3EA6DAEC-B59D-45B1-9E8CBEE9F8ED41C9_source.jpg?w=590&h=800&98F5B427-06BC-4353-8A985B363DC73A73","link":"https://blogs.scientificamerican.com/observations/in-search-of-lifes-beginnings/"},{"authors":"Morgan D. Bazilian","pub_date":"June 10, 2019","abstract":"The U.S. is 100 percent import-reliant on 14 minerals and metals that are essential for defense technologies, consumer goods and clean energy technology, and 50 percent or more reliant for another 30, according to the U.S. Geological Survey. These numbers go beyond the recent headlines on rare earths to illustrate fundamental building blocks of the energy transition: lithium, cobalt and nickel for batteries, and materials for solar power and wind turbines. In many of these areas, China has become the dominant world player. The issue is not geological resource constraints, but on whether domestic focus on mining production, processing and manufacturing should be prioritized.\n\nIn response to these trends, on May 2, 2019, Senator Lisa Murkowski of Alaska released her American Mineral Security Act into the Senate. This follows other, similar bills over the past few years. In the run-up to this one, she said that, \"energy and mineral security are the building blocks of a robust economy.\" The energy part of that equation is not new to the political realm.\n\nEnergy security has, for decades, been high on the agenda of politics and defense, but less so for minerals and metals. In April, Murkowski, along with Senators Joe Manchin and Shelley Moore Capito of West Virginia released a bill in a related genre—the Rare Earth Element Advanced Coal Technologies Act. Much of the framing of both bills is around fighting Chinese dominance.\n\nIn May, Chinese state media put out some statements indicating a ramp-up of language around using rare earths supply as a strategic counter to the Trump administration's latest tariffs. The \"tools\" China has, according to the article, include, \"cutting the number of rare-earth mining licenses, raising market access standards for miners, reducing exports of primary rare-earth products, and restricting outbound and inbound investment in related industries.\" The primary industries mentioned in these Chinese articles are linked to the defense industry, but rare earths are used in many other critical sectors including energy (think oil refining, electric vehicles and wind turbines).\n\nA recent article in Foreign Policy covers some of these issues in the context of an increasingly interconnected world. That essay notes the previous use, in 2010, of limiting rare earth exports to Japan. It is around that time that the price of rare earths spiked and were thrust squarely into the public's attention.\n\nAlso imminent is the final report on critical minerals in the U.S. under Executive Order 13817 of 2017. The principal impetus for that order is straightforward: \"The United States is heavily reliant on imports of certain mineral commodities that are vital to the Nation's security and economic prosperity. This dependency of the United States on foreign sources creates a strategic vulnerability for both its economy and military to adverse foreign government action, natural disaster, and other events that can disrupt supply of these key minerals.\" In early June, with little fanfare, the Department of Commerce issued a “A Federal Strategy to Ensure Secure and Reliable Supplies of Critical Minerals.” It usefully proposes six action areas across the value chain, including areas of trade, workforce education, R&D, trade, and domestic production.\n\nThese issues of supply threats, international relations, security and the related analysis is has been intensely analyzed and addressed in in energy policy—especially in relation to oil. The 1967 and 1973 oil embargoes had global ramifications, and helped usher in an era of oil security that we largely still occupy. The recent big change in energy security has been due to the shale revolution. The U.S. has become the largest producer of crude in the world, and one of the largest exporters of natural gas, through a combination of technology development and supportive regulations and policy. That trend has not gone unnoticed in the mining sector.\n\nAbout a year ago, in May 2018, the USGS came forward with a draft list of 35 critical minerals. Many of the minerals are essential for the defense or aerospace sectors, and of course, many for energy. The links to energy are increasing as the energy transition—a move toward a low-carbon, battery and solar energy–intensive energy system—continues apace. It also appears clear that whatever form the energy transition takes, it will be more mineral-intensive than in the past. \n\nBut how do we measure security or criticality in a meaningful way? The methodology used in the U.S. list essentially boils down to if it is deemed \"essential\" and if it is estimated to have a supply chain risk. Neither of those hurdles is precise, so proxies are used. That is typical in security assessments of all kinds.\n\nThe main proxies are import dependence and production concentration (measured by something called the Herfindahl-Hirschman index, or HHI). The methodology was developed in 2016, and updated in 2017. That used a geometric average of three indicators of supply risk, production growth and market dynamics.\n\nThe U.S. is not the only country or region to consider mineral criticality. Japan, the European Union and the Australians have all produced critical minerals lists. Interestingly, Australia's list is not focused so much on their own needs, but which minerals they have deposits of that others consider critical. The European Commission's list started in 2011, and has been revised three times since then. Its methodology, as with the U.S., looks at supply risk and economic importance, and also the Worldwide Governance Indicators.\n\nThere does not appear to be any use of the powerful precedents of the measurement efforts around energy security, which has evolved considerably since the 1970's and the various oil crises that took place in the following decades. There is likely something to be learned from both the political and technical approaches to measuring and addressing energy security for the minerals sector—especially now as they become more entwined than ever.\n\nWhat has become clear over decades of energy security analysis is that a reliance solely on import dependence does not account for the economic impacts of energy supply, nor many other factors, and thus is only one of many elements that need be considered for robust decision making in energy security and resilience. In its formal derivations, energy security requires a rigorous aggregation of dozens of variables that impact energy flows in the real world. One assessment, for example, noted at least 45 separate definitions of energy security presented in the academic and policy literature over the past decade.\n\nSeveral have used the \"four A's\" to describe energy security, namely: availability, reliability, affordability and sustainability. Availability refers to the ability for consumers and users to secure energy that they need. It requires an extensive commercial market, buyers and sellers trading goods, parties that agree on terms, as well as sufficient physical resources, investments, technology, and legal and regulatory frameworks to back them up. Others have gone \"beyond\" these four A's toward a definition of energy security as \"low vulnerability of vital energy systems.\" Most studies and metrics focus on the supply side, but supply side security needs are tightly coupled with the other pillars of energy policy and security, namely environmental considerations, governance and regulation, affordability and industrial competitiveness.\n\nThe future will likely bring more globally interdependent markets and systems. As a result, it is useful to further encourage new quantitative and qualitative approaches to the issues of security and criticality—in both minerals and energy. Additionally, some of the tools developed during the early oil shocks, such as the development of the International Energy Agency, the Strategic Petroleum Reserve, and sending the Navy's Fifth Fleet to protect key supply choke points (such as the Strait of Hormuz), are now being considered to protect access to critical materials.\n\nFinally, policy must take into consideration issues across the supply chain from raw materials through final manufacturing in an interconnected world. A narrow focus on domestic “dominance” will not be sufficient, nor useful in addressing mineral criticality. The lessons from the energy sector are attractive as an analogy—a thoughtful application in a very different sector is required.","title":"We Need to Get Serious about “Critical Materials”","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/A1956F64-C7E1-46B1-8495515464344129_source.jpg?w=590&h=800&D40CFEFD-6C5E-4FD9-9135060AE5247481","link":"https://blogs.scientificamerican.com/observations/we-need-to-get-serious-about-critical-materials/"},{"authors":"Evelyn Lamb","pub_date":"June 1, 2019","abstract":"Imagine that in a few hundred years, archaeologists stumble on some of your old files. Maybe they find spreadsheets of tax information, medical bills, or bank statements, or maybe text files with old emails or drafts of your novel. These archaeologists cannot read Latin script, and no one uses Hindu-Arabic numbers anymore. How do the archaeologists figure out what your files meant? Or can they at all?\n\nThe Inka* civilization, which flourished in the fifteenth and sixteenth century and was centered in what is now Peru, used sets of knotted cords called khipus to record information, both quantitative and narrative. Deciphering them has been a challenge, to say the least. I recently read Gary Urton’s book Inka History in Knots: Reading Khipus as Primary Sources, which collects and synthesizes khipu research from his 25 years studying the objects.** It is an invaluable resource for anyone interested in understanding khipus and, more importantly, understanding how and to what extent we can understand them. Scholars have learned to interpret quantitative khipus to a certain degree, but narrative khipus cannot be interpreted at this time.\n\nWhen I taught math history, I was excited to learn about khipus. When you teach a math history class, it is very easy to end up with a syllabus that covers only Hellenistic and Western European mathematicians and mathematicians. With a little more work, you can incorporate information about ancient Egypt and Mesopotamia and the Islamic world in the Middle Ages. After that, it’s not too hard to throw in some topics related to classical Chinese and Indian mathematics. But engaging with mathematics indigenous to sub-Saharan Africa, Oceania, or the Americas is difficult. Due to colonization by Europeans who believed native people to be intellectually and morally inferior to them and often actively demonized non-European ways of interacting with the world, much knowledge of mathematical thought from those parts of the world has been lost and will probably never be fully reconstructed.\n\nI did not end up teaching khipus in class, in part because of the difficulty I had finding good information about them beyond the very basics that they are knotted cords that can encode a base ten number system. But I did include them as a suggestion for student project topics, and I enjoyed reading student papers about them. Urton’s book would make it easier for a teacher who isn’t a khipu researcher to prepare material for a math history class, but if you are going into the endeavor hoping for a neat and tidy lesson with a satisfying bow on top, you might be disappointed. To me, the most important lesson from the book is about how to approach primary sources from foreign cultures and about the limitations of our understanding of these sources.\n\nMath students are used to seeing material almost as if it has emerged fully-formed from the head of Zeus (or Spivak). The beauty of using primary sources in a math or math history classroom is that they allow students to see how the ideas and understanding of topics changed over time. At its best, a math history class gets students to probe how we know what we know and how the way we understand mathematics and even what mathematical questions we ask are shaped by our culture.\n\nKhipus provide ample chances to ask those questions and take a peek into the archaeological and anthropological methods researchers use to study them. I was fascinated by Urton’s descriptions of where and how the khipus were found, and I appreciated Urton’s detailed explanations of what is known and clear delineations when he was moving towards material that was more speculative, as in the case of his interesting hypothesis that the demographic collapse of the Inka population due to the Spanish invasion is stored in khipus.\n\nKhipus record information in ways I found surprising. Aside from the numbers of knots and their positions on a khipu, the color, direction of twisting, and perhaps even fiber composition of the cords may also have had meaning. The concept of duality, which was important to the Inka people, may be encoded in khipu. If and when we are able to interpret narrative khipu, they may record stories or histories in ways that are completely unlike European linear, chronological narratives.\n\nFor more resources on khipus, check out the Khipu Database Project.\n\n* Inka is often spelled Inca, and khipu is often spelled quipu. I am taking Urton’s lead and using the spellings he does in the book. Inka and khipu are the Quechua spellings of these words. Quechua was the primary language of the Inka Empire and is still spoken in parts of the Andes mountains.\n\n**A previous version of this post misstated Gary Urton's name. I apologize for the error.","title":"Inka History in Knots (Book Review)","origin":"Roots of Unity","image":"https://static.scientificamerican.com/blogs/cache/file/F053A741-C357-4B11-8008CEE63E7DF810_source.jpg?w=590&h=800&133E0BD0-036E-4E55-AEB109C93566D468","link":"https://blogs.scientificamerican.com/roots-of-unity/inka-history-in-knots-book-review/"},{"authors":"Caleb A. Scharf","pub_date":"June 5, 2019","abstract":"As space-faring organisms, humans are pretty useless. We've evolved to be highly tuned to the constant acceleration of standing on an Earth-mass planet. Our bodies don't appreciate micro-gravity. It messes with our circulatory system, it makes us puffy, and it messes up our vision. Our cells also don't like the increased radiation environment of space very much, especially our neurons. All told, as the landmark Twin Study with the astronauts Mark and Scott Kelly has shown in detail, going into space causes a host of stress-like responses. Many of these reach down to our genome itself, as well as our immune system and gut microbiome.\n\nRadiation is a particular concern for any long-term Earth-orbiting presence, or for getting to places like Mars or asteroid targets. On these journeys the hazards from solar and cosmic particle radiation increases significantly from the environment of low Earth orbit, where the planetary magnetic field offers a modicum of protection.\n\nData from various robotic Mars missions and other investigations indicate that during transit to Mars an astronaut might be exposed to an average radiation dosage some 700 times that on Earth. Data from the ExoMars Trace Gas Orbiter, for instance, suggests that during a fairly typical 6 month journey to Mars an astronaut might receive about 60% of their lifetime radiation exposure back on Earth.\n\nFinding the right shielding that can protect people is challenging. Any space mission is limited by mass. More mass means you need more fuel for propulsion. But radiation shielding is also a matter of not making things worse. For example, when very high energy cosmic particles smack into material they can cause a burst of slower but devastating radiation in the form of particles like neutrons. You might be protected from the original cosmic radiation, but by trying to block it you've actually caused it to dump its energy right onto you instead of zipping on through.\n\nOne answer is for a layer of shielding to damp out the energy in all of those secondary particles. To do that it can actually help if the atomic nuclei in the shield material are similar in mass to (for example) damaging neutrons - absorbing energy in elastic nuclear collisions. That means that material with a high hydrogen content, such as organic compounds like polyethylene, should work better.\n\nBut figuring out exactly the right set of ingredients is tough, and requires experimentation. Intriguingly, one of the most interesting avenues is the use of lithium compounds. A paper published in February 2019 by Schuy and colleagues presents the results of a study of lithium hydride (LiH). What they find is promising, with evidence that lithium hydride can attenuate particle radiation over distances 20% shorter than for polyethylene. \n\nThe idea of using lithium hydride as a radiation shield in space is not entirely new; it was discussed back in the 1970s. It's unpleasant material, highly reactive to water in air. But it could also be useful stuff for a developing Mars settlement. LiH can be a starting compound for making lithium aluminum hydride, used as a reagent in organic synthesis. It also releases hydrogen in its reaction with water. And lithium is of course a critical ingredient in modern battery technology, as well as numerous other industrial processes. \n\nLithium hydride shields might help you get to Mars in one piece, and set up a fledgling society when you get there.","title":"Deep-Space Shielding","origin":"Life, Unbounded","image":"https://static.scientificamerican.com/blogs/cache/file/B522B574-5390-460D-9D088C60A255D155_source.jpg?w=590&h=800&277095B3-F219-4016-883EB673AB92112B","link":"https://blogs.scientificamerican.com/life-unbounded/deep-space-shielding/"},{"authors":"Rachel Nuwer","abstract":"Nearly four decades ago zoologist Michael Thompson, then at the University of Adelaide in Australia, made an alarming discovery: invasive red foxes were gobbling up more than 90 percent of all the turtle eggs laid along the banks of Australia’s Murray River. Thompson’s surveys also revealed a disproportionate number of older turtles, suggesting that fox predation had already reduced the amount of juveniles in the river. If no one took action, he warned, the formerly abundant turtles would eventually disappear.\n\nVery little was done, and Thompson’s prediction now appears to be on its way to coming true. A recent study confirms that several turtle species have either drastically declined or disappeared from various sections of the Murray River. “The problem is that the longevity of turtles gives the perception of persistence,” says Ricky Spencer, an ecologist at Western Sydney University and a co-author of the study, which was published in February in Scientific Reports. “It’s human nature that only when something is gone do we start missing it.”\n\nSpencer and his colleagues tallied populations of three once common turtle species—the broad-shelled turtle, the eastern long-necked turtle and the Murray River turtle—at 52 sites along the southern reaches of the river. The researchers inferred the species’ population sizes from the number of individuals they trapped in a given amount of time. They found the turtles have been extirpated in places where they were previously abundant, and most of the specimens they managed to capture elsewhere were large—and likely old—adults. Spencer and his colleagues blame the losses on ongoing nest predation by foxes, compounded by other problems, including environmental degradation and severe drought in the 2000s.\n\n“We have known about [the turtle die-off] for decades, and despite intense media hype in Australia about the ‘plight of our rivers,’ nothing has been done to reverse that decline,” says Rick Shine, a herpetologist at Macquarie University in Sydney, who was not involved in the research. “This paper is a wake-up call that unless we begin to do something about turtle conservation on a landscape scale, we may lose a fascinating component of our native fauna.”\n\nThe turtles could recover quickly if action is taken to protect nests from foxes and restore habitat, Spencer notes. But governments tend to respond only when losses reach crisis levels, and the Murray River species currently lack federal protection, he says. He and his colleagues have a work-around, however: “Our next step is to start designing community conservation efforts for common turtle species,” he explains, “so people can actually do things without having to wait for government funding.”","title":"As Predicted, Some of Australia’s Turtles Are Going Extinct","origin":"Conservation","image":"https://static.scientificamerican.com/sciam/cache/file/7D6B085D-AB07-4C57-90B7A4217FE44A43_source.jpg?w=590&h=800&F3D07009-BDA7-4D82-B990F91C0514CE64","link":"https://www.scientificamerican.com/article/as-predicted-some-of-australias-turtles-are-going-extinct1/"},{"authors":"David Cyranoski, Nature magazine","pub_date":"June 11, 2019","abstract":"A Russian scientist says he is planning to produce gene-edited babies, an act that would make him only the second person known to have done this. It would also fly in the face of the scientific consensus that such experiments should be banned until an international ethical framework has agreed on the circumstances and safety measures that would justify them.\n\nMolecular biologist Denis Rebrikov has told Nature he is considering implanting gene-edited embryos into women, possibly before the end of the year if he can get approval by then. Chinese scientist He Jiankui prompted an international outcry when he announced last November that he had made the world’s first gene-edited babies—twin girls.\n\nThe experiment will target the same gene, called CCR5, that He did, but Rebrikov claims his technique will offer greater benefits, pose fewer risks and be more ethically justifiable and acceptable to the public. Rebrikov plans to disable the gene, which encodes a protein that allows HIV to enter cells, in embryos that will be implanted into HIV-positive mothers, reducing the risk of them passing on the virus to the baby in utero. By contrast, He modified the gene in embryos created from fathers with HIV, which many geneticists said provided little clinical benefit because the risk of a father passing on HIV to his children is minimal.\n\nRebrikov heads a genome-editing laboratory at Russia’s largest fertility clinic, the Kulakov National Medical Research Center for Obstetrics, Gynecology and Perinatology in Moscow and is a researcher at the Pirogov Russian National Research Medical University, also in Moscow.\n\nAccording to Rebrikov he already has an agreement with an HIV centre in the city to recruit women infected with HIV who want to take part in the experiment.\n\nBut scientists and bioethicists contacted by Nature are troubled by Rebrikov’s plans.\n\n“The technology is not ready,” says Jennifer Doudna, a University of California Berkeley molecular biologist who pioneered the CRISPR-Cas9 genome-editing system that Rebrikov plans to use. “It is not surprising, but it is very disappointing and unsettling.”\n\nAlta Charo, a researcher in bioethics and law at the University of Wisconsin-Madison says Rebrikov’s plans are not an ethical use of the technology. “It is irresponsible to proceed with this protocol at this time,” adds Charo, who sits on a World Health Organization committee that is formulating ethical governance policies for human genome editing.\n\nRules and regulations\n\nImplanting gene-edited embryos is banned in many countries. Russia has a law that prohibits genetic engineering in most circumstances, but it is unclear whether or how the rules would be enforced in relation to gene editing in an embryo. And Russia’s regulations on assisted reproduction do not explicitly refer to gene editing, according to a 2017 analysis of such regulations in a range of countries. (The law in China is also ambiguous: in 2003, the health ministry banned genetically modifying human embryos for reproduction but the ban carried no penalties and He’s legal status was and still is not clear).\n\nRebrikov expects the health ministry to clarify the rules on the clinical use of gene-editing of embryos in the next nine months. Rebrikov says he feels a sense of urgency to help women with HIV, and is tempted to proceed with his experiments even before Russia hashes out regulations.\n\nTo reduce the chance he would be punished for the experiments, Rebrikov plans to first seek approval from three government agencies, including the health ministry. That could take anywhere from one month to two years, he says.\n\nKonstantin Severinov, a molecular geneticist who recently helped the government design a funding programme for gene-editing research, says such approvals might be difficult. Russia’s powerful Orthodox church opposes gene editing, says Severinov, who splits his time between Rutgers University in Piscataway, New Jersey, and the Skolkovo Institute of Science and Technology near Moscow.\n\nBefore any scientist attempts to implant gene-edited embryos into women there needs to be a transparent, open debate about the scientific feasibility and ethical permissibility, says geneticist George Daley at Harvard Medical School in Boston, Massachusetts, who also heard about Rebrikov’s plans from Nature.\n\nOne reason that gene-edited embryos have created a huge global debate is that, if allowed to grow into babies, the edits can be passed on to future generations—a far-reaching intervention known as altering the germ line. Researchers agree that the technology might, one day, help to eliminate genetic diseases such as sickle-cell anaemia and cystic fibrosis, but much more testing is needed before it is used in the alteration of human beings.\n\nIn the wake of He’s announcement, many scientists renewed calls for an international moratorium on germline editing. Although that has yet to happen, the World Health Organization, the US National Academy of Sciences, the UK’s Royal Society and other prominent organizations have all discussed how to stop unethical and dangerous uses—often defined as ones that pose unnecessary or excessive risk—of genome editing in humans.\n\nHIV-positive mothers\n\nAlthough He was widely criticized for conducting his experiments using sperm from HIV-positive fathers, his argument was that he just wanted to protect people against ever getting the infection. But scientists and ethicists countered that there are other ways to decrease the risk of infection, such as contraceptives. There are also reasonable alternatives, such as drugs, for preventing maternal transmission of HIV, says Charo.\n\nRebrikov agrees, and so plans to implant embryos only into a subset of HIV-positive mothers who do not respond to standard anti-HIV drugs. Their risk of transmitting the infection to the child is higher. If editing successfully disables the CCR5 gene, that risk would be greatly reduced, Rebrikov says. “This is a clinical situation which calls for this type of therapy,” he says.\n\nMost scientists say there is no justification for editing the CCR5 gene in embryos, even so, because the risks don’t outweigh the benefits. Even if the therapy goes as planned, and both copies of the CCR5 gene in cells are disabled, there is still a chance that such babies could become infected with HIV. The cell-surface protein encoded by CCR5 is thought to be the gateway for some 90% of HIV infections, but getting rid of it won’t affect other routes of HIV infection. There are still many unknowns about the safety of gene editing in embryos, says Gaetan Burgio at the Australian National University in Canberra. And what are the benefits of editing this gene, he asks. “I don’t see them.”\n\nHitting the target\n\nThere are also concerns about the safety of gene editing in embryos more generally. Rebrikov claims that his experiment—which, like He’s, will use the CRISPR-Cas9 genome-editing tool—will be safe.\n\nOne big concern with He’s experiment—and with gene-editing in embryos more generally—is that CRISPR-Cas9 can cause unintended ‘off-target’ mutations away from the target gene, and that these could be dangerous if they, for instance, switched off a tumour-suppressor gene. But Rebrikov says that he is developing a technique that can ensure that there are no ‘off-target’ mutations; he plans to post preliminary findings online within a month, possibly on bioRxiv or in a peer-reviewed journal.\n\nScientists contacted by Nature were sceptical that such assurances could be made about off-target mutations, or about another known challenge of using CRISPR-Cas 9—so-called ‘on-target mutations’, in which the correct gene is edited, but not in the way intended.\n\nRebrikov writes, in a paper published last year in the Bulletin of the RSMU, of which he is the editor in chief, that his technique disables both copies of the CCR5 gene (by deleting a section of 32 bases) more than 50% of the time. He says publishing in this journal was not a conflict of interest because reviewers and editors are blinded to a paper’s authors.\n\nBut Doudna is sceptical of those results. “The data I have seen say it’s not that easy to control the way the DNA repair works.” Burgio, too, thinks that the edits probably led to other deletions or insertions that are difficult to detect, as is often the case with gene editing.\n\nMisplaced edits could mean that the gene isn’t properly disabled, and so the cell is still accessible to HIV, or that the mutated gene could function in a completely different and unpredictable way. “It can be a real mess,” says Burgio.\n\nWhat’s more, the unmutated CCR5 has many functions that are not yet well understood, but which offer some benefits, say scientists critical of Rebrikov’s plans. For instance, it seems to offer some protection against major complications following infection by the West Nile virus or influenza. “We know a lot about its [CCR5’s] role in HIV entry [to cells], but we don't know much about its other effects,” says Burgio. A study published last week also suggested that people without a working copy of CCR5 might have a shortened lifespan.\n\nRebrikov understands that if he proceeds with his experiment before Russia’s updated regulations are in place, he might be considered a second He Jiankui. But he says he would only do so if he’s sure of the safety of the procedure. “I think I’m crazy enough to do it,” he says.\n\nThis article is reproduced with permission and was first published on June 10, 2019.","title":"Russian Biologist Plans More CRISPR-Edited Babies","origin":"Medical & Biotech","image":"https://static.scientificamerican.com/sciam/cache/file/3F650D59-B553-4F7C-AFB737D0C1744217_source.jpg?w=590&h=800&839A52FB-B07F-4511-AA178B684BE44BC0","link":"https://www.scientificamerican.com/article/russian-biologist-plans-more-crispr-edited-babies/"},{"authors":"Caterina Gawrilow, Sara Goudarzi","pub_date":"June 11, 2019","abstract":"Those affected by attention deficit hyperactivity disorder (ADHD) are clinically thought of as inattentive, hyperactive and impulsive. However, people with ADHD are also perceived as being very spontaneous, curious, inquisitive, enthusiastic, lively and witty, a perception that creates an impression they are more creative than those without ADHD. But is there truth to this idea?\n\nCreativity is generally the ability to generate something original and unprecedented. The ideas must not only be new and surprising, but also useful and relevant. Among other things, creativity comes through intensive knowledge and great motivation in a particular field, be it painting, music or mathematics.\n\nFor years, both laypersons and scientists have been fascinated by the proverbial proximity of genius and madness. According to psychologist Dean Keith Simonton from the University of California, Davis, unusual and unexpected experiences, such as psychological difficulties and psychiatric stays, are an important characteristic of people who create masterpieces.\n\nTwo core symptoms, inattention and impulsiveness, suggest a connection between creativity and ADHD. Inattention, which occurs more frequently in those affected with the disorder, likely leads to mind wandering, or the drifting of thoughts from an activity or environment. Such drifting can lead to new, useful and creative ideas.  \n\nIn a study, detailed in the Journal of Creative Behavior, researchers asked 26 college students with ADHD and 26 without ADHD to perform two creativity tests. The first involved inventing and drawing alien fruit without copying those on Earth. Students with ADHD were able to create more unique fruit. Similarly, when asked to invent product labels, those with ADHD were able to come up with less conventional names.\n\nPeople with ADHD are also more impulsive and therefore more willing to take risks: they dare to approach new things and situations without fear of contact. In a 2011 study, 203 five- to 10-year-old children participated in a Balloon Analogue Risk Task (BART) experiment. BART is a computerized test where participants can blow up balloons one click at a time. Each time a balloon is inflated, participants earn money. If, however, the balloon pops, participants lose their earnings. Researchers found that those with ADHD pumped more than the control group—meaning they took greater risks than those without ADHD. Those with both ADHD and oppositional defiant disorder (ODD), children with defiance towards authority figures, pumped the most of all groups.  Even at primary school age, teachers perceive more impulsive children as more curious. This is likely to create more learning opportunities for these students, which in turn could enhance their creativity.\n\nADHD is, however, a highly heterogeneous disorder. Not only are there large differences between affected individuals, but the symptoms are also not always the same in patients. In addition, cognitive performance fluctuates from person to person. For example, for some with ADHD, the disease has a flip side in that they possess the ability to focus intensely on one single thing, when interested. A 2018 study showed that adults with ADHD had higher, and more frequent episodes of, hyperfocus when it came to hobbies, school and screen time. Similar to mind wandering, this ability is also extremely beneficial for creative or artistic tasks.\n\nNonetheless, research into the relationship between ADHD and creativity to date has painted an inconsistent picture, partly due to the fact that it is more difficult to understand creativity using psychological tests than it is to comprehend, say, intelligence. So far though studies that are of higher quality and involve sufficient test subjects do not provide clear evidence that people with ADHD are actually the better lateral thinkers.","title":"Are People with ADHD More Creative?","origin":"Behavior & Society","image":"https://static.scientificamerican.com/sciam/cache/file/924A03E2-9D4A-49C4-A013F8AE15DE2D32_source.jpg?w=590&h=800&FBEC82AE-3F07-45EF-922A948126474B00","link":"https://www.scientificamerican.com/article/are-people-with-adhd-more-creative/"},{"authors":"Tara Haelle","pub_date":"June 11, 2019","abstract":"Health organizations recommend children receive the varicella vaccine at one year old to protect them against chickenpox, but the vaccine appears to have another benefit: it cuts the risk of shingles, a painful and potentially debilitating rash caused by the reactivated chickenpox virus, by more than half in children over two years old, according to a new study.\n\nApproximately 38 per 100,000 children vaccinated against chickenpox developed shingles per year, compared with 170 per 100,000 unvaccinated children, researchers found. Furthermore, shingles infection rates were lower in children who received both recommended doses of the chickenpox vaccine compared with those who only got the first dose.\n\nChickenpox, a once common childhood virus that causes fever and a rash lasting up to a week, rarely causes death in children. Before the vaccine, two to three out of every 1,000 U.S. children who got the disease were hospitalized, and approximately 100 children a year died from it. It is often more severe in adolescents and adults.\n\nSerious complications such as infection and brain inflammation can occur, as well as permanent scarring, but the bigger threat from chickenpox is what can happen years later. After an infection, the varicella virus remains latent in nerve roots and can reactivate to cause shingles, which typically strikes decades later and can cause severe long-term nerve pain or vision loss. Formally called herpes zoster, the disease infects about a third of people who have had chickenpox, usually showing up in older adulthood, according to the U.S. Centers for Disease Control and Prevention. Shingles infection rates have been increasing for more than two decades. Although the risk increases with age, children can develop it as well, especially if their immune system is weakened.\n\nAbout 91 percent of U.S. children are vaccinated against chickenpox, according to the most recent National Immunization Survey data, but that does not necessarily mean they cannot get shingles. The chickenpox vaccine is made with the live attenuated (weakened) varicella virus, so “not surprisingly, it can also become latent after vaccination,” explains Anne A. Gershon, a professor of pediatric infectious disease at Columbia University. “The virus has been altered so the vaccine rarely causes symptoms, but once you’ve been immunized and after the natural infection, you carry the virus in your neurons for the rest of your life,” says Gershon, who wrote an editorial accompanying the new study, which was published Monday in Pediatrics, and who was not involved in the work.\n\nPrevious research with small groups found conflicting results regarding shingles rates in children vaccinated against chickenpox, with lower rates in older children but higher rates in toddlers. In the new study, researchers analyzed the medical records of nearly 6.4 million children (ranging from newborns to 17-year-olds) who received care at six health care organizations in the West, Northwest and Midwest from 2003 to 2014. They looked at records from the child’s birth or entry into the health system up until age 18 (or leaving the system), so any shingles infections after age 18 were not included. Half the children were vaccinated for at least part of the full study period; the other half were not.\n\nThe authors found that one dose of vaccine reduced shingles infection by 78 percent—except in young toddlers. Shingles rates were significantly higher in vaccinated one-year-olds than unvaccinated ones, although this increased risk for vaccinated children vanished by age two. The authors suspect the higher risk in toddlers “could be related to the developing immune system in very young children,” says lead study author Sheila Weinmann, a senior investigator at the Center for Health Research, Kaiser Permanente Northwest in Portland, Oregon.\n\nThat does not mean delaying the vaccine past the recommended age of one year for the first dose is wise, she added. The longer children go without the first vaccine dose, the more likely they are to catch the wild chickenpox virus—“and maybe even pass it on to young infants who are too young to get vaccinated,” Weinmann says. “So it probably makes more sense to stick with the current recommendation.” (Three of Weinmann’s co-authors have received research funding for other studies from the pharmaceutical company Merck, which manufacturers the varicella vaccine.)\n\nEven unvaccinated children appear to be benefiting from the vaccine’s use. Despite a brief shingles uptick in unvaccinated children from 2003 to 2007, overall rates in children declined by 72 percent from 2003 to 2014. Four years after the CDC began recommending the second varicella vaccine dose in 2006, shingles cases in unvaccinated children began dropping rapidly, likely because of herd immunity, Weinmann says. Herd immunity refers to the inability of a disease to travel easily through a highly vaccinated population. In this case, herd immunity’s effect on shingles rates would occur by protecting unvaccinated children from developing wild chickenpox in the first place, thereby preventing shingles later on. As they grow older, however, unvaccinated children would remain susceptible to chickenpox (and therefore shingles).\n\n“This study makes it clearer than ever before that the benefits of the varicella vaccine go beyond simply preventing chickenpox,” says Nathan Boonstra, a general pediatrician at Blank Children’s Hospital in Des Moines, Iowa, who cohosts the podcast Vax Talk and was not involved in the study. “There’s very good evidence now that the vaccine prevents a serious complication of chickenpox down the road, and shingles is really awful,” especially since it can show up anywhere on the skin, including the face and eyes, he notes. This study’s large population size and 12-year duration, as well as the big difference in infection rates it found, will also help doctors explain the vaccine’s benefits to parents, Boonstra says.\n\nTwo vaccines exist against shingles: Zostavax for adults age 60 and older, and the much more effective Shingrix, approved in 2017, for adults age 50 and older. But it is not yet clear if children vaccinated against chickenpox will need a shingles vaccine in older adulthood. “We need to continue to follow a cohort of children who have been vaccinated and see what happens,” Gershon says, although she expects shingles will be less of a problem for them. There are not much data on adult shingles rates in the study group yet because the CDC first recommended the vaccine in 1996, so the first generation to receive it is currently in their early 20s. Shingles becomes much more common after age 50.\n\nNevertheless, the fewer children who are getting chickenpox in the first place, the fewer are likely to develop shingles later on. “Because vaccination coverage in the population has been increasing over time,” Weinmann says, “probably these [shingles] rates will continue to drop.”","title":"Two-for-One: Chickenpox Vaccine Lowers Shingles Risk in Children","origin":"Medicine","image":"https://static.scientificamerican.com/sciam/cache/file/152CE75B-4783-4EC2-BC853705C5C436EB_source.jpg?w=590&h=800&4711F5C7-58E2-42F7-82AC32E7587735DA","link":"https://www.scientificamerican.com/article/two-for-one-chickenpox-vaccine-lowers-shingles-risk-in-children/"},{"authors":"Sophie Bushwick","pub_date":"June 12, 2019","abstract":"Watching a wildfire from within—seeing the flames fly above, below, and for 360 degrees all around—can offer scientists and firefighters valuable information about how these increasingly threatening blazes behave and spread. But the average forest fire sends temperatures rocketing up past 800 degrees Celsius (1,472 degrees Fahrenheit), hot enough to cremate a human or melt a camera. So, researchers at the National Institute of Standards and Technology (NIST) recently surrounded their equipment with a coolant that also happens to filter damaging infrared light: water. This let them capture completely surrounding footage of fires like the controlled burn in the video featured here, in the New Jersey Pine Barrens. Viewers can simply click the screen and drag the cursor to shift their viewpoint.\n\nIn the NIST setup, a waterproof camera sits in a liquid-filled bulb made of heat-resistant glass. “It’s exactly like a snow globe with a camera in the middle,” says the system’s developer, Matthew Hoehler, a research structural engineer at NIST’s National Fire Research Laboratory. Pipes connect the glass to a buried water tank that acts as a heat sink: As the flames heat the liquid, it is pumped out of the bulb and replaced with cool water from the tank. This not only prevents the camera from overheating; it also filters out thermal radiation. “Even if you could cool the camera in some way with air or liquid nitrogen, you still have this infrared radiation from a fire, which is very intense and is problematic for laser optics and camera sensors,” Hoehler says. Water blocks wavelengths in the infrared range while allowing visible light to pass through.\n\n\n\nBecause it takes time to set up this system, Hoehler can only use it when he knows he has two to three hours before a fire reaches his site. For example, because the blaze at the New Jersey Pine Barrens was a “prescribed burn” (deliberately set to help with forest management), the fire service was able to tip off NIST team members well before the fire started. This gave the researchers plenty of time to set up their equipment and bury the water tank. In the future, Hoehler hopes to develop an aboveground version that would only take 20 to 30 minutes to deploy, allowing him to film actual wildfires.\n\nAlthough researchers already observe ongoing fires with drones in the air and measuring devices on the ground, they had not shot 360-degree videos like this before. “This is really the first thing that’s able to capture the photos and the real-time video imagery in the way that it does with the three-dimensional perspective,” says Robin Verble, associate professor of biological sciences and director of the Ozark Research Field Station at Missouri University of Science and Technology, who was not involved with the NIST research. “Being able to see inside and get an interior perspective will be immensely useful.” For example, she suggests that this view will help researchers study a fire’s inner “weather” and test existing models of fire severity.\n\nThe videos could also serve as educational tools for the public. Verble points out that they can teach people how fire spreads. “The general public has a lot of lot of misconceptions,” she says, “so anything that can engage a general audience would be really beneficial.” To make them even more informative, Hoehler wants to augment the videos by layering data about temperature and air flow on top of the visuals. “The videos are neat, and I think they’re compelling for people,” he says. “But it’s also a more modern way of communicating data from complex experiments.”","title":"Watch a Raging Forest Fire Surround You in 360 Degrees","origin":"Natural Disasters","image":"https://static.scientificamerican.com/sciam/cache/file/C1CBDCD7-0471-4C47-846607ADAF8585F4_source.jpg?w=590&h=800&FC18A8C1-CEC1-456D-ACBEB444E6E52C9F","link":"https://www.scientificamerican.com/article/watch-a-raging-forest-fire-surround-you-in-360-degrees/"},{"authors":"Lee Billings","pub_date":"June 10, 2019","abstract":"On July 20, 2019, a half-century will have passed since Apollo 11 astronauts Neil Armstrong and Edwin “Buzz” Aldrin became the first humans to walk on the moon. More than just an excuse to celebrate an epochal achievement, the 50th anniversary is also an opportunity to reflect on the Apollo program’s complex origins and legacy—and on how lunar exploration in general has changed our understanding not only of the moon, but also of Earth and ourselves.\n\nTo that end, a huge number of commemorative media and memorabilia are already appearing on screens and shelves around the world, with even more to follow in coming months. Of the books in this overwhelming flood, one stands out for the understated elegance of its prose and the profoundly wide-angle view it offers of its subject: Oliver Morton’s The Moon: A History for the Future. Only one of the book’s eight chapters is explicitly devoted to the Apollo missions, but the tome, in its entirety, places humanity’s lunar forays into new, thought-provoking contexts guaranteed to surprise and delight even the most knowledgeable space buff.\n\nScientific American spoke with Morton, a writer and editor at the Economist, about the motivations for future lunar voyages, how to responsibly conduct them and why the moon should make us all reconsider what it means to live on Earth.\n\n[An edited transcript of the conversation follows.]  \n\nWhy write this book right now? Is it just the 50th anniversary of the first human lunar landing, or is it more than that?\n\nIt’s two things. The 50th anniversary of Apollo 11 is important, particularly for people like me in their mid-50s, because it’s quite remarkable to realize that of all the things we thought back then about the extraordinary future, one thing we didn’t think about was that by 1972 human journeys to the moon would be over and that no one would go back. But the other thing, of course, is that it’s quite clear now that people are going to go back. I believe there are more people on Earth today who will walk on the moon than who have walked on the moon.\n\nThere are many proffered reasons for going back: doing interesting science or the possibility of using resources there. And of course, there’s the matter of “great power” geopolitics and the symbolism involved in being there, overhead in the skies of everyone on Earth. I don’t exactly applaud that, but I can see the reality of it. But when it comes down to it, the real reason for going back is that people in general have more power now, and getting to the moon is less difficult than it used to be. In the 1960s it took the supreme efforts of the world’s preeminent superpower to put people on the moon. And that’s just not the case anymore. The attitude is shifting from being “Why go to the moon?” to “Hey, why not?”\n\nSo it’s worth thinking again about what it is that the moon means to people and what it could come to mean to people as we return.\n\nWell, what does the moon mean to you? You discuss the spectrum of attitudes toward the moon a great deal in the book: Some people want the “sky moon”—just something to see in the sky. Others want the “rock moon,” an object to be scientifically studied or mined for resources. Or “a moon that is at one with their Earth,” a place one might routinely visit that, although exotic, is really not so out of this world. Which moon do you want?\n\nI am somewhat confused about the moon. But I think what most fascinates me about the moon is its sheer unworldliness, the way it makes you think about [what] it is to be a world like Earth, and [what] the moon is deprived [of]. Thinking about the moon hard made me realize how extraordinary it is that on Earth, if you put something down, Earth will move it away from you—wind will blow it away, rain will wash it away. Eventually, a mountain range will rise up, or a sea will open, and that something will tumble down. On the moon—unless it’s unhappy enough to be at ground zero for another asteroid strike—you put something down, and it stays down. Nothing much happens. The lack of anything on the moon really puts one’s sense of what it is to be a “world” into question. A question that simply speculating about one’s feet on the moon, or about the moon becoming something other than a rock in the sky, doesn’t quite reach.\n\nEarlier this year, Vice President Mike Pence announced NASA is going to somehow get U.S. astronauts back to the lunar surface by 2024. What do you think about that? Who do you think will be there next, and when and why?\n\nWhat I think is that things are moving considerably faster than I would have expected when I began writing this book! I believe the next people to land on the moon will probably be American. Quite how they’ll do so, I’m not sure. It’s fairly unlikely that NASA will do it by 2024, as Pence suggested, partly because NASA has various handicaps in the “resource” sense of the word—it is carrying unnecessary weight in terms of being required to use a very large, very expensive, as yet unfinished NASA-developed booster, the Space Launch System, rather than alternatives such as SpaceX’s Falcon Heavy or perhaps the new rocket being developed by Jeff Bezos’s Blue Origin, the New Glenn. And I think that's a genuine problem for NASA, as well as this idea of building a little space station—the lunar Gateway—around the moon before going back down to the surface, which is not something that has a great deal of support outside of NASA and the contractors who are building this thing.\n\nMeanwhile the Chinese seem to be planning to go, too, but they are in no rush. It would be a significant effort, and I think China sees human lunar landings as something that would just be “nice to have.” But China’s interest kind of forces America’s hand, in that there is a symbolism to being the first on the moon that is lost if someone else goes up there, and you’re not there, too. There’s a real aspect of “great power” rivalry here.\n\nOne thing I enjoyed about the book was your unflinching discussion of the profound social inequalities often associated with space exploration. You grapple with a perennial criticism of the Apollo program—that it was an overly expensive distraction from more pressing problems on Earth. And you write about how those missions and the prosperity that made them possible in the first place are inseparable products of historical injustices, from the obliteration of Native American populations to the slave trade.\n\nYes, it’s important to remember that Apollo was not universally popular, even among Americans, even at the time. I suppose the most famous example is Gil Scott-Heron’s song “Whitey on the Moon.” You know, “A rat done bit my sister Nell, with Whitey on the moon.” Hugely though I respect NASA’s astronauts of the 1960s and 1970s, they were all middle-aged white men, mostly from the officer class—that’s not “humanity” as the term is usually construed! One uncontestably interesting thing about a return to the moon is the opportunity it presents for more of “humanity”—women, people of color, people of developing nations, people of a wider range of ages, and so on—to actually go there.\n\nAnd the idea that Apollo was a distraction from Earth is quite a strong one, particularly in the context of global ecological change, climate change especially. But being able to go into space helped alert people to those problems. At the same time, if all you can do with the moon is watch Earth heat up from a distance, that’s not so great. One could argue, and I might, that sending humans to the moon is still too expensive—but it’s a tiny fraction of what we spend on many other things, and what we should be spending on problems such as climate change. If I had to choose between spending really effectively on climate change or spending profligately on missions to the moon, well, I’d be hard-pressed to choose the moon. But I don’t think that’s really the choice the world is facing at the moment. I don’t think the costs of human missions to the moon and of dealing with climate change are remotely of the same scale.\n\nSomewhat relatedly, then, do we need to be concerned about protecting the environment of the moon? If so, how?\n\nI’d like people to plan and perform their lunar missions in ways that don’t leave behind a terrible amount of mess. At the same time, the amount of mess that humans could make on the moon, compared with the messes we can and do make on Earth, is always going to be absolutely trifling. For the time being, I’d certainly suggest that people avoid visiting the obvious heritage sites, such as the Apollo 11 and Apollo 17 landing locations.\n\nThe situation has become more complex since the days of Apollo, though, because there’s now a strong consensus that interesting volatiles—water ice, in particular—are stored in shadowed craters at the moon’s poles. That water ice could be used for, among other things, producing rocket fuel, which has many people excited. I’d like to see some discussion of an international agreement to cover the use of those potential resources, because right now, I don’t believe there are any meaningful constraints on what anyone can do with them. I wouldn’t want anything needlessly punitive, and we don’t need every molecule of ice that’s ever settled in a crater to be preserved as is. But the discussion is important, because we don’t really know yet the extent of the water-ice deposits there, and we also don’t know the trade-off between using them as a physical resource versus using them as a scientific resource. We have no real sense yet of what information from lunar and even earthly history is stored in those ices.\n\nSpeaking of science, what do you think would be the most compelling scientific reason to go to the moon now?\n\nTo me, the most compelling thing is the possibility of finding samples from the very early Earth on the moon. Some scientists have called the moon “Earth’s attic,” because for billions of years, it has been collecting material ejected from our planet by impacts and other processes. The arguments for all this remain somewhat theoretical, but there really should be quite a significant number of extremely old Earth rocks up there, on the lunar surface, from parts of our planet’s history we can’t otherwise directly study. Similarly, there might be a much smaller amount of rocks from early Venus there, from back when that world may have been much more Earth-like, which would be really fascinating to study. And frankly, it’s much easier to gather up and sort through moon rocks by the ton than to retrieve any rocks at all from present-day Venus, the surface of which is very hard to get to and even harder to return from.\n\nI also find something poetic and scientific about the notion of doing radio astronomy from the moon’s far side, which, because it always faces away from Earth, is the only place within light-years where such observations could be unaffected by our planet’s electromagnetic babble. There are radio-based studies of the early universe that, at the moment, scientists can only imagine performing from that vantage point. Most of my thinking about the moon involves using it to gaze back at, and better understand, Earth, so the idea that it could be a platform for looking farther out to the universe’s beginnings is one that similarly pleases me.","title":"Interview: The Once and Future Moon","origin":"Space","image":"https://static.scientificamerican.com/sciam/cache/file/FE8E3A9B-36C8-4223-9D6D4188E688C2A9_source.jpg?w=590&h=800&528A8EBE-ECE3-4D49-A361985755A77B77","link":"https://www.scientificamerican.com/article/interview-the-once-and-future-moon/"},{"authors":"Chelsea Harvey, E&E News","pub_date":"June 11, 2019","abstract":"A strange event occurred off the Antarctic coast in the dark midwinter of 2016. A gaping hole opened in the middle of the sea ice on the Weddell Sea, eventually expanding to nearly 13,000 square miles in size. It was the largest ice break observed in that region for decades.\n\nThe following winter, another hole formed, this time exposing a whopping 20,000 square miles of ocean water.\n\nHoles in the sea ice, known as polynyas, are observed from time to time in the waters of both the Antarctic and Arctic. They’ve been cropping up in the Weddell Sea every so often for decades, typically over a plateau in the ocean floor known as the Maud Rise, although rarely in such dramatic fashion. Before the 2016 and 2017 events, the biggest polynyas dated back to the 1970s.\n\nThe conditions that caused the holes in the first place had remained something of a mystery. Now, scientists believe they’ve figured out what made the last batch—and how these events might be affected by future climate change.\n\nIn a new paper, published yesterday in Nature, researchers suggest that a combination of saltier-than-usual water in the Southern Ocean and the influence of strong storms helped open the holes. They used measurements taken by a variety of innovative sources, from robotic floats in the Southern Ocean to deep-diving elephant seals equipped with special sensors, to conduct the study.\n\nUnder typical conditions, the water in the Southern Ocean around Antarctica exists in layers, with warmer, saltier water at the bottom of the sea and lighter, fresher water resting on top. But in 2016, sensors revealed that the water near the surface was saltier than usual.\n\nWhen that happens, there’s less difference between the layers in the ocean, making it easier for them to mix with each other. And when they mix up, warmer layers are able to transfer heat toward the surface of the water, weakening the sea ice.\n\nIn 2016, the saltier conditions were likely caused by a shift in the westerly winds, or the winds that blow from west to east, around Antarctica, said lead study author Ethan Campbell of the University of Washington.\n\n“In some years, these winds are closer to Antarctica and they’re stronger, and in other years they’re farther and they’re weaker,” he told E&E News. “Sort of like a tightening of a belt around Antarctica. When they’re closer, it causes upwelling of deep water into the surface layer.”\n\nIn keeping with the theory, measurements from floats in the area showed that the region’s sea ice formed unusually late in the season. That means it may have been thinner or weaker than usual.\n\nNext, scientists say, stormy weather likely helped break up the ice and mix up the water. Records show that the 2016 polynya originally formed at the same time a storm swept through the region. Additional storms helped it to expand.\n\nThe formation of the 2016 polynya allowed for even further mixing between the ocean layers that primed the region for the occurrence of another, even bigger polynya in 2017, researchers say.\n\nConflicting climate influences\n\nFully understanding the formation of events like polynyas can provide scientists with deeper knowledge of the oceanic and atmospheric systems in climate-sensitive regions like the poles. That, in turn, can help scientists evaluate the models they use to simulate physical processes in these regions and make predictions about how they might respond to future climate change.\n\nAnd being able to predict the formation of polynyas, specifically, could be useful to climate scientists—because they have the potential to affect the climate system themselves, according to Campbell.\n\nFor instance, the deep water that upwells to the surface at a polynya can sometimes contain rich stores of carbon that have been sitting at the bottom of the ocean for hundreds or even thousands of years. When that water hits the surface, it may release that carbon back into the atmosphere in a phenomenon known as “outgassing,” Campbell pointed out.\n\nIn the same way, winter polynyas may release heat from the upwelling warm water that could also affect the formation of storms or other weather patterns in the Southern Hemisphere, he added.\n\nAt the same time, the likelihood that polynyas will form at all may be influenced by human-caused climate change. It’s complicated, Campbell notes, because some effects of climate change may make polynya formation more difficult, and other effects may make it easier.\n\nFor instance, melting glaciers on the Antarctic ice sheet release cold, fresh water into the ocean, strengthening the layers in the water and making it more difficult for them to mix up. That’s a process that will likely hinder the formation of polynyas in the long run.\n\nOn the other hand, research also suggests that human-caused warming is causing changes in the wind patterns around Antarctica, including a shifting and strengthening of the westerly winds that help mix up the ocean water. Those changes may make polynyas more likely to form in places like the Weddell Sea—and they may be occurring faster, for now, than the freshening of the water caused by the melting ice.\n\nAs a result, the researchers say there may be at least a temporary increase in holes and other disturbances in the Antarctic sea ice.\n\nIn the long run, though, Campbell notes that these effects are likely to constitute a kind of tug of war between two different sets of climate consequences, and scientists are still working to understand what that will look like in the future.\n\n“We’re not sure which one is going to win out,” he said.\n\nReprinted from Climatewire with permission from E&E News. E&E provides daily coverage of essential energy and environmental news atwww.eenews.net.","title":"Storms, Salty Water Caused Mystery Hole in Antarctic Sea Ice","origin":"EARTH","image":"https://static.scientificamerican.com/sciam/cache/file/16A4A29C-67C5-427B-9A9D69425311F2DF_source.jpg?w=590&h=800&63146E05-A276-45B6-A80EAAFFBE558313","link":"https://www.scientificamerican.com/article/storms-salty-water-caused-mystery-hole-in-antarctic-sea-ice/"},{"authors":"Nicholas Florko, STAT","pub_date":"June 9, 2019","abstract":"Next Wednesday, a cadre of ALS patients will gather for a protest outside the FDA’s headquarters in suburban Maryland with a clear message: “No More Excuses.”\n\nThe rally is being organized by a ragtag group of ALS patients who connected mostly through Facebook, and it’s far less a production than other efforts like the 2014 ice bucket challenge that swept around the world.\n\nThese protesters haven’t even established a formal organization or a website; some said they’ve never even been to a protest. There’s nary a poster board in sight, yet.\n\nBut the band of patients is already catching the attention of regulators at the Food and Drug Administration, establishment advocates at the ALS Association, and top bioethicists around the country. Top Senate lawmakers, too, are beginning to advocate for their cause.\n\nThese protesters say, at least, that they plan to emulate the aggressive, even “radical” approaches and the take-no-prisoners strategy of the AIDS activists who protested the FDA’s slow work on that disease in the 1980s. Like the ACT UP activists before them, the ALS patients are coming armed with a list of specific therapies they want regulators to approve—and fast. But unlike their predecessors, they’re hoping not to be arrested next week.\n\nThe ALS patients are fed up: They had hoped that the $115 million raised from the viral ice bucket challenge would be poured into promising research. They’ve seen no results from their advocacy for the national “right-to-try” law that promised access to new cures. And they’ve felt left out as the FDA approved treatments for other similarly fatal neuromuscular diseases.\n\n“We as ALS patients are just tired of no action being taken on our part,” said Mike Henson, one of the organizers. “I don’t know what’s worse, getting ALS 10 years ago and just knowing you’re going to die or getting it now and knowing that you could be saved if only people would take this seriously.”\n\nHenson lives in Tulsa, Okla., and was diagnosed with ALS last year. He is a former logistics manager for a rock casino who speaks his mind, calling ALS “a murderous son of a bitch of a disease.’’ He’s made a name for himself in the ALS community by starting his own ALS talk show on YouTube, complete with a chyron and green screen.\n\nJust Wednesday, the protesters adopted a name for their troupe: “Contagious for A Cure.”\n\nAnd there’s a specific potential cure they’re seeking—or rather, for now, there are three. A trio of drug companies is in the middle of the development process for separate, still-investigational ALS drugs: BrainStorm’s stem cell treatment NurOwn, Collaborative Medicinal Development’s synthetic molecule CuATSM, and Dr. Stanley Appel’s cell therapy known as Tregs.\n\nThe drugs are all at quite different stages of clinical development: BrainStorm is currently enrolling a Phase 3 clinical trial, Collaborative Medicinal Development has only completed Phase 1 testing, and Appel’s Tregs has only been studied in three patients.\n\nFor those knowledgeable about ALS, it feels like deja vu: ALS patients have demanded access to BrainStorm’s NurOwn throughout the fight for a federal right-to-try law. So much so that the company’s CEO Chaim Lebovits promised that patients would be able to access the drug once right to try was law, only to largely go back on that promise and instead provide access to only one right-to-try activist, Matt Bellina. (BrainStorm declined to comment for this story.)\n\nHenson and his fellow organizers want to know why these drugs aren’t being fast-tracked through the FDA’s approval process, especially while drugs for other neuromuscular diseases are being approved, seemingly left and right. He points to Zolgensma, Novartis’ gene therapy for spinal muscular atrophy, which was approvedbased on a trial of only 15 children—albeit with impressive results.\n\n“That drug was approved very quickly with a very small trial,” Henson said. “So, we know this can be done, if we have the will to do this.”\n\nThe FDA said it “stands ready to help” the ALS community—including by helping patients and their doctors evaluate options like clinical trials or using the FDA’s expanded access program to get ahold of experimental, unapproved treatments. A spokeswoman also said the agency “is prepared to use all expedited development and approval programs at its disposal” to make new treatments available.\n\n\n\nTwo top officials in the agency’s drug approval centers, Janet Woodcock and Peter Marks, also offered to meet with advocates and drug makers, so long as the drug makers would also participate.\n\n“We encourage you to reach out to each of the manufacturers to request a meeting with the agency that includes you and any other members of the ALS community that you would like to include. With their permission, we welcome the opportunity to have a transparency discussion about their investigational products,” Woodcock and Marks wrote in an email shared with STAT.\n\nMore and more, cries for more progress are coming from Capitol Hill as well. Republican Sens. Ted Cruz (Texas), Marco Rubio (Fla.), Mike Braun (Ind.) and Mike Lee (Utah) wrote to acting FDA Commissioner Ned Sharpless last month making a similar demand.\n\nThe senators are calling for the FDA to bring back the so-called parallel track that the agency unveiled for AIDS patients in the 1990s. Under that pathway, which was never extended beyond HIV and AIDS patients, those ineligible for clinical trials could gain access to certain drugs still being studied. It’s estimated that thousands of AIDS patients got access to one such drug, Stavudine, under the parallel track in the 1990s.\n\n“It is time for the FDA to create and encourage an environment that promotes patient choice, patient access, and patient affordability,” they wrote. “Expansion of the Parallel Track is a proven solution to do just that, and we strongly encourage the FDA to act immediately on this issue.”\n\nIt’s a perplexing request. In the years since it created the parallel track, the FDA has stood up several more concrete pathways for patients to access experimental drugs.\n\nThe FDA isn’t the only target of these protesters. The group also plans to protest the ALSA. They want answers as to why the $115 million raised by the ice bucket challenge hasn’t gone to fund development of their three favorite potential therapies. ALSA has technically funded two of the therapies, although the protestors insist their funding was not nearly enough.\n\n“What we just can’t figure out is why ALSA is not vigorously pursuing getting any of these drugs into our bodies immediately!” the organizers of next week’s protest wrote to ALSA.\n\nThey’re so frustrated with ALSA that they’re calling for the resignation of the group’s President and CEO Calaneet Balas.\n\nIn a statement to STAT, ALSA said it was “working urgently to end ALS,” which includes funding “many early stage clinical trials and work[ing] with the FDA and scientists to help trials run faster.” An ALSA-sponsored report released Tuesday also found that the ice bucket challenge resulted in ALSA increasing its research funding by 187%. Among the achievements from that research: discovery of five new genes potentially tied to ALS.\n\nBut the protestors want more specific commitments from ALSA to work with BrainStorm, Appel, and Collaborative Medicinal Development “with the ultimate goal of having any of these viable therapies available to the ALS community within the next 6 months maximum.”\n\nHenson is convinced these three treatments—either alone or in some combination—are the cure to curing ALS.\n\n“They’ve been proven,” he insisted.\n\nRepublished with permission from STAT. This article originally appeared on June 7, 2019","title":"Fed Up with Washington, ALS Advocates Consider ACT UP’s Take-No-Prisoners Approach","origin":"Medicine","image":"https://static.scientificamerican.com/sciam/cache/file/2422DD98-3E33-4927-9D162CAA788A9A7F_source.jpg?w=590&h=800&43C8E9C4-4DF3-4E06-B24BABA53ADCB3F3","link":"https://www.scientificamerican.com/article/fed-up-with-washington-als-advocates-consider-act-ups-take-no-prisoners-approach/"},{"authors":"John Horgan","pub_date":"June 3, 2019, 8","abstract":"I can live without God, but I need free will. Without free will life makes no sense, it lacks meaning. So I’m always on the lookout for strong, clear arguments for free will. Christian List, a philosopher at the London School of Economics, provides such arguments in his succinct new book Why Free Will Is Real (Harvard 2019). I met List in 2015 when I decided to attend, after much deliberation, a workshop on consciousness at NYU. I recently freely chose to send him some questions, which he freely chose to answer. –John Horgan\n\nHorgan: Why philosophy? Was your choice pre-determined?\n\nList: I don’t think it was. As a teenager, I wanted to become a computer scientist or mathematician. It was only during my last couple of years at high school that I developed an interest in philosophy, and then I studied mathematics and philosophy as an undergraduate. For my doctorate, I chose political science, because I wanted to do something more applied, but I ended up working on mathematical models of collective decision-making and their implications for philosophical questions about democracy. Can majority voting produce rational collective outcomes? Are there truths to be found in politics? So, I was drawn back into philosophy. But the fact that I now teach philosophy is due to contingent events, especially meeting some philosophers who encouraged me.\n\nHorgan: Free-will denial seems to be on the rise. Why do you think that is?\n\nList: The free-will denial we are now seeing appears to be a by-product of the growing popularity of a reductionistic worldview in which everything is thought to be reducible to physical processes. If we look at the world solely through the lens of fundamental physics, for instance, then we will see only particles, fields, and forces, and there seems no room for human agency and free will. People then look like bio-physical machines. My response is that this kind of reductionism is mistaken. I want to embrace a scientific worldview, but reject reductionism. In fact, many scientists reject the sort of reductionism that is often mistakenly associated with science.\n\nHorgan: Would you describe your belief in free will as faith? Do you ever, perhaps in the dead of night, doubt that free will doesn’t exist?\n\nList: No, I wouldn’t describe it as faith. As I explain in my book, there are rational arguments in support of the view that free will exists. But this is not a dogma. If new scientific developments were to vindicate strict determinism in psychology, rather than physics, then this would be evidence against free will. But, as of now, there is no support for a deterministic picture of psychology. Do I ever have any doubts about this? Not in my day-to-day life. But as an academic, it is my job to ask critical questions and to scrutinize my views. That’s why I take the challenges for free will very seriously and devote much room to them in my book.  \n\nHorgan: Can you give me the cocktail-party version of your free will argument?\n\nList: I am not sure whether this would work at a cocktail party. It would depend on the cocktail party… But here is a summary. My goal is to argue that a robust form of free will fits into a scientific worldview. How do I show this? Well, there are two ways of thinking about human beings. We can either think of them as heaps of interacting particles, and thus as nothing but physical systems, or we can think of them as intentional agents, with psychological features and mental states. If we tried to understand humans in the first, reductionistic way, there would be little room for free will. But the human and social sciences support the second way of thinking, the non-reductionistic one, and this, in turn, supports the hypothesis that there is free will. \n\nSpecifically, I accept that free will requires intentional agency, alternative possibilities between which we can choose, and causal control over our actions. But unlike free-will skeptics, I don’t look for these things at the level of the body and brain understood solely as a physical system. Rather, I argue that agency, choice, and control are emergent, higher-level phenomena, like cognition in psychology and institutions in economics. They “supervene” on physical phenomena, as philosophers say, but are not reducible to them. \n\nReferences to agency, choice, and control become indispensable once we think of humans in this way. We would not be able to make sense of human behavior if we didn’t view people as choice-making agents. It would be impossible to understand people at the level of the gazillions of molecules and cells in their brains and bodies. And even if we could describe human behavior at that level, we would fail to pick up the beliefs, preferences, and other psychological features that most naturally explain their decisions. This supports treating agency, choice, and control as real.\n\n\n\t\n\t\tCredit: Harvard University Press\n\t\n\n\nNow, you might ask, isn’t this incompatible with physical determinism? My answer is that when we understand human beings as intentional agents, they shouldn’t be viewed as determined. There is a perfectly intelligible sense in which they face forks in the road, namely when they make decisions. This may sound counterintuitive, but indeterminism at the level of agency is compatible with determinism at the level of physics. \n\nThe issue is a little subtle, but the key point is that the distinction between determinism and indeterminism is a level-specific one. It doesn’t make sense to ask whether a given system is deterministic or indeterministic simpliciter. The question becomes meaningful only once we specify the level of description at which we are asking the question. A system can be deterministic at a micro-level and indeterministic at a macro-one. While some would interpret this as merely “epistemic” – due to our lack of information about the micro-state – I give arguments in my book for interpreting it as a real phenomenon.  \n\nThere is some room for debate about the best interpretation here, but others, too, have recognized that when we move from a lower level of description to a higher one, we may see a transition from deterministic to indeterministic behavior in a system. Jeremy Butterfield expresses this point by saying that a system’s micro- and macro-dynamics need not “mesh”.\n\nHorgan: Free-will deniers claim that Benjamin Libet’s experiments undercut free will. Why are they idiots?\n\nList: They are certainly not idiots! They have made important contributions to our understanding of the mechanisms underlying voluntary motor actions. What they show is that when experimental participants are asked to perform spontaneous movements at a time of their choice, some brain activity can be detected before they feel the conscious intention to act. Libet and others take this to be a challenge for free will. I do not deny the experimental findings. The issue is how to interpret them. \n\nContributors to this debate do not always define precisely what they mean by “causation”. For instance, if we define a cause as a systematic difference-making factor for the resulting effect, then it’s not clear that the neuronal readiness potentials measured by Libet would qualify as causes of the actions. As Libet acknowledges, subjects can still abort an initially intended action after the neural activity has begun. Libet describes this capacity as “free won’t”. Others, such as my London colleague Patrick Haggard, have shed further light on how this capacity is implemented in the brain. \n\nI argue that if we apply the theory of causation that is most suitable for the human and social sciences – namely the so-called “interventionist” or “difference-making” theory – then we have reasons to conclude that the most systematic causal explanations of human behavior won’t always be lower-level, neuronal ones, but can involve higher-level, psychological variables. The psychological – not just neuronal – level remains hospitable to causal regularities.  \n\nHorgan: Does free will require consciousness? \n\nList: Free will and consciousness are conceptually distinct. Free will, as I define it, requires intentional agency, alternative possibilities, and causal control over our actions. Consciousness – especially “phenomenal” consciousness – requires the presence of subjective experience from a first-person perspective. There must be something it is like to be a particular agent, as Thomas Nagel famously says. Whether free will requires consciousness depends, among other things, on whether agency itself requires consciousness. I don’t explicitly build a consciousness requirement into my definition of agency. This is because I want to keep my concepts modular, and I think there can be intentional agents without consciousness – corporate agents, for example, on which I have worked with Philip Pettit. It could still turn out that, as a matter of fact, most or all agents with full-blown free will also have consciousness.  \n\nHorgan: Speaking of consciousness, can science ever explain it?\n\nList: The hard problem of consciousness, as David Chalmers calls it, is due to the fact that phenomenal consciousness involves first-person experience. It is inherently subjective. Science aims to give us an objective picture of the world. And although scientific objectivity is a contested idea, the sciences usually describe the world from a third-person perspective – that of an observer studying phenomena from the outside. It is therefore not clear whether a purely third-personal scientific approach can fully explain consciousness, in a way that will satisfy those who are interested in the nature of first-person experience. \n\nI think the most promising scientific approaches to consciousness are those that take first-person data seriously and seek to accommodate them, perhaps by formulating psycho-physical hypotheses: hypotheses about how physical processes are associated with subjective experience. Integrated information theory, developed by Giulio Tononi and others, is one promising approach, though there is no consensus yet about whether this theory is right.  \n\nHorgan: Can non-human animals, like chimps or dogs, have free will? What about robots?\n\nList: My theory clarifies what is at stake in answering this question. To figure out whether a given entity has free will, we must determine whether that entity has intentional agency, alternative possibilities to choose from, and causal control over its actions. In the case of many non-human animals, I would be inclined to give a positive answer. Chimps don’t have the same agential capacities as humans, but they arguably meet the requirements of agency. Our best theories of their behavior may well attribute to them the ability to make choices, together with a certain level of control over those choices. They may then count as having a certain kind of free will. Similar things might be said – to a lesser extent – about other mammals. In the case of robots and AI systems, we can talk much about how advanced they will become in the foreseeable future, and whether they are best interpreted as intentional agents. But, conceptually speaking, future complex robots and AI systems might well satisfy the three requirements for free will. That would raise important questions about responsibility.\n\nHorgan: Will we ever stop arguing about consciousness and free will?\n\nList: These are perennial questions, which every generation is likely to grapple with. But even if we never reach a consensus, there can still be philosophical progress. We now have a much better understanding of the relevant conceptual terrain than earlier generations did. Thinking about philosophical questions can help us clarify our concepts and categories and make our worldview more coherent. This is relevant to the sciences and to public debate. Remember that the issues we are tackling here are not merely intellectual: they inform our views about publicly relevant notions such as responsibility and personhood.\n\nHorgan: You’ve written about democracy. How do you think democracy is holding up? Do you have any ideas for improving it?\n\nList: Like others, I’m concerned about increasing political polarization, the erosion of trust in democratic politics, and the rise of populism. Many factors have contributed to this. The high levels of inequality that we are seeing in many countries are one factor, and the changes due to globalization another. The public sphere has been transformed by social media and the use of big data. The solution to this perceived crisis of democracy cannot be a backward-looking nationalistic turn or a rewinding of the clock to an earlier era. To reinvigorate democracy, we must tackle inequality and achieve a more deliberative democratic culture that places more emphasis on civil and respectful debate instead of simplistic soundbite politics. To do so, we should explore new forms of political communication such as citizens’ assemblies and invest in education. The media, including high-quality journalism, also play an important role. Democracy cannot be reduced to majority voting alone. Voting must be preceded by a period of thorough and inclusive public deliberation about the relevant issues, based on the best available information and a careful consideration of the reasons for and against the various options. \n\nIn my social-choice-theoretic work, especially with the political scientists James Fishkin and Robert Luskin, we looked at how group deliberation affects participants’ political preferences. We observed that while – unsurprisingly – deliberation doesn’t generate a consensus, it can generate something we may describe as a “meta-consensus”: a shared understanding of what the disagreement is about. This meta-consensus can help us generate support for compromise positions.\n\nHorgan: You also write about economics. Are we stuck with capitalism?\n\nList: I’m not sure whether we are, but I’m certainly not alone in thinking that the status quo requires major improvements. I’d like to see smart forms of regulation that reduce inequality and improve environmental sustainability, a reinvigorated democratic culture, and extensive international cooperation. I find a democratically governed, internationally collaborative, and socially and environmentally regulated market economy far more attractive than insufficiently constrained forms of capitalism.\n\nHorgan: Steven Pinker and John Gray have been battling over whether everything is fine and getting better (Pinker) or everything is terrible and getting worse (Gray). Where do you stand?\n\nList: I’m not convinced that we can have a single unidimensional measure of progress. On some dimensions, things have been getting better, on others worse. There is still large-scale inequality and poverty, both across countries and within countries; insufficient respect for human rights; massive biodiversity loss, environmental degradation, and climate change. These are matters of grave concern.Against this background, it’s difficult to be fully optimistic, yet too much pessimism is unhelpful too. Clearly, urgent action is needed to tackle the many challenges humanity is facing.  \n\nHorgan: What’s your utopia?\n\nList: “Utopia” is ambiguous between “no place” and “ideal place”. In his classic 1516 book, Thomas More used this label to refer to a fictional “optimal” island republic, and he imagined a society with norms and customs quite different from the familiar ones at the time. More’s “Utopia” was in fact one of the first works of political philosophy that I read – I think when I was still a teenager. Although utopias are intellectually fascinating, I think that, rather than pursuing some unrealistic and, in the worst case, dangerous utopian vision, we should focus on the problems we are facing here and now.\n\nFurther Reading:\n\nMind-Body Problems (free online book, also available as Kindle e-book and paperback)\n\nMeta-Post: Posts on the Mind-Body Problem (includes posts on free will)\n\nMeta-Post: Posts on Poverty and Other Social Problems. See also List's article on social choice theory in the Stanford Encyclopedia of Philosophy.\n\nSee Q&As with Scott Aaronson, David Albert, David Chalmers, Noam Chomsky, David Deutsch, George Ellis, Marcelo Gleiser, Robin Hanson, Nick Herbert, Jim Holt, Sabine Hossenfelder, Stuart Kauffman, Christof Koch, Garrett Lisi, Tim Maudlin, Priyamvada Natarajan, Naomi Oreskes, Martin Rees, Carlo Rovelli, Rupert Sheldrake, Lee Smolin, Sheldon Solomon, Paul Steinhardt, Philip Tetlock, Tyler Volk, Steven Weinberg,  Edward Witten, Peter Woit, Stephen Wolfram and Eliezer Yudkowsky.","title":"Free Will Is Real","origin":"Cross-Check","image":"https://static.scientificamerican.com/blogs/cache/file/692FE748-A414-4BE0-ACA85853828CE3F7_source.jpg?w=590&h=800&00C3D945-6F7A-4426-87A31F2FDC319E5E","link":"https://blogs.scientificamerican.com/cross-check/free-will-is-real/"},{"authors":"Get-Fit Guy Brock Armstrong","pub_date":"June 11, 2019","abstract":"Over the past 13 years, the editors of ACSM’s Health & Fitness Journal® (FIT) have circulated an electronic survey to thousands of wellness professionals around the world to determine the current year’s health and fitness trends. The first survey was conducted in 2006 and it introduced what was meant to be a systematic way of predicting health and fitness trends. The survey has been conducted annually since that time, using the same methodology.\n\nSince this is a survey that is solely about trends, the participants of the survey were asked to make the very important distinction between what they thought was a “fad” and what was a “trend.” These are the definitions they were given as a guideline:\n\n\n\nTrend: a general development or change in a situation or in the way that people are behaving.\n\n\nFad: a fashion that is taken up with great enthusiasm for a brief period.\n\n\n\nResponses were received from just about every continent and included the countries of United Kingdom, Australia, Canada, China, France, Germany, Japan, India, Italy, Russia, Singapore, Taiwan, and the United States.\n\n\n\n»Continue reading “The 2019 Worldwide Fitness Trends” on QuickAndDirtyTips.com","title":"The 2019 Worldwide Fitness Trends","origin":"Wellness","image":"https://static.scientificamerican.com/sciam/cache/file/2DBE9BDC-630D-4402-98DAA4311A995E90_source.jpg?w=590&h=800&4A994F8E-6C0E-462B-822C522942420F2C","link":"https://www.scientificamerican.com/article/the-2019-worldwide-fitness-trends/"},{"authors":"Drew Gronewold, Richard B. Rood, The Conversation US","pub_date":"June 8, 2019","abstract":"The following essay is reprinted with permission from The Conversation, an online publication covering the latest research.\n\nThe North American Great Lakes contain about one-fifth of the world’s surface fresh water. In May, new high water level records were set on Lakes Erie and Superior, and there has been widespread flooding across Lake Ontario for the second time in three years. These events coincide with persistent precipitation and severe flooding across much of central North America.\n\nAs recently as 2013, water levels on most of the Great Lakes were very low. At that time some experts proposed that climate change, along with other human actions such as channel dredging and water diversions, would cause water levels to continue to decline. This scenario spurred serious concern. Over 30 million people live within the Great Lakes basin, and many depend directly on the lakes for drinking water, industrial use, commercial shipping and recreation.\n\nBut since 2014 the issue has been too much water, not too little. High water poses just as many challenges for the region, including shoreline erosion, property damage, displacement of families and delays in planting spring crops. New York Gov. Andrew Cuomo recently declared a state of emergency in response to the flooding around Lake Ontario while calling for better planning decisions in light of climate change.\n\nAs researchers specializing in hydrology and climate science, we believe rapid transitions between extreme high and low water levels in the Great Lakes represent the “new normal.” Our view is based on interactions between global climate variability and the components of the regional hydrological cycle. Increasing precipitation, the threat of recurring periods of high evaporation, and a combination of both routine and unusual climate events—such as extreme cold air outbursts—are putting the region in uncharted territory.\n\nCalculating the lakes’ water budget\n\nCurrent water levels on the Great Lakes are setting records. Lake Superior, the largest freshwater lake on Earth by surface area, surpassed its record of 602.82 feet for the month of May, and is poised to set a new record for the month of June. Lake Erie, the world’s ninth largest lake by surface area, surpassed not only its record water level for the month of May, but also its all-time monthly water level record of 574.28 feet, which has stood since June 1986.\n\nThese extremes result from changes in the Great Lakes’ water budget—the movement of water into and out of the lakes. Water levels across the lakes fluctuate over time, influenced mainly by three factors: rain and snowfall over the lakes, evaporation over the lakes, and runoff that enters each lake from the surrounding land through tributaries and rivers. Runoff is directly affected by precipitation over land, snow cover and soil moisture.\n\n\n\nRunoff from melting snow that accumulates around the Great Lakes each winter, shown here on March 25, 2019, is one element of the lakes’ water budget. Credit: NASA Earth Observatory\n\n\n\nInteractions between these factors drive changes in the amount of water stored in each of the Great Lakes. For example, in the late 1990s surface water temperatures on Lakes Superior and Michigan-Huron rose by roughly 2 degrees C. Water evaporates more rapidly when it is warmer, and during this period evaporation rates were nearly 30% above annual average levels. Water levels on Lake Michigan-Huron dropped to the lowest levels ever recorded.\n\nThen in 2014 the Midwest experienced an extraordinary cold air outbreak, widely dubbed the “polar vortex.” The lakes froze and evaporation rates dropped. As a result, water levels surged.\n\nAt roughly the same time, precipitation was increasing. The 2017 Lake Ontario flood followed a spring of extreme overland precipitation in the Lake Ontario and Saint Lawrence River basins. The 2019 flood follows the wettest U.S. winter in history.\n\nWhat do these trends mean for water levels? In addition to the current onset of record highs, water levels in Lake Erie have been rising earlier in spring and declining earlier in fall. More winter precipitation is falling, often as snow. The snow is melting earlier in response to rising temperatures and shorter winters. The resulting runoff is then amplified in years like 2019 with large springtime rains. The net effect of this combination of hydrological events is that Lake Erie’s current water levels are much higher than usual for this time of year.\n\n\nSignificant lakeshore flooding and erosion along western #LakeErie this evening. Highest levels will occur this evening. This is uncharted territory with near record high levels. Help @NWSCLE by sharing your pictures and reports of flooding! #OHwx #CLEwx #Toledo #NWS #GreatLakes pic.twitter.com/qH7Wx1cANZ\n— NWS Cleveland (@NWSCLE) May 8, 2019\n\n\nThe role of climate change\n\nGreat Lakes water levels have varied in the past, so how do we know whether climate change is a factor in the changes taking place now?\n\nPrecipitation increases in winter and spring are consistent with the fact that a warming atmosphere can transport more water vapor. Converting water from vapor to liquid and ice releases energy. As a result, increased atmospheric moisture contributes to more precipitation during extreme events. That is, when weather patterns are wet, they are very wet.\n\n\n\nFlooding in New York state along the Lake Ontario shoreline, May 28, 2019.\n\nChanges in seasonal cycles of snowmelt and runoff align with the fact that spring is coming earlier in a changing climate. Climate models project that this trend will continue. Similarly, rising lake temperatures contribute to increased evaporation. When weather patterns are dry, this produces lower lake levels.\n\nWet and dry periods are influenced by storm tracks, which are related to global-scale processes such as El Niño. Similarly, cold air outbreaks are related to the Arctic Oscillation and associated shifts in the polar jet stream. These global patterns often have indirect effects on Great Lakes weather. It is uncertain how these relationships will change as the planet warms.\n\nTools for better forecasts\n\nRapid changes in weather and water supply conditions across the Great Lakes and upper Midwest are already challenging water management policy, engineering infrastructure and human behavior. We are undoubtedly observing the effects of a warming climate in the Great Lakes, but many questions remain to be answered.\n\n\n\nSoils in most of the Great Lakes states are extremely wet. For example, in 99th percentile zones, soil moisture is higher than 98% of the entire historical record. Credit: NOAA\n\n\n\nThe Great Lakes are, collectively, a critical water resource. Government agencies and weather forecasters need new tools to assess how future climate conditions may affect the Great Lakes water budget and water levels, along with better shorter-term forecasts that capture changing conditions.\n\nInnovative techniques, such as incorporating information from snow and soil moisture maps into seasonal water supply forecasts, can help capture a full picture of what is happening to the water budget. The bigger point is that past conditions around the Great Lakes are not a reliable basis for decision-making that will carry into the future.\n\nThis article was originally published on The Conversation. Read the original article.","title":"Climate Change Sends Great Lakes Water Levels Seesawing","origin":"Environment","image":"https://static.scientificamerican.com/sciam/cache/file/96F0478D-72F2-41C7-98801DD50128ADBD_source.jpg?w=590&h=800&BCC3D385-9412-4C12-A44239962F5C066E","link":"https://www.scientificamerican.com/article/climate-change-sends-great-lakes-water-levels-seesawing/"},{"authors":"Michelle Andrews, Kaiser Health News","pub_date":"June 7, 2019","abstract":"The announcement this week that the federal government is changing its policy on the use of human fetal tissue in medical research is designed to please anti-abortion groups that have strongly supported President Donald Trump.\n\nBut it could jeopardize promising medical research and set back attempts to make inroads in devastating diseases such as HIV, Parkinson’s and diabetes, U.S. scientists said.\n\nUnder the new policy, employees at the National Institutes of Health (NIH) will no longer conduct research with human fetal tissue obtained from elective abortions, after using up any material they have on hand. Officials also immediately stopped funding a multiyear contract at the University of California-San Francisco using human fetal tissue in mice to research HIV therapies.\n\nFederally funded projects at other research institutions using fetal tissue can continue until their grants expire. But renewal for these projects and future proposals will have to go through a newly established ethics review process to receive funding. It’s not clear yet what standards that process will entail or whether such experiments will be able to proceed under government sponsorship.\n\nThe change was enthusiastically welcomed by abortion opponents, who have long had fetal tissue research in their sights. Many scientists had a very different view.\n\nHere are a few answers to questions about the issue.\n\nQ: What exactly does fetal tissue research refer to?\n\nFetal tissue is any tissue or organ obtained from a fetus that was fertilized at least eight weeks earlier. (Anything younger than that is called an embryo.)\n\nThe statement from the Department of Health and Human Services referred repeatedly to “human fetal tissue from elective abortions.”\n\nResearchers generally use fetal tissue from elective abortions rather than miscarriages because miscarriages often result from chromosomal or other developmental abnormalities that could make the tissue unsuitable for research.\n\nQ: What is fetal tissue research used for?\n\nThese cells are less specialized than adult tissue cells and can be grown readily, making them valuable in research. Fetal tissue has been used in many types of medical research, including the development of vaccines for polio, measles and other diseases, and therapies to treat Parkinson’s, diabetes, rheumatoid arthritis and to prevent the transmission of HIV.\n\nSome researchers graft fetal tissue onto mice, creating “humanized mice” with human blood-forming and immune systems.\n\nFetal tissue helps researchers learn about birth defects and human tissue development. In recent years, it has been instrumental in understanding how the Zika virus crosses the placenta and affects the development of the human brain, according to a letter to HHS Secretary Alex Azar signed by 70 organizations in December in support of continued fetal tissue research.\n\nQ: Are there rules about using fetal tissue?\n\nStrict federal rules govern the collection and use of human fetal tissue. It’s against the law for anyone to accept payment for human fetal tissue, except for reasonable amounts associated with acquisition, storage or other costs. There are also provisions that require women who are donating fetal tissue for research to provide informed consent and prohibit physicians from altering the timing or method of an abortion in order to obtain fetal tissue.\n\nQ: Has it always been as controversial as it is today?\n\nNot really. The level of controversy around fetal tissue research waxes and wanes. Human fetal tissue research has been done in the United States since the 1930s, and NIH has been funding this type of research since the 1950s. There was a ban on such funding, however, during part of the terms of Presidents Ronald Reagan and George H.W. Bush. Federal money was restored with bipartisan support in a 1993 bill for the NIH. Among the backers of that effort were some strong abortion opponents, such as Sen. Strom Thurmond (R-S.C.), who argued that the research could help people—like his daughter—with diabetes.\n\nNIH spent $115 million on human fetal tissue research in 2018, a tiny fraction of the nearly $14 billion it spent on clinical research overall. NIH currently funds roughly 200 projects that use fetal tissue, according to HHS.\n\nFetal tissue once again became a hot-button issue in 2015 with the release of doctored videos, later discredited, purporting to show Planned Parenthood officials discussing tissue donation policies and reimbursement. Last fall, the Trump administration announced it was conducting a review of all research involving fetal tissue to ensure it was consistent with statutes and regulations governing it.\n\nQ: Aren’t there effective alternatives?\n\nIt depends on whom you ask. Opponents of fetal tissue research point to a number of other possible options, including monkey or hamster cells for vaccines as well as blood collected after birth from umbilical cords that are rich in blood-forming stem cells. They also suggest the use of adult stem cells and “organoids”—artificially grown cells that somewhat mimic organs.\n\n“Why do we keep focusing on these archaic models when newer, better alternatives are out there?” asked Tara Sander Lee, a senior fellow and director of life sciences at the Charlotte Lozier Institute, which opposes research using fetal tissue from elective abortion.\n\nShe suggested that using tissue from a miscarriage could be an acceptable alternative to using tissue from an aborted fetus because it’s from “a natural death, not an intentional killing of the child.”\n\nThe letter from researchers to Azar in December called the idea that other cells could replace fetal tissue “patently incorrect.”\n\n“The study of human fetal tissue provides researchers with incomparable insights into how birth defects arise and how they can be prevented as well as an unparalleled window into the complexity of human tissue development,” the letter said.\n\nSally Temple, scientific director of the Neural Stem Cell Institute who is a past president of the International Society for Stem Cell Research, said that while these other types of cells hold promise, they aren’t ready for prime time.\n\n“There’s a lot of excitement about using stem cells and talk about how we can use three-dimensional organoids,” said Temple. But organoids don’t have the same cellular arrangement or blood vessel network. “Organoids can’t mimic real tissue,” she said.\n\n“If we’re going to understand how tissues are made in humans, you really have to have access to human tissue,” she added. “It makes you so nervous that scientists aren’t being heard.”\n\nThis story was originally published by Kaiser Health News on June 7, 2019. Read the original story here.","title":"FAQ: How Does New Trump Fetal Policy Impact Medical Research?","origin":"Policy & Ethics","image":"https://static.scientificamerican.com/sciam/cache/file/C16082DD-57F8-4B2E-8582E025F8C7A8FF_source.jpg?w=590&h=800&35E8F200-BCCB-45E2-86C1ED5D699A5375","link":"https://www.scientificamerican.com/article/faq-how-does-new-trump-fetal-policy-impact-medical-research/"},{"authors":"Jean Chemnick, E&E News","pub_date":"June 10, 2019","abstract":"EPA Administrator Andrew Wheeler says the use of an inappropriately pessimistic climate modeling tool is driving bad press around climate change, and he’s pledged to halt its use.\n\nWhile he was still acting EPA chief, Wheeler blamed overly dire assumptions for the National Climate Assessment, released by the Trump administration last Black Friday—a launch that seemed calculated to bury the congressionally mandated report, which highlighted the findings of experts at 13 federal agencies that harmful man-made climate change is underway and growing worse.\n\nAnd Wheeler used a summit last month in Metz, France, with ministers from six principal foreign allies to promise to “reexamine comprehensive modeling that best reflects the actual state of climate science.”\n\nThen, last Monday at a National Press Club briefing, Wheeler lamented that most press coverage of the NCA focused on what he called “the worst-case scenario.”\n\n“I do think we should take a more realistic look at the worst-case scenarios ... all the scenarios ... going forward,” he said.\n\nThe “worst-case scenario” Wheeler seems concerned about is something called Representative Concentration Pathway, or RCP, 8.5—a scenario long used by the U.N. Intergovernmental Panel on Climate Change, federal agencies responsible for the NCA, and the climate modeling community writ large to represent the upper extreme of greenhouse gas concentrations that could exist in the world’s atmosphere by the end of this century.\n\nRCP 8.5 assumes the world will curb fossil fuel use by only about 20% over the next 80 years while experiencing relatively low income growth and very high growth in population and global energy demand.\n\nDeveloped by researchers in Austria in 2007, RCP 8.5 is used by the IPCC and other entities together with three other baseline scenarios that incorporate more optimistic assumptions for global economic development, technology, trade and other factors that could inform future atmospheric greenhouse gas concentrations and, by extension, warming. The IPCC and NCA don’t rely on one scenario for any one report but show a range. The scenarios don’t seek to predict future climate policies but instead focus on longer-term trends.\n\n“We’re not fortune tellers; we’re scientists,” said Richard Moss, a senior scientist doing climate modeling work at the Pacific Northwest National Laboratory’s Joint Global Change Research Institute and a participant in the NCA.\n\nThe four scenarios don’t translate precisely to specific degrees of temperature rise—experts note uncertainty about feedback loops and other variables. But an RCP 8.5 pathway could usher in a catastrophic level of warming equal to about 4 or 5 degrees Celsius above preindustrial levels. It’s those findings that triggered some of the headlines Wheeler objected to last week.\n\nEPA seems to be building a case for exiling this “worst-case scenario” from future climate reports by the U.S. government. Sir Robert Watson, a British chemist who briefed Wheeler and his foreign counterparts at Metz last month on the U.N. species extinction report he’d spearheaded, said the EPA administrator singled out RCP 8.5 for criticism in his remarks there, calling the scenario “extreme.”\n\nEPA did not respond to an E&E News query about whether the “worst-case scenario” Wheeler referenced at the press club was RCP 8.5.\n\nBut spokesman James Hewitt asserted in a statement that “the previous use of inaccurate modeling that focuses on worst-case emission scenarios, that does not reflect real-world conditions, needs to be thoroughly re-examined and tested if such information is going to serve as the scientific foundation of nationwide decision-making now and in the future.”\n\n“The fundamental problem with worst-case emission scenarios is that they are based on the flawed supposition that the significantly positive trends in global poverty reduction, economic improvement, and demographics could actually slide backwards,” he said.\n\nBut experts in climate modeling, including some like Moss, who participated in the NCA, say that while RCP 8.5 represents the more pessimistic end of the report’s range, it isn’t actually extreme. It represents about the 90th percentile of what scientists think could happen this century in terms of greenhouse gas concentrations, which means there’s a 10% chance it’s an underestimate.\n\nAnd with Brazil, Australia and the United States rolling back climate policies and emissions ticking upward, it’s currently more in line with global emissions than the more-optimistic baseline scenarios.\n\n“We are right on 8.5 right now,” said Don Wuebbles, a professor of atmospheric science at the University of Illinois, Urbana-Champaign, who participated in the NCA. “We are on emissions that are higher than any of the other scenarios right now.”\n\nThe Global Change Research Act of 1990, which was signed by President George H.W. Bush and mandated the periodic release of the NCA, was intended to furnish Congress with information to aid with planning, Moss said, not reassurance.\n\n“The whole point of something like this is to prepare for the future, and I’ve never heard of effective risk analysis that picks a low scenario and says, ‘Let’s work from that,’” he said. “This is facetious, but if the planners of D-Day had said, ‘Let’s pick a scenario where the Germans see us coming and lay down their weapons and run away, and let’s plan for that,’ that’s not exactly going to get us on the beach.”\n\nLast week, Wheeler said that the IPCC was “moving away” from using RCP 8.5 in its assessments, but EPA did not respond to E&E News requests to elaborate.\n\nLast year’s landmark IPCC report referenced RCP 8.5, but the purpose of that special assessment was to show the difference between a temperature rise of 1.5 C and 2 C. Since RCP 8.5 correlates to at least twice 2 C, it played a less central role in the analysis than did more optimistic scenarios.\n\nWheeler also blamed the “Obama White House” for setting the parameters of the NCA, including what he called its “focus” on “the worst-case scenario.”\n\n“I thought that was political interference by the Obama White House in that process,” he told his press club audience.\n\nWhen asked, EPA’s press office supplied a May 2015 memo from the Subcommittee on Global Change Research of the National Science and Technology Council’s Committee on Environment, which EPA said reflected Obama White House political policies.\n\nBut Wuebbles said it reflected consensus among experts at the federal agencies and tracked with the IPCC and previous NCAs. The memo doesn’t set RCP 8.5 as the only scenario to be used—it sets a range of RCP 4.5, a moderate scenario, to RCP 8.5, with some analysis of other baseline scenarios.\n\nPeter Frumhoff, director of science and policy for the Union of Concerned Scientists, said it was ironic that Wheeler was advocating the use of assumptions that track with a quick global shift away from fossil fuels even as he presides over efforts to keep that shift from occurring.\n\n“Mr. Wheeler’s and this administration’s policies to expand fossil fuels use is only increasing the prospect that we’re going to continue to hug that scenario for some time to come until we come out our senses,” he said. “But there’s no evidence in the trajectory that we’re experiencing today that that bending is taking place.”\n\nReprinted from Climatewire with permission from E&E News. E&E provides daily coverage of essential energy and environmental news atwww.eenews.net.","title":"EPA Head Targets “Worst-Case” Climate Scenarios","origin":"Climate","image":"https://static.scientificamerican.com/sciam/cache/file/36357168-DEE8-4F5D-8718719CE588E5D6_source.jpg?w=590&h=800&A2FB5A9B-E97E-4238-8593997F5F312A4E","link":"https://www.scientificamerican.com/article/epa-head-targets-worst-case-climate-scenarios/"},{"authors":"Scott Waldman, E&E News","pub_date":"June 7, 2019","abstract":"Scientists reacted sharply to revelations that the White House is discussing ways to potentially force federal researchers to debate the credibility of mainstream climate science, saying it threatens to pull experts into conspiratorial exercises that might harm their careers.\n\nThe pushback came as some White House officials try to formalize a “red team” debate to question the scientific studies that underlie the National Climate Assessment. The exercise is widely viewed among scientists as an attempt to confuse public perceptions about the impacts of rising temperatures.\n\nThe White House is considering whether to require scientists from NOAA, NASA or other agencies to participate in the review. The program would be run through the National Security Council and would be portrayed as a “correction” to the National Climate Assessment, according to sources involved in the planning.\n\nKatharine Hayhoe, an atmospheric scientist and professor of political science who was a lead author of the climate assessment, described the White House plan as a political effort that could harm scientists professionally.\n\n“Forcing federal scientists to relitigate solid science that has been firmly established by thousands of peer-reviewed studies dating back over 160 years when many of these same scientists are already self-censoring themselves so as to not run afoul of agency politicals is an intimidation tactic that could conceivably be used to drive people out of federal positions or even ruin the careers of those who disagree,” Hayhoe said. “True science is debated in the scientific literature, not in kangaroo courts set up by politicians with agendas that rely on muddying the waters and hiding the truth to succeed.”\n\nShe said the “red team” should undergo the same level of scrutiny that was applied to the National Climate Assessment. The report was reviewed by federal agencies and underwent a public comment period.\n\n“Who feels that they understand the science better than over 400 federal and academic authors, experts at every relevant federal agency and the National Academy of Sciences, the premier scientific body of this country?” Hayhoe asked.\n\nGavin Schmidt is one scientist who was identified by White House officials as a possible participant in the debate. As the director of the NASA Goddard Institute for Space Studies in New York, Schmidt is a leading expert on climate change. He declined to comment on the plan and said no one has asked him to participate.\n\n“Nobody has actually approached me, and I’m not aware that anybody has approached my management to request my time to do this,” Schmidt said.\n\nIt’s not clear if NASA Administrator Jim Bridenstine supports the idea of having a debate. As a Republican congressman, Bridenstine was a critic of climate science. However, since he took over the space agency, he has defended the agency’s research into climate change.\n\nThe White House effort seems like a “show trial” that’s designed to confuse public perception about scientists’ confidence in human-caused global warming, said Bob Kopp, director of the Rutgers University Institute of Earth, Ocean and Atmospheric Sciences and a co-author of the National Climate Assessment.\n\n“They’re going to take a highly educated but small set of climate scientists and ask them, compel them, to repeat a congressionally mandated exercise,” he said, referring to the National Climate Assessment. “The fundamental thing that’s achieved is that it’s distracting people from trying to understand legitimate assessment exercises, which are intended to serve the public and the policy community.”\n\nReprinted from Climatewire with permission from E&E News. E&E provides daily coverage of essential energy and environmental news atwww.eenews.net.","title":"White House Climate Review Could Damage Careers, Scientists Warn","origin":"Policy & Ethics","image":"https://static.scientificamerican.com/sciam/cache/file/A5533646-6027-41AD-BD04C82046305CDD_source.jpg?w=590&h=800&11174E90-9CB0-4ECF-B77C4A113FFD7878","link":"https://www.scientificamerican.com/article/white-house-climate-review-could-damage-careers-scientists-warn/"}]