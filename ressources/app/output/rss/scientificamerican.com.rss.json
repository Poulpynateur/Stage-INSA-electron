[{"authors":"Jeremy Hsu","pub_date":"May 7, 2019","abstract":"Nobody wants to spend hours picking through a steady stream of garbage; this is why many recycling operations rely on automated systems to separate plastic containers, glass bottles, aluminum cans and mixed paper. But these systems have an imperfect track record, so human workers must stand by to nab what the machines fail to catch. Waste Management, a trash-hauling corporate titan with 100 recycling facilities in North America, employs about 3,000 human sorters—but it has difficulty finding workers willing to show up every day, and many quit within hours. This is one reason the company has begun testing new types of robots that could eventually join humans in the sorting lines.\n\n“It’s a very hard role to staff, which is why robotics makes sense for that position,” says Brent Bell, vice president of finance for WM Recycle America, a Waste Management subsidiary.\n\nThe United States generates massive volumes of waste. Each American on average tosses about 2,555 pounds of trash per year, an estimated 75 percent of it recyclable. Machines can process the mess much faster than humans; an optical sorter developed by Quebec-based manufacturer Machinex separates recyclables at up to 3,000 objects per minute. This type of technology relies on magnets to pull out some metals, eddy currents to catch others that are nonmagnetic, and near-infrared light to help optical sorters detect different grades of plastic. But their accuracy is compromised by the high rate of contamination inherent to U.S. recycling systems.\n\nUnlike most industrialized countries, the U.S. has overwhelmingly embraced a convenient single-stream recycling approach that lets people toss plastics, glass, metals and paper into one collection bin. This convenience leads to high recycling rates and relatively efficient collection. But it also worsens contamination: U.S. industry estimates suggest 20 to 25 percent of submitted items are unrecyclable trash. The problem stems partly from consumer confusion about what items qualify, along with “wish recycling”: taking a chance that chucking something in a designated bin will give it a new life. Many items also become unusable during collection, transit, or in-truck compacting that smashes everything together, staining paper with fluids and grinding broken glass into other items.\n\n\n\nThus, human workers have to step in. Otherwise, misidentified items can damage expensive equipment or temporarily shut down recycling operations. For example, light plastic and paper pieces are processed in giant spinning machines. Overlooked metal or glass containers that sneak in can turn into high-speed projectiles, says Susan Collins, executive director for the nonprofit Container Recycling Institute based in Culver City, Calif.\n\nInstead of struggling to find humans to constantly oversee the process, some companies are testing AI-driven robots equipped with grippers or suction cups to pick out recyclable objects. They use cameras and other sensors, coupled with machine-learning software, to recognize visual patterns associated with specific items. In order to replace people, however, such bots will eventually have to outperform us—human quality-control workers pick out about 30 to 40 items per minute. “If this [robot] can pick two times or three times as many objects as human workers, then we could start looking at the economics and seeing if we can justify a purchase,” Bell says.\n\nLast year Finnish company ZenRobotics debuted its Fast Picker robot, which can grab approximately 66 objects per minute. Now some recycling companies are talking about AI-driven “dark factories” or “lights-off sites” without human workers, says Janica Johansson, marketing director for ZenRobotics. Other companies still envision robots working alongside humans. “They’re not going to replace people—we will absolutely keep quality control people in our facilities—but they will allow us to process more tons per hour and provide cleaner recyclable products,” says Steve Sargent, director of recycling for Rumpke Waste & Recycling.\n\nLike Waste Management, Rumpke hopes to integrate recycling robots into its existing operations, which include regional services in Ohio, Indiana, Kentucky and West Virginia. The company is especially keen on trying out a Machinex-developed robot called SamurAI at a recycling facility in Cincinnati. SamurAI can pick out about 70 objects per minute, so it works more slowly than Machinex’s standard sorting equipment. But its still-superhuman speed means it could really help in the quality control department.\n\nNewer technologies could ratchet up robotic efficiency even more. The Massachusetts Institute of Technology’s Computer Science and Artificial Intelligence Laboratory (CSAIL) recently unveiled an experimental RoCycle system that uses soft Teflon “fingers,” which have fingertip sensors to detect object size and stiffness. This robot is much slower than humans; when it tried identifying objects on a simulated conveyor belt it only achieved 63 percent accuracy by touch alone (paper-covered tins posed a special challenge). Still, that accuracy could improve by combining tactile data with visual data from cameras, says Daniela Rus, a professor of electrical engineering and computer science and director of CSAIL at MIT.\n\n\n\n“AI solutions allow us to preserve this convenience and widespread adoption [of single-stream recycling] while ensuring that we aren't just shifting the labor of sorting to more vulnerable populations,” says Lillian Chin, a PhD candidate in electrical engineering and computer science at MIT and lead author on the RoCycle paper. However, skeptics point out that robots in recycling facilities cannot fix some of the fundamental flaws of the U.S. single-stream system, including the contamination issue that begins at curbside collection. “The robots will not be able to unbreak glass if the glass is arriving at sorting facilities already broken,” Collins says. MIT researchers suggest robots could someday presort recyclables before curbside pickup—but it is unclear who would pay to install such robots, even if the technology matures.\n\nDespite these limitations, the turn toward robots has gained new momentum after China rocked the global recycling industry by halting its imports of the world’s contaminated mixed paper and plastic waste in early 2018. No longer able to outsource the dull, dirty and dangerous task of trash-sorting, many American cities and towns have resorted to dumping recyclables straight into landfills or incinerators. Both the U.S. and overall worldwide recycling average for plastic is a measly 9 percent; another 12 percent of global plastic waste ends up incinerated, and 79 percent either goes into landfills or clutters up the natural environment.\n\nRobots that can demonstrate superhuman sorting speeds without too many mistakes may prove good enough to join the recycling lines. But beyond new technologies, companies such as Waste Management and Rumpke still emphasize the need to educate customers about what they can recycle and how they should do it. That means focusing on the basic recyclable categories and teaching people to avoid adding items like garden hoses, Christmas lights and plastic bags into the mix. It is a long-term struggle to change humanity’s wasteful lifestyles—and everyone acknowledges that AI and robots cannot solve the recycling crisis without humans doing their part.","title":"Can Robots Help Pick Up after the Recycling Crisis?","origin":"Engineering","image":"https://static.scientificamerican.com/sciam/cache/file/00C17737-4ACC-4BB0-B90E6E9AE595D2A3_source.jpg?w=590&h=800&118A9984-9050-4D0B-8639A92BC1A2D1F7","link":"https://www.scientificamerican.com/article/can-robots-help-pick-up-after-the-recycling-crisis/"},{"authors":"Sam McNerney","pub_date":"May 6, 2019","abstract":"In the late 19th century, the English social reformer Charles Booth went door-to-door with a small group of researchers asking London residents questions about their socioeconomic status. He published his findings in a series of maps that used an eight-point color scale to illustrate wealth on a street-by-street level. A few years later, Parliament passed the Old-Age Pensions Act, which provided financial aid for people over the age of 70. The new laws paved the way for modern social welfare in the United Kingdom. \n\nAlthough collecting data on populations predates the 19th century—the concept of a census is a few thousand years old—Booth played a special role by recording social and economic data of a population and using the findings to influence legislation. Subsequently, surveys became one of the most important features of the modern era. They were the first scientific tool we used to study ourselves in groups, and our new self-knowledge has paid off enormously. The social progress, economic growth, improvements in education and advances in health of the last 150 years were made possible, at least in part, by survey work. Progress became a lot easier once we could measure the things we were trying to change.\n\nOver the past few years, I’ve conducted about 300 market research surveys while working in a creative advertising agency. As I was reading about Charles Booth it occurred to me that there are two types of survey questions: those that extract and those that evoke. Questions that extract involve collecting information; this is Booth on the street asking people about their income. Questions that evoke involve eliciting a reaction. Think about a focus group moderator versus a standup comedian. Both care about what you think. But the stand-up comedian goes one step further to provoke a response.\n\nQuestions that extract represent the vast majority of survey questions. But in the market research world, questions that evoke are more valuable. Brands revolve around consumer insights akin to comedic bits—observations that make you grin, nod your head and say: “I hadn’t thought about it that way.” In an era in which data are seen as key to understanding people, brands still crave deeper emotional responses. Not data per se but a firm grip on emotional levers that, once pulled, generate a perspective-alerting response, the kind of response people have when they’re exposed to a story or piece art that changes them—permanently. \n\nIn the decades after Booth published his findings, academic psychologists began measuring attitude—Rensis Likert proposed his famous five-point scale in 1932—and businesses started employing full-time researchers. Sadly, market research cut itself off from the creativity it should have achieved by following the formal thinking of Booth and Likert rather than the informal but socially attentive judgment of writers and artists of the day. Questions that extract are important—Booth was the first person to accurately calculate the poverty rate in London—but they became a norm that few people in the field thought to challenge.\n\nMaking it a goal to elicit a response changes how you draft survey questions. Instead of thinking about what information you want to collect, you’re forced to think about the origins of your own impressions. The idea is to interrogate perception and use survey questions to surface a new observation, much like a comedian uses a story to set up a joke.    \n\nFor example, a few months ago, the managers of a toilet paper brand were interested in learning more about what people thought about their product. I considered what might happen if my company switched toilet paper brands. I had a neutral view of toilet paper but noticed that I was disappointed about the hypothetical change. As the scenario played out in my mind, I wanted the old brand back.\n\nConsider these two questions.\n\n\n\t“How likely are you to buy toilet paper brand X in the future?” \n\t“How much would you be willing to pay for toilet paper brand X?”\n\n\nThese are questions that extract. Now, consider this question.\n\n\n\t“If your office switched toilet paper brands and began using [insert brand] in the bathrooms, how excited would you be?”\n\n\nThis is a question that evokes. If you filled in “toilet paper brand X” with an expensive or cheap brand, you’d react. And by swapping in different brands and measuring how people respond, you can see how people perceive each brand relative to one another. Questions that extract still matter—asking people in a separate survey what toilet paper brand they would buy for their home bathroom could reveal a difference between stated versus revealed preference—but they play a supporting role in the research process.\n\nWe ended up conducting a survey (n=1,200) with this question. We found something the client had suspected but had struggled to validate. Although their product was seen as a little pricey, people still preferred it. In fact, it was the brand that people were most excited to have in the office. With a few edits to the survey question, we recommended a new tagline that inspired the subsequent creative work. “The toilet paper brand everyone wants in their office bathroom.” \n\nThinking about toilet paper in terms of questions that evoke turned a relatively mundane item into an object of interest. Toilet paper, it turns out, carries cultural weight. It works on us as travelers and consumers and not just homeowners. “Don’t bias the respondent” is perfectly responsible advice for questions that extract. But for questions that evoke, contemplating how the wording of a question may influence respondents is the last thing you want to think about. Introspection and curiosity need to guide the research process, not rigid questionnaire principles.\n\nThe advertising legend David Ogilvy once complained that advertising agencies “use research as a drunkard uses lamppost—not for illumination but for support.” It’s a great metaphor because it’s true. You think you’re uncovering new perspectives, but, actually, you’re just stuck in the old one. Questions that evoke, in this sense, illuminate. They shed light on the details of daily life that we notice and feel but don’t consciously acknowledge. Until we’re prompted to.","title":"The Science of Asking What People Want","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/FA2D3EC4-5461-4B9B-A77FBA88ED06B226_source.jpg?w=590&h=800&298FC561-F4C5-4C76-821505F689FB6B08","link":"https://blogs.scientificamerican.com/observations/the-science-of-asking-what-people-want/"},{"authors":"James Freeman","pub_date":"May 3, 2019","abstract":"Laboratory tests play a significant role in detecting, diagnosing and monitoring diseases. Clinical decisions are often influenced by test results that indicate, for example, whether a patient is having a heart attack or needs a medication adjustment; whether a patient is a candidate for surgery or chemotherapy; whether a patient is healthy enough to return home from a hospital stay. Waiting for the answers stress patients.\n\nHowever, producing these results quickly is increasingly challenging. A constantly growing number of patients and a greater number of available tests means that more samples are headed to the lab. There also is a shortage of staff to process the samples and keep up with the rising demand. According to the Bureau of Labor Statistics, demand for lab workers has grown 13 percent in the last year, almost double the average for other U.S. jobs.\n\nThis shortage of skilled workers has been decades in the making—the result of an aging workforce and a shrinking number of accredited training programs across the country. In fact, the number of such programs available to those interested in pursuing this field has decreased by nearly 25 percent since 1990.  \n\nBut despite challenges with attracting new talent, it is a great time for STEM students to consider a career in diagnostics. The field of in vitro diagnostics (IVD) is reaching an exciting tipping point: historically antiquated technology and processes are making way for unprecedented advancements that will help tackle testing challenges with greater speed and efficiency.\n\nLaboratory testing plays a critical role in patient care, in particular in the inpatient and emergency care settings, with tests conducted on nearly all of hospital patients and more than half of emergency patients. So, when a looming staffing crisis foreshadows a risky impact on patient care, manufacturers can help to ease the burden. Working together means more exciting opportunities for laboratory staff, greater productivity so physicians can have their patients’ results faster, better patient satisfaction when wait times are shortened, and overall enhanced patient safety with more safeguards in place to help prevent human error.\n\nLaboratorians across the U.S. are enthusiastic about implementing state-of-the-art technology and assays. Siemens Healthineers, the company I work for, provides more than 10 billion IVD tests to labs globally each year and built its newest offering, the Atellica Solution, to deliver the innovations laboratories worldwide need to modernize and future-proof their operations. The solution produces results for the most common tests run in the laboratory: pregnancy tests, blood glucose, electrolytes, hormones and lipids, and specialty tests. Staff can now spend less time maintaining the analyzers thanks to built-in quality control features that are important for delivering accurate results.\n\nAutomation reduces the number of manual procedures staff must perform, offering laboratories standardization and consistency that leads to better patient satisfaction, and, potentially, patient safety. Further, laboratory staff benefit from a decreased exposure to patient fluids through features that reduce sample handling. The laboratory also can now process patient samples by priority order on one instrument using technology that transports each test tube individually so that emergency samples for cases such as cardiac emergencies and sepsis can cut to the front of the line.\n\nHowever, for some patients, the amount of blood that needs to be drawn is more important than the speed of their results. Parents of infant patients with little blood to spare can gain peace of mind with laboratory advances, as can patients who face extended hospital stays. Patients undergoing extended hospital stays sometimes have their blood drawn daily, which in many cases for these sick patients is more frequently than their bodies can replenish. With new technology designed with the patient in mind, sensitive and precise results can be produced from very small sample volumes.\n\nAll these features are designed to help the laboratory operate more efficiently and ultimately ease the burdens facing many laboratorians today. Yet, many laboratories regularly face budget cuts and, as a result, are unable to upgrade their technology as often as the increasing patient testing demand would encourage. Modern technology advancements afford laboratorians the ability to keep up with the testing demands of today’s patients while improving turnaround time, quality of results, ease of use, and safety when handling patient samples. The latest innovations and technological advancements also are helping to restore the clinical laboratory as an exciting place to establish one’s career and address the pitfalls of understaffed laboratories.\n\nWhile technology and patient testing needs will continue to evolve, what remains absolute is the fact that IVD testing plays a critical role in detecting, diagnosing and monitoring diseases, and laboratory staff provide undeniable value to both the patient experience and the healthcare system. Without the silent heroes in lab coats who also serve as detectives of disease states and interpreters of patients’ DNA data, the laboratory would fail to serve its purpose within the health care system. For this critical reason, it is in all of our best interest as stakeholders in the health system to invest in the clinical laboratory by at least encouraging education in the fields that will alleviate the staffing shortage.","title":"A Shortage of Skilled Medical Lab Workers Is Looming","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/5EE1BB90-E73C-4C3A-B5F056B02A78F7AF_source.jpg?w=590&h=800&C9DA0DDD-5CC8-4742-AAE55D3E3001DB9E","link":"https://blogs.scientificamerican.com/observations/a-shortage-of-skilled-medical-lab-workers-is-looming/"},{"authors":"","pub_date":"","abstract":"","title":"","origin":"","link":"https://www.scientificamerican.com/store/books/your-brain-in-the-smartphone-age/"},{"authors":"Cassandra Robertson","pub_date":"May 6, 2019","abstract":"The Trump administration's recent decision to push for a complete invalidation of the Affordable Care Act, and the Medicaid expansions that accompany it, has implications for more than simply the provision of healthcare in this country. It will also impact economic opportunity. Understanding the downstream ramifications of healthcare policy is crucial to any discussion to changes in the ACA.\n\nThere is a direct link between health and one of the most cherished American values: equality of opportunity. And research shows specifically that access to Medicaid reduces infant mortality, improves children’s graduation rates, and improves health in the long term. Not surprisingly, it also increases upward economic mobility. This decision will therefore not only determine whether individuals will have access to health insurance, but also whether or not children have access to opportunity.\n\nNew research demonstrates that increasing Medicaid eligibility reduces the correlation between parent and child income at the county level. This means that Medicaid makes it less likely that the children of poor mothers will grow up to be poor adults. And when this happens, economic opportunity in that county as a whole improves. For example, today’s adults whose mothers were covered by the Medicaid expansions of the 1980s—when they were pregnant—are more likely to have climbed the economic ladder than children born in the same county to poor mothers before the expansions. This increase in upward mobility was particularly strong for children born to parents at the bottom of the income distribution.\n\nHealth insurance can therefore minimize the extent to which parents’ negative economic position is passed on to their children. Making Medicaid accessible to low-income pregnant mothers had a lasting positive effect on their children that is still felt decades later.\n\nWe also know that access to Medicaid has profound effects on student achievement, which is often thought to be a pathway of economic mobility. Increased access to Medicaid improves high school graduation rates for newly eligible students, compared to their peers in earlier years who were not eligible. And because access to Medicaid is important for educational achievement, it is important for overall economic opportunity as well.\n\nWhat does this mean? Health care, particularly for low income Americans, is a social policy that promotes equality of opportunity. Like access to public education, access to health insurance is a pathway for social mobility. Limiting health care subsidies shrinks social opportunity and\n\nperpetuates inequality, reproducing poverty across generations and undermining growth overall.\n\nThis demonstrates that the ACA is about far more than healthcare, and the impact of this decision could be felt throughout the economy.","title":"The Affordable Care Act and Economic Opportunity","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/8AA5038E-43C3-4239-A0473205264A191D_source.jpg?w=590&h=800&26F1893D-7D86-4355-BEFB71CD8217524D","link":"https://blogs.scientificamerican.com/observations/the-affordable-care-act-and-economic-opportunity/"},{"authors":"Christopher Intagliata","pub_date":"May 6, 2019","abstract":"Researchers want to outfit air conditioners with carbon capture technology. Christopher Intagliata reports.","title":"Could Air Conditioners Help Cool the Planet?","origin":"Climate 60-Second ScienceSubscribe:Apple iTunesRSS","image":"https://static.scientificamerican.com/sciam/cache/file/1FD002A5-49CF-4C82-BF7F8E8B22D52BE9_source.jpg?w=590&h=800&708E0294-4E37-4EF8-A55513F3E4750765","link":"https://www.scientificamerican.com/podcast/episode/could-air-conditioners-help-cool-the-planet/"},{"authors":"Jeff Tollefson, Nature magazine","pub_date":"May 6, 2019","abstract":"Up to one million plant and animal species face extinction, many within decades, because of human activities, says the most comprehensive report yet on the state of global ecosystems.\n\nWithout drastic action to conserve habitats, the rate of species extinctions—already tens to hundreds of times higher than the average across the last ten million years—will only increase, says the analysis by a United Nations-backed panel, the International Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES).\n\nThe report also finds that agricultural activities have had the largest impacts on ecosystems that people depend on for food, clean water and a stable climate. The loss of species and habitats is as much a danger to life on Earth as climate change, says a summary of the report released on 6 May.\n\nThe analysis distills findings from nearly 15,000 studies and government reports, integrating information from the natural and social sciences, Indigenous peoples and traditional agricultural communities. It is the first major international appraisal of biodiversity since 2005. Representatives of 132 governments met last week in Paris to finalize and approve the analysis.\n\nBiodiversity should be at the top of the global agenda alongside climate, said Anne Larigauderie, IPBES executive secretary, in a 6 May press conference in Paris, France. “We can no longer say that we did not know,” she said.\n\n“We have never had a single unified statement from the world’s governments that unambiguously makes clear the crisis we are facing for life on Earth,” says Thomas Brooks, chief scientist at the International Union for Conservation of Nature in Gland, Switzerland, who helped to edit the biodiversity analysis. “That is really the absolutely key novelty that we see here.”\n\nWithout “transformative changes” to the world’s economic, social and political systems to address this crisis, the IPBES panel projects that major biodiversity losses will continue to 2050 and beyond. “We are eroding the very foundations of our economies, livelihoods, food security, health and quality of life worldwide,” says IPBES chair Robert Watson, an atmospheric chemist at the University of East Anglia, UK.\n\nReshaping life on Earth\n\nAbout 75% of the planet’s land and 66% of its ocean areas have been “significantly altered” by people, driven in large part by the production of food, according to the IPBES report, which will be released in full later this year. Crop and livestock operations currently co-opt more than 33% of Earth’s land surface and 75% of its freshwater resources.\n\nAgricultural activities are also some of the largest contributors to human emissions of greenhouse gases. They account for roughly 25% of total emissions due to the use of fertilizers and the conversion of areas such as tropical forests to grow crops or raise livestock such as cattle. Agricultural threats to ecosystems will only increase as the world’s population continues to grow, according to the IPBES analysis.\n\nThe next biggest threats to nature are the exploitation of plants and animals through harvesting, logging, hunting and fishing; climate change; pollution and the spread of invasive species. The IPBES report finds that the average abundance of native plants, animals and insects has fallen by at least 20% since 1900 because of invasive species.\n\nThe report draws inextricable links between biodiversity loss and climate change. An estimated 5% of the planet’s species would be threatened with extinction by 2 °C of warming above preindustrial levels—a threshold that the world could breach in the next few decades unless greenhouse gas emissions are drastically reduced. Earth could lose 16% of its species if the average global temperature exceeds 4.3 °C. These losses would undermine global efforts to reduce poverty and hunger and promote more sustainable development, the IPBES report says.\n\nPulling back from the brink\n\nScientists may quibble about some of the extinction estimates and other details, but the report doesn’t pull punches when it describes how humans have altered Earth’s ecosystems, says Stuart Pimm, an ecologist at Duke University in Durham, North Carolina.\n\nThe world can reverse this biodiversity crisis, the report says, but doing so will require proactive environmental policies, the sustainable production of food and other resources and a concerted effort to reduce greenhouse-gas emissions.\n\nThe IPBES report is solid on the science, but the panel should do more when it comes to outlining practical solutions for governments, businesses and communities, says Peter Bridgewater, an ecologist at the University of Canberra who led a separate analysis—released on 29 April—of the effectiveness of the biodiversity panel. That report, commissioned by IPBES, recommended that the body develop partnerships with governments and communities, and assess policies that can be implemented at local and national levels.\n\nDespite those shortcomings, the IPBES report will help to set the agenda when governments negotiate new conservation goals for the next decade at the UN Convention on Biodiversity next year, says Brooks. “Then we will need to see implementation across all sectors of society,” he says. “That’s when we will see a difference.”\n\nThis article is reproduced with permission and was first published on May 6, 2019.","title":"Humans Are Driving One Million Species to Extinction","origin":"EARTH","image":"https://static.scientificamerican.com/sciam/cache/file/8E9761A8-3113-45F9-95AB4E6AB92758E5_source.jpg?w=590&h=800&C13A4252-656A-4F8A-BE8889F8E993D07C","link":"https://www.scientificamerican.com/article/humans-are-driving-one-million-species-to-extinction/"},{"authors":"Caleb A. Scharf","pub_date":"May 1, 2019","abstract":"The old trope that ‘we are starstuff’ has embedded itself in our minds to such an extent that it tends to lose some of its poetry. Yes, elements heavier than hydrogen and helium in our earthly environment were forged as part of the varied life-cycles of long-gone generations of stars. Many of these cosmic furnaces expunged their guts into the void, polluting our galaxy with traces of the atomic nuclei we call oxygen, carbon, iron and more. And over the eons gravity has caused the re-condensation of this interstellar matter. As a result the elements have been segregated, allowing the starstuff to become extraordinarily concentrated – making new stars, planets, and the clusters of heavy nuclei that constitute human beings and their ridiculous complexity.\n\nIt is truly fantastic, but repeat this tale enough times and it begins to sound a little ordinary. Part of the reason is that the narrative can become vague – from talking in broad terms about earlier generations of now unseen stars, to our loose descriptions of the nature of interstellar matter. It’s a bit like when an aged relative tells you about your extended family tree. There can be little to identify with, even though you really want to make the connection.\n\nThe story gets a lot more interesting when you look closer though. For one thing, not all elements are produced in the same way. Perhaps the most intriguing example is that of the so-called ‘r-process’ elements. These have nuclei heavier than iron and are built by a mechanism called rapid neutron-capture. As the name implies, you need something to capture the neutrons, in the form of ‘seed’ nuclei, and you need a fearsome flux of neutrons – enough coming in fast enough to build up nuclei beyond any highly unstable intermediate configurations.\n\nBut where do such environments exist?\n\nIn 2017 the gravitational wave observatories LIGO and Virgo made headlines by detecting the tell-tale signature of a binary neutron-star merger. Two stellar-mass balls of nuclear material spiralling together with an escalating shriek of spacetime oscillations.\n\nUnlike binary black hole mergers this event churned out a prodigious amount of electromagnetic radiation in what’s termed a kilonova (literally a thousand times the output of an ordinary stellar nova). Telescopic study of the kilonova produced compelling support for a picture where merging neutron stars are an r-process heaven. This suggests that these cataclysmic events play a major role in supplying some of the heaviest elements to our galactic landscape. From gold, platinum, and iridium, to thorium, uranium, and short-lived elements like plutonium.\n\nNow, a new piece of research by Bartos and Marka, published this week in Nature, provides an ingenious and somewhat startling insight to the origins of r-process elements in our own solar system. To achieve this they combine two key analyses. One is data from meteorites that preserve evidence of the elemental mix in our forming solar system some 4.6 billion years ago. The other is a clever statistical model of the Galaxy’s history of neutron star mergers.\n\nWhat the research points to is a very nearby neutron star collision that took place at the dawn of our local cosmic history. Traces of this one event seem to be present in the details of radioisotopes coming from the r-process that got sprayed into our forming system after the neutron stars collided.\n\nReaching this conclusion requires some nimble thinking and tricky foot-work. Neutron star-on-neutron star mergers are cosmically rare in the Milky Way, with between one and a hundred occurring per million years across its entire expanse. Certain r-process elements, like the actinides (including Curium-247, Plutonium-244, and Iodine-129), not only have relatively short half-lives, measured in the tens of millions of years, but have left distinct signatures in ancient solar system meteoritic material that allow us to measure their original abundances. So, the amount of these elements that existed during the window of time that our solar system was forming offers a lever arm on not only how recently those elements had been forged, but also how nearby the forge must have been. \n\nBy constructing a simulation of neutron star mergers across our galaxy, and throughout its history leading up to our solar system’s formation (about 9 billion years into the Milky Way’s existence), Bartos and Marka are able to examine what scenarios could have produced the actinide mix inferred from meteoritic analyses.\n\nThe upshot is that it appears that there was a single kilonova from a neutron star merger that occurred within 80 plus-or-minus 40 million years of the formation of the solar system and was about 1,000 light years away. The researchers estimate that such a nearby kilonova event would outshine everything in the night sky for over a day. Four and a half billion years ago, as the merger's freshly made elements exploded outwards and diffused across interstellar space, a total of about 1020 kilograms wound up being deposited into our young system.\n\nFrom there you can work out how much of the Earth’s repository of r-process elements came from that one event. For example, about one eyelash’s worth of the iodine in your body will have come from those neutron stars. A Tesla Model 3 contains a total of about 5 grams of the nuclei generated by this specific neutron star merger. A modern fission reactor, utilizing enriched uranium, will have about 200 kilograms of material that was produced in this singular cosmic explosion.\n\nCritically, this study also seems to rule out events such as core-collapse supernova – where massive stars implode – as the primary producers of r-process elements across the Galaxy. Those events, which occur hundreds or even thousands of times more frequently than neutron star mergers, just don’t seem to fit the evidence.\n\nTaken altogether it looks like we can update the tale of our origins in ‘starstuff’. Not only are we indebted to even more esoteric and extreme physics than we perhaps imagined, we now have two very specific members of our ancestral tribe to put on the family tree, a pair of neutron- star-crossed lovers whose embrace literally ended in fire.\n\n[Full disclosure: I am acknowledged in the paper by Bartos and Marka, and they are both colleagues. But my contribution to their work was entirely in the form of making encouraging noises.]","title":"Gravitational Waves, Nuclear Fire, Rocks and Love","origin":"Life, Unbounded","image":"https://static.scientificamerican.com/blogs/cache/file/50307351-5A2D-4F63-A661337F6E73D952_source.jpg?w=590&h=800&F2846B68-0582-4915-956FCE93DA029A41","link":"https://blogs.scientificamerican.com/life-unbounded/gravitational-waves-nuclear-fire-rocks-and-love/"},{"authors":"John Rennie, Quanta Magazine","pub_date":"May 6, 2019","abstract":"From Quanta Magazine (find original story here).\n\nIn February, a genomics study appearing in Nature Ecology & Evolution drew attention to the bizarre Antarctic blackfin icefish, which swim in the brutally cold waters off the coast of the southernmost continent. The icefish of the Channichthyidae family are unusual in several ways—they lack scales and have transparent bones, for example—but what stands out most is their so-called white blood, which is unique among vertebrates. These fish are the only ones known to have neither red blood cells nor hemoglobin pigments for transporting oxygen. Oxygen simply diffuses into their circulating blood plasma from the frigid seawater by way of the fish’s enlarged gills and smooth skin.\n\nBy looking at the genome of one icefish species, the researchers were able to peek at the evolutionary adaptations that allowed it to survive. Some were common to red-blooded fish that are also native to Antarctic waters, like the presence of extra genes for making blood proteins that act like antifreeze. Some were more distinctive to the icefish’s lack of red blood cells, such as a boost in the enzymes that protect  tissues from the highly reactive free oxygen in its blood.\n\nOdd as the icefish may seem, what makes it peculiar among vertebrates is the norm across the rest of the animal kingdom. Most invertebrates carry genes for hemoglobins, but they generally use other metalloprotein pigments in their versions of blood. Insects, crustaceans and other arthropods use hemocyanin, a bluish copper-based pigment. Mollusks, ranging from clams to squids and octopuses, use hemocyanin, too, but they seem to have invented their version of it independently. Some worms use purplish hemerythrin; others use greenish chlorocruorin; some use a combination of pigments.\n\nIt may seem puzzling that so many varieties of blood exist, and more puzzling still that while invertebrates have experimented wildly, vertebrates—aside from the icefish—have stayed universally loyal to the kind with red cells and hemoglobin. The explanation is deeply entrenched in the history of life, going back to the earliest cells.\n\nAn Affinity for Oxygen\n\nFrom the very beginning of life, cells needed to move electrons around between molecules as part of their metabolism, explained Ross Hardison, a professor of biochemistry and molecular biology at Pennsylvania State University. As controls over these redox (oxidation-reduction) reactions, cells deployed ring-shaped molecules called porphyrins. When these porphyrins held a metal atom like iron or copper, they had a ferocious affinity for oxygen. “Once you have an iron in that porphyrin ring, it’s used throughout the biosphere,” Hardison said. He speculated that it “might be one of the earliest molecules that eventually got incorporated into cells.”\n\nHemoglobin arose out of four interlinked globin proteins, each holding a heme, and it rapidly became ubiquitous. “Hemoglobins predate the origin of animals and even predate the common ancestor of animals and plants,” said Mark Siddall, a curator in the division of invertebrate biology at the American Museum of Natural History.\n\nWhen respiring animals were only a few cells thick, they could count on diffusion to satisfy their needs for oxygen. But when they grew too bulky for simple diffusion to continue to oxygenate their tissues, hemoglobin was ingeniously ready for the job.\n\nThe secret of hemoglobin’s success is collaborative bonding: With every oxygen molecule that the pigment binds, it can bind to the next one more easily, until all four vacancies are filled. This makes hemoglobin extremely efficient at collecting oxygen where it’s abundant (as in the open air and in lungs) and then releasing it again gradually in oxygen-starved tissues.\n\nVertebrates typically carry genes for several variant globin proteins with finely tuned uses. For example, fetal mammals have a special hemoglobin in their blood with extra affinity for oxygen, which helps them to draw oxygen out of the maternal blood supply in the placenta. Our skeletal muscles make myoglobin, a single globin protein ancestral to hemoglobin, which helps muscle hang on to a reserve of oxygen to use during exercise.\n\nBut as good as hemoglobin is, it’s not the ideal molecule for transporting oxygen in all circumstances. Consider hemocyanin, which is so widely used among invertebrates. Hemocyanin is less efficient than hemoglobin at grabbing oxygen because it, like the other hemoglobin alternatives, usually does not bond collaboratively. But the disadvantage of collaborative bonding is that hemoglobin performs worse when oxygen is in short supply. Hemoglobin’s effectiveness also drops with temperature. Consequently, for creatures like octopuses and crabs that live on or near the cold ocean floor, hemocyanin may be a more practical choice.\n\nFor insects, it’s different. Their equivalent to blood is hemolymph, a mostly clear fluid that contains small amounts of hemocyanin. But they generally don’t rely on this hemolymph to transport oxygen. Most insects breathe through a network of “tracheal tubes” that pervade their tissues and connect to the air through openings in the exoskeleton. The “open” circulatory system of insects doesn’t have vessels like capillaries to direct the hemolymph; instead, the hemolymph sloshes through the body cavity and helps to distribute dissolved nutrients. The hemocyanin may be in the hemolymph just to help insects store oxygen for later use.\n\nHemerythrin, the blood pigment found in annelids (segmented worms), leeches and certain other worms, has a deceptive name, because it contains no heme at all. However, like hemoglobin, it is an iron-based pigment descended from a family of ancient proteins that early bacteria used to control redox reactions. Hemerythin has only about one-quarter the oxygen capacity of hemoglobin, though this seems to serve the worms adequately. The pigment also seems to have some immunological functions.\n\nA Toxic Triple Threat\n\nEven if the alternative blood pigments are generally a poor second to hemoglobin at grabbing oxygen, they do have an advantage in terms of simplicity: They usually don’t need something like a red blood cell to hold them. In squids, lobsters and the other blue-blooded animals, for example, hemocyanin is dissolved directly in their plasma. This approach works because hemocyanin, hemerythrin and the other pigments are big, frequently polymerized molecules that keep their oxygen-binding metal atoms tucked away from casual interactions. Conversely, hemoglobin is small and its aggressively reactive heme is easily exposed, which makes it highly toxic—so much so that our livers make a protein, haptoglobin, to scavenge stray hemoglobin from broken blood cells out of our blood.\n\n\n\nHuman red blood cells are elastic, biconcave disks filled with the red pigment hemoglobin. Their design helps them to transport oxygen to tissues efficiently while also safely isolating the contained hemoglobin, which can become toxic outside of cells. Credit: David McCarthy Getty Images\n\n\n\nFrom a toxicity standpoint, hemoglobin is a triple threat, explained Pampee Young, the chief medical officer of biomedical services for the American Red Cross. Heme has even greater affinity for nitric oxide than oxygen, and the body uses nitric oxide as a signaling molecule to control blood pressure. Excess free hemoglobin will therefore rob the blood of nitric oxide, constrict blood vessels and potentially cause hypertension and reduced blood flow to the organs. Compounding the problem is that hemoglobin, when unprotected in blood plasma, decomposes into its component globin subunits. The naked heme molecules then randomly attack the lipid membranes and other structures in the tissues, damaging them. And as a coup de grâce, the isolated globin proteins can clog the filtration system of the kidneys and shut them down.\n\nPackaging hemoglobin into red blood cells (erythrocytes) helps to contain the toxicity problems. It also makes the distribution of oxygen more efficient by keeping the hemoglobin inside the blood vessels: The molecule is otherwise so small that some of it would leak out into the tissues and fall out of circulation.\n\nThe Perils of Justifying Evolution\n\nHuman red blood cells are particularly optimized for the job of oxygen distribution. They are compact, flexible and shaped like biconcave disks, which helps them slip through narrow capillaries and gives them a high volume-to-surface area ration, so they can hold a lot of hemoglobin and oxygen. Moreover, human erythrocytes go a step further than those in most species by ejecting their nucleus and other organelles after stockpiling all the proteins they will need for the balance of their lives—what’s left is “basically a bag of hemoglobin,” Young said. The cells pay a penalty for that streamlining, however: Because of their limited ability to repair the wear and tear of pushing through capillaries, circulating human red cells have a lifespan of only about 120 days.\n\nWhen red cells die, the body converts the hemoglobin down into somewhat less toxic compounds including the green pigment biliverdin. (The green color of a healing bruise is from biliverdin.) Too much biliverdin in a human causes jaundice, but biliverdin is normally present in the blood of certain insects and fish, even though it does not transport oxygen. Last year, the herpetologists Christopher Austin and Zachary Rodriguez of Louisiana State University and Susan Perkins, a parasite researcher in the division of invertebrate zoology at the American Museum of Natural History, reported on their genetic analysis of certain skinks from New Guinea that have so much biliverdin in their blood that its green overpowers the hemoglobin’s red. (“They have something like 50 times the amount of biliverdin that it would take to kill a human being,” Perkins said.) Genetic evidence suggests that this trait evolved four separate times among these lizards, which led researchers to think that the biliverdin might help protect the skinks from malaria or other parasitic infections. Unfortunately for that theory, preliminary evidence suggests that’s not the case, Perkins said, which leaves it mysterious why evolution favors the trait so much in this one small group.\n\nThe green blood of the skinks illustrates the perils of trying to justify the variety of blood pigments in nature as purely adaptive. Much of evolution depends on historical contingency, too. The earliest organisms had many oxygen-controlling pigments at their disposal. But once lineages of organisms committed to using certain ones for certain jobs, it may have been difficult if not impossible for them to drastically revise that choice. The reason that vertebrates show less diversity in their blood pigments than invertebrates do is simply that invertebrates are a much more diverse group of organisms overall (all vertebrates fall within a single phylum, Chordata, while invertebrates are in more than 30).\n\nThe unusual blood of icefish doesn’t contradict this generalization; it actually confirms it. When biologists discovered that icefish had clear blood in the 1950s, they at first assumed it was an adaptation to the cold. Subsequent work, however, pointed to the icefish’s loss of hemoglobin genes as more of a lucky accident. In most environments, that mutation would have been fatal. But because the frigid Antarctic waters hold more dissolved oxygen than warmer water does, and because the ancestors of icefish probably already had some adaptations to help them prosper in the cold, the fish survived. It may be true, as Louis Pasteur said, that chance favors the prepared mind, but having a well-prepared genome doesn’t hurt.\n\nReprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.","title":"Icefish Study Adds Another Color to the Story of Blood","origin":"Evolution","image":"https://static.scientificamerican.com/sciam/cache/file/516BE823-D4F3-41F4-9347705F8DCE43EB_source.png?w=590&h=800&A3EAC248-C186-46C8-95F97D9B5E5A4A72","link":"https://www.scientificamerican.com/article/icefish-study-adds-another-color-to-the-story-of-blood/"},{"authors":"Alex Hanna, Nikki L Stevens, Os Keyes, Maliha Ahmed","pub_date":"May 3, 2019","abstract":"Abigail Saguy and Julie Williams recently published an op-ed on the Scientific American website advocating that we begin addressing everyone with gender-neutral pronouns (“they,” “them”) rather than binary-gendered pronouns (“he,” “him” or “she,” “her”). They assert the act of gendering highlights gender when it is not relevant in social interaction.\n\nFor instance, they draw on data from audit studies showing that when a job candidate is identified as a woman, this identification puts the candidate at a stark disadvantage. They draw on psychological work which suggests that even the act of drawing attention to one’s own gender can lead to poorer performance on cognitive tests.\n\nAs a corrective, they suggest something of a “universal design” approach. They argue that using the gender-neutral third-person pronoun “they” could reduce discrimination and bias in social situations, and even reduce the cognitive load on the person being gendered. As a parallel, Saguay and Williams offer the quick adoption of “Ms.” as a model for moving rapidly past restrictive gender norms, and advocate for a similar adoption of “they/them” pronouns.\n\nWe are writing here as as a mix of queer, nonwhite, non-American, bicultural, trans people, and we share Saguy and Williams’ political bent. We firmly understand that the impacts of gender are felt unequally, in light of both our lived experience and our research in sociology, gender, technology, and linguistics. Transgender and gender-nonconforming (abbreviated below as trans/GNC) people, particularly transfeminine people of color, are disproportionately and in some ways uniquely victimized by the rigid and specific ways that Western society views, constructs and penalizes nonheteronormative and noncisnormative gender performances and identities. This unique position with regards to the structures of gender is precisely why scholars of gender have been studying and examining trans/GNC lives for decades.\n\nHowever, we strongly disagree with their proposal. Fundamentally, a move toward gender-neutral pronouns ignores the important work that gendered pronouns perform in everyday life. For many trans/GNC people, gender is an important part of their identity and actively avoiding the act of gendering manifests as another form of violence—a violence that trans/GNC people have been fighting against throughout the long history of lesbian, gay, bisexual, transgender, queer, intersex, asexual and two-spirit (LGBTQIA2S) experience.\n\nFurthermore, this proposal is embedded in a particularly Western approach to language. Languages that don’t have gendered pronouns are still riddled with strong gendered norms. In some ways, those languages make it even more difficult to signify a confirmation of trans/GNC identity. Lastly, it’s important that in this kind of research, researchers treat trans/GNC subjects as active agents in the production of knowledge, rather than as objects of study. We worry that this research, although it claims to engage with LGBTQIA2S people, has failed to do so in a way that encounters us as people, rather than as conceptual stand-ins.\n\nFirst, this proposal isn’t well-founded in prior research, nor is the proposal supported by how Saguy and Williams characterize their own data. Their qualitative research suggests that announcing pronouns signals to trans/GNC people that they “are going to be welcome in this space.” However, based on these data, Saguy and Williams suggest that the inverse should be true: that taking pronouns out of the equation altogether would avoid a gendered interaction and therefore make more situations inclusive.\n\nThis view contradicts both our own experiences and scholars’ prior research on misgendering. Misgendering—that is, addressing someone by the incorrect pronoun or honorific—is a form of microaggression. The act of misgendering denies the gendered and human legitimacy of trans people, and causes significant negative psychological effects, including reduced sense of self-worth, anxiety, depression and a feeling of hypervigilance and surveillance.\n\nSecond, this proposal takes a Western-centric view of language and ignores the fact that there are languages that already exist without gendered pronouns. Bangla, for instance, is such a language. However, this doesn’t have the effect of reducing gender inequality, nor does it reduce the cisnormative desire to categorize by gender. Bengali speakers, like English speakers, will violate conversational norms by asking extra questions to find out someone’s gender, and will rely on gendered nouns to provide information that gender-neutral pronouns will not (Bondhu or bandhobi? A male friend or female friend?).\n\nMoreover, Bengalis still cue their gender in myriad ways other than pronouns; women are still looked down upon for having feminine signifiers. These effects don’t disappear when gendered pronouns are removed. Getting rid of gendered pronouns as a means of addressing gender inequalities would be the equivalent of treating symptoms without treating the root of the illness. Enforcing ungendered pronouns will not necessarily trigger a change in someone who is unconsciously biased against women and feminine people, but it would contribute to harm of trans/GNC people by taking away one of the more usefully subtle ways to discuss topics where gender is contextually salient.\n\nLastly, for authors talking about alleviating gendered discrimination and oppression, Saguy and Williams manage to get almost all of the way through their piece without mentioning trans/GNC existences. When they do, it is phrased euphemistically: “people who have experienced the pain of being denied gender recognition in the past.” This phrasing elides the fact that gendered terms of address are not meaningless—if they were, their elimination would presumably not be sought—but deeply meaningful, particularly to people who are not represented in Saguy and Williams’ narrative.\n\nThis evasion is presumably because considering trans/GNC needs, talking to trans/GNC people and taking trans/GNC experiences of gender seriously would reveal a much more complicated narrative. For instance, Saguy and Williams cite evidence that women will seek gender-anonymity in online spaces to avoid gendered judgements. However, Oliver Haimson and other scholars who work at the intersection of trans/GNC issues and technology point out how online spaces offer affordances that allow those people to explore their gender expression and identity.\n\nConsidering trans perspectives would reveal a range of concerns with these proposals, mostly centered around their practicality and tmaterial consequences. Suppose we were to take their suggestion seriously, and begin insisting on gender-neutral pronouns everywhere. What would happen then? What would it mean for those of us who have fought to have our gender recognized as women, who have fought seemingly endless amounts of administrative red tape—what transgender legal scholar Dean Spade calls administrative violence—to have that erased? And how does this proposal factor in the history of misogynistic and trans-erasing narratives in this world of scholarship—a history that has often consisted of punishing and shaming trans women for asserting femininity, with the precise argument that this is somehow reinforcing the patriarchy?\n\nTaking away gendered pronouns is premised on the idea that simplification will lead to a flattening of gender disparity, but this work must consider the real-world conditions of the people who are the subject of multiple intersecting oppressions—of sexism, racism, transmisogyny and poverty—and begin with their concerns, rather than moving away from nuance of their lived experience.\n\nIt must begin with substantial, dramatic alterations that improve lives and factor in the concerns of trans/GNC people, particularly trans/GNC people of color, rather than changes that apply a universalizing view to what gender is and who is subject to it. We invite Saguy and Williams to revisit their data with an eye towards a liberatory politics that engages with and amplifies these voices, rather than papers them over with a bland and homogenizing universalism","title":"Actually, We Should Not All Use They/Them Pronouns","origin":"Voices","image":"https://static.scientificamerican.com/blogs/cache/file/C5415894-6CAC-4DEE-BFC46DA85B7EC291_source.jpg?w=590&h=800&CA754A03-70B8-4535-AD585983DA3D3E8C","link":"https://blogs.scientificamerican.com/voices/actually-we-should-not-all-use-they-them-pronouns/"},{"authors":"Lydia Denworth","pub_date":"May 6, 2019","abstract":"Swedish teenager Greta Thunberg became famous this spring for launching a student movement to compel adults to take action on climate change. Instead of going to school, Greta has been spending her Fridays in front of the Swedish parliament with a sign reading: “School Strike for Climate.” Students in more than 70 countries have since followed her lead. But before she started trying to convince the world to take action, Thunberg worked on her parents. She showered them with facts and showed them documentaries. “After a while, they started listening to what I actually said,” Thunberg told the Guardian newspaper. “That’s when I realized I could make a difference.”\n\nThunberg is not alone. Other young people can be equally convincing, according to a paper published May 6 in Nature Climate Change. The team of social scientists and ecologists from North Carolina State University who authored the report found that children can increase their parents’ level of concern about climate change because, unlike adults, their views on the issue do not generally reflect any entrenched political ideology. Parents also really do care what their children think, even on socially charged issues like climate change or sexual orientation.\n\nPostulating that pupils might be ideal influencers, the researchers decided to test how 10-to-14–year-olds’ exposure to climate change coursework might affect, not only the youngsters’ views, but those of their parents. The proposed pass-through effect turned out to be true: teaching a child about the warming climate often raised concerns among parents about the issue. Fathers and conservative parents showed the biggest change in attitudes, and daughters were more effective than sons in shifting their parents’ views. The results suggest that conversations between generations may be an effective starting point in combating the effects of a warming environment. “This model of intergenerational learning provides a dual benefit,” says graduate student Danielle Lawson, the paper’s lead author. “[It prepares] kids for the future since they’re going to deal with the brunt of climate change’s impact. And it empowers them to help make a difference on the issue now by providing them a structure to have conversations with older generations to bring us together to work on climate change.”\n\nScientists in the field find the study heartening. “These encouraging results suggest that not only are children increasingly engaged in advocating for their future, they are also effective advocates to their parents,” says climate scientist Katharine Hayhoe of Texas Tech University. She was not involved in the research but works to bridge the gap between scientists and stakeholders on the issue. “As a woman myself and someone who frequently engages with conservative Christian communities,” she says, “I love that it’s the daughters who were found to be most effective at changing their hard-nosed dads’ minds.”\n\nThe intergenerational model is “a promising avenue for those of us in climate change education,” says Nicole Holthuis, a researcher in science education at Stanford University, who was not a researcher on the study. Too often, Holthuis says, scientists and educators believe that delivering the facts of global warming will be enough to change minds. “With this study,” she says, “they’re addressing a critical need to acknowledge that the sociopolitical aspects of climate change make it very difficult for people to take [the facts] in. Maybe we can leverage these intergenerational relationships in ways that can be very productive.” As a next step, Holthuis would like to see if increasing levels of concern from this curriculum translate into actual changes in behavior. Child-focused lessons on a similar issue did alter parents’ actions. A 2016 study of Girl Scout troops found that an educational program on energy consumption resulted in reduced energy use by their families.\n\nIn the North Carolina study, the curriculum consisted of four classroom activities and a field-based service-learning project. Of 238 families in that study, 92 served as controls; those children’s teachers did not use the new curriculum. Parents were invited to view outdoor projects and were interviewed by their children. Instead of addressing climate change directly, children asked adults about local changes they might have noticed. Parents, says Lawson, responded to a series of questions from their children: “How have you seen the weather change? Have you ever seen the sea-level rise? We wanted to take climate change out of it just to make it more ideologically neutral.” At the beginning and end of the study, parents were surveyed on demographic characteristics such as age and political ideology as well as their views on climate change.\n\nConcern about the issue was measured on a 17-point scale from least concerned (–8) to most concerned (+8).  Over two years, levels of concern increased among all parents, including those in the control group. But those who engaged in the curriculum with their children showed larger increases and parents who identified as male or conservative more than doubled their level of concern about climate change from relatively unconcerned (–2) to relatively concerned (+2).\n\nLawson believes that conversations about climate change were easier because of the level of trust between parents and their children. “That doesn’t necessarily exist between two adults talking to each other,” she says. The authors do not know why girls were more effective than boys but suggest that girls may have been more concerned to begin with or are better communicators in this age group than boys. While this paper doesn’t measure behavioral change, it does provide hope, says Lawson, “that if we can promote this community-building and conversation-building on climate change, we can come together and work together on a solution.”","title":"Children Change Their Parents’ Minds about Climate Change","origin":"Behavior & Society","image":"https://static.scientificamerican.com/sciam/cache/file/EA4C1652-EFB6-4FDA-AD3CE82735A11A77_source.jpg?w=590&h=800&AB186ED3-7FAC-4789-994ADB3F8E5D2710","link":"https://www.scientificamerican.com/article/children-change-their-parents-minds-about-climate-change/"},{"authors":"Starre Vartan","pub_date":"May 6, 2019","abstract":"Every day an average of 18 people die waiting for an organ transplant in the United States. Donated organs are tough to come by, which is why many scientists have spent the last two decades trying to create new livers, kidneys, hearts or lungs from scratch. One potential way to craft such delicate structures is 3-D printing with biologically compatible materials, or bioprinting—which has now reportedly produced functional models of lung and liver tissues, with a little help from an unconventional ingredient: food dye.\n\nWould-be organ printers previously have been stymied by the complexity of certain organs. Our lungs and livers, for example, contain physically and biochemically entangled networks of blood vessels and airways (in the lung) or bile ducts (in the liver). Being able to recreate this vasculature—and make the fluid dynamics work so blood and other fluids flow properly—has been an ongoing challenge.\n\nNow, a team of researchers from the University of Washington and Rice University say they have produced functional tissue models using a 3-D printing technique called projection stereolithography. This method exposes thin layers of liquid resin to blue light, which solidifies them into intricate arrangements of hydrogels—gels made up of tangled strings of polymer molecules. These form a structural “scaffolding,” into which researchers can implant live cells that enable it do the work of a lung or liver. In the new study the implanted cells survived, and the resulting models of organ tissue demonstrated some functions of the real thing. The results were published last week in Science.\n\n“This is definitely a major advance in our ability to create 3-D-printed structures that approximate normal tissue,” says Anthony Atala, the director of the Wake Forest Institute for Regenerative Medicine, who was not involved in the new study.\n\nThe basic technology of projection stereolithography has been around since the 1980s, but “it wasn’t designed with biology in mind; it was used to make plastic structures,” says Jordan Miller, assistant professor of bioengineering at Rice’s Brown School of Engineering and a co-author of the new paper. The technique can produce finer layers than standard 3-D printing, and is faster, too. “Instead of creating one layer in minutes by extrusion, we can do it in seconds” with stereolithography,” Miller says. That speed is crucial: since the printed structure ultimately channels oxygen and nutrients to the cells, faster work means fewer cells die in the process of making it.\n\nBut there was a challenge. This type of printing process relies on photoreactor chemicals (ones that respond to light), so that certain preprogrammed areas of the liquid will solidify while other areas remain soft and can later be washed away. Unfortunately, many of these chemicals are carcinogenic. For a 3-D printer to create the fine vasculature an organ requires for nutrient delivery and waste removal, it needs the precision offered by stereolithography; but for transplants it would need safe, water-soluble photoreactors.\n\nSo, the researchers had to find a replacement for the proven but toxic chemicals. When Miller and his team guessed food dye might do the trick—they knew it would absorb the right light wavelengths to make the 3-D printing process work, and is relatively biocompatible—they were too impatient to wait for a supplier to ship the ingredient. So, Miller says, “I went to the supermarket, and I bought a kit of food coloring dye that people use to make confectionery.”\n\nIt worked. First, the team colored liquid polymers with the food dye yellow no. 5, or tartrazine, and then had the printer’s projector shine blue light on it. This induced a local chemical reaction that solidified the liquid. Because the printer projected light in a preprogrammed pattern, it created a design that hardened into a thin but tough biological structure. “We were screaming with joy, because it was stunning how simple an idea it was; it immediately enabled us to make this dramatically more complex architecture,” Miller says.\n\nYellow no. 5, found in many snack foods, had another advantage: It easily rinsed off the bioprinted structures, leaving a clear framework ready to nurture whatever cells the scientists filled it with. The traces of the dye that remained were not expected to affect cell health. (Studies suggest yellow no. 5 does not affect sperm count, as has been rumored; it might, however, exacerbate preexisting hyperactive disorders in children.)\n\nTo the Test\n\nAlthough researchers have bioprinted tissues before, they have been unable to keep cells alive long enough. The latest study had to test the newly printed scaffolding in this regard, and red blood cells were a simple way to start.\n\nThe team created a scale model of an air sac mimicking a crucial part of a lung’s complex vascular network. It included one passage for air and separate channels for blood cells. In a healthy human lung, these two structures exchange oxygen without ever touching. The model performed the same feat, keeping the blood cells alive. It also proved sturdy enough to retain its structure as a simulated “breath” expanded and contracted the printed tissues.\n\nNext, the researchers tested a model of liver tissue. Part of the printing process here included injecting specialized liver cells called hepatocytes into the printed structure. The team implanted the artificial liver tissues into live mice with chronic liver injury, as well as non-injured mice, then tested them. A fully functioning liver has over 500 functions and in this case they examined just one, but it did prove successful—and the hepatocytes survived in the living mice.\n\nThe new printing method also produced working intravascular valves, which play a key role in the heart and leg veins. In tests the printed versions maintained their structure as fluid flowed through them, and they kept it from moving backward through the valves.\n\nPrinted Organs Open Up\n\nSo how long until bioprinted organs are available to those on transplant lists? Scientists still have a lot to figure out—starting with the basics, such as determining the optimal base hydrogel. What kind of protein works best? And should additives such as growth factors be used to speed the process? “Now we can start methodically varying these factors to see which are more important—and asking how this affects the functions of the cells,” says paper co-author Kelly Stevens, an assistant professor in the University of Washington’s bioengineering and pathology departments. Then there is the question of how best to build the scaffolding, and how much printed material could realistically replace tissue. “Those are questions that this new leap in tech enables us to ask for the first time,” Stevens says.\n\nThe researchers did not want to be the only ones trying out these possibilities—so they made their technology open-source, allowing other bioengineers to test their own applications. “Bioprinting being open-source really helps to accelerate this technology—it really advances the field faster,” Atala says. He plans on applying the findings to a number of organ tissue structures his team is working on.\n\nOther would-be organ-builders can buy specialized printers and inks—Miller and some of the paper’s other collaborators have founded a start-up, called Volumetric, to sell these materials—or can replicate the work themselves. Miller says sharing the DIY option was important to him. “We really are excited,” he says, “about opening up a new set of design freedoms in bioprinting.”","title":"Can 3-D Printing Produce Lung and Liver Tissue for Transplants?","origin":"Medical & Biotech","image":"https://static.scientificamerican.com/sciam/cache/file/7822E89E-71FC-4C5C-91573E777B3EDDE3_source.jpg?w=590&h=800&F882AEFF-298E-4417-9708517242D40B73","link":"https://www.scientificamerican.com/article/can-3-d-printing-produce-lung-and-liver-tissue-for-transplants/"},{"authors":"Remi Shaull-Thompson","pub_date":"May 7, 2019","abstract":"Lucina and I crossed the Millennium Bridge discussing a man who had broken her heart, and eating candied peanuts, which smelled far better than they tasted. From the opposite side of the bridge, a fragrant plume of cinnamon sugar with hints of warm nuttiness drew us over the steady Thames. At the river’s center, a hearty man with a stainless-steel cart churned a vat of glistening brown syrup and scooped crystalized peanuts into rows of clear plastic cups. For a pound, he offered us the fresh, piping hot batch. It was our final week as visiting students in London, and this chill, clear day was a feast.\n\nAs an English literature student, I was obligated to spend my days neck deep in medieval verse. Most days, however, “endless blisse” could not come soon enough, and I opted to walk the streets instead, often accompanied by a classmate like Lucina. That day with her, as we took the South Bank walk along the Thames, making our way through Trafalgar and eventually to Soho, London was more alive to me than ever. It was a week from Christmas, and the streets were bustling with shoppers and brightened with lights, banners, and carolers. The work of the semester was nearly over, and a nostalgia for my time there had begun to sink in.\n\nThe modernity of London dazzled me, from its seamless transport system to its bizarre skyscrapers and diverse neighborhoods. Conversely, for all the striking features of development around the city, there was always a stately local pub or moss-covered ruin just around the corner. Lakes were spotted with swans and broached with womanly stone fountains, and canal narrowboats skimmed through green water and willow drapery. So, even if I hadn’t been reading such ancient texts, the city of London felt, as it must surely to all visitors, pervasively and tangibly old.\n\nThrough it all ran the Thames—as many have noted, old and steady yet ever surging, changing, like London itself. Moving briskly, perhaps still discussing love, Lucina and I passed under the Southwark Bridge and noticed an engraved stone mural along the wall. It depicted the Thames, its waters unexpectedly a setting for snowflakes, men pulling boats on wheels, market stalls and ice-skaters. An inscription that danced along its length read:\n\nBehold the Liquid Thames frozen o’re,\n\nThat lately Ships of mighty Burthen bore…\n\nAnd lay it by that ages yet to come\n\nMay see what things upon the ice were done.\n\nI had never seen snow pile up in London or even witnessed anybody wearing real winter boots. The only “winter wonderland” I had experienced in London was the alpine-themed amusement park that had sprung up for the season in Hyde Park. In fact, these friezes, by the artist Richard Kindersley, depict a tradition now lost to London: the Thames frost fairs.\n\nEvery twenty years or so from the 17th to the early 19th centuries, temperatures dropped low enough in London to cause the River Thames to completely freeze over. And with the Thames frozen thick enough to walk on, the miserable cold was transformed into a cause for spontaneous celebration.\n\nTake, for example, the 1814 frost fair, the last of its kind. When the ice solidified that winter, people flocked to the riverbanks and slowly gained the confidence to venture out. A thoroughfare dubbed “City Road” was established and lined with colorfully decorated shops and booths. Trinkets were sold at three times the normal price. Ten printing presses were there to document the occasion. Books and toys were sold; there was dancing, skating and music, and plenty of beer, gin, gingerbread and roasted meat. There was even an elephant. After two days’ worth of revelry, the river began to thaw again, and the crowd thinned, escaping the danger of the cracking ice.\n\nIt was not all fun and games when the Thames froze. Its bustling ports became unusable, and trade stood still. In fact, people made up for lost profits by opening fair stalls. The ice did thousands of pounds in damages to houses, shops, boats and bridges. And worst of all, people sometimes fell through the ice and drowned. Many froze to death in the city and in even greater numbers in the countryside.\n\nFor a moment, though, the people of London took advantage of the opportunity afforded by a perilous phenomenon to create a public place infused with a rare magic. In her novel Orlando, Virginia Woolf describes the beauty of a Thames frost fair. In 1608, the king orders the frozen river to be prepared for his citizens with “arbours, mazes, alleys, drinking booths,” and conducts matters of state and war right there on the ice under “plumes of ostrich feathers.” In a beautiful passage, Woolf writes:\n\n“Frozen roses fell in showers…. Coloured balloons hovered motionless in the air. Here and there burnt vast bonfires of cedar and oak wood, lavishly salted, so that the flames were of green, orange, and purple fire … there could be seen, congealed at a depth of several feet, here a porpoise, there a flounder…. But it was at night that the carnival was at its merriest … the nights were of perfect stillness; the moon and stars blazed with the hard fixity of diamonds, and to the fine music of flute and trumpet the courtiers danced.”\n\nIn this fantastical, timeless landscape, the protagonist Orlando meets the first true love of his life, a Russian princess he calls Sasha. His surmounting adoration of her, as he strives for the perfect poetry to describe her, mimics the elaborate and elegant construction of the fair. Likewise, the improbability of their union is mirrored by the fair’s surreal and rare occurrence, a scene of suspended time and reality. So that, when Sasha leaves Orlando, the ice that had been 20 feet thick for months suddenly fissures, sweeping floes, people and cats down the frigid river to their certain demise. Orlando stands on the bank, furious and heartbroken, watching it all surge away.\n\nThough likely not as ornate or stately as Woolf’s, there were many fairs, each of which offered its own innovations and diversions. Some popular pastimes from the 1607–8 fair included throwing rocks at chickens, getting a shave from the barber, and bowling. Bull-baiting was recorded in 1688–9, a bloody sport in which dogs are set against a tethered bull. Bull-baiting turned to bear-baiting in 1788–9, a century later. That fair also saw a menagerie and puppet shows.\n\nThe goings-on at the outdoor Christmas markets I visited in London were not as riotous, but something of the spirit of the frost fairs seems to live on in them. So, what caused the Thames to freeze so thoroughly back then when today it barely so much as snows in London? What allowed for the frost fairs, and why haven’t they—like many of the city’s other relics—stuck around?\n\nFor one, the frost fairs occurred during a period of cold weather referred to as the Little Ice Age. In the period from the 15th to the mid-19th centuries, the average global temperature dropped by half a degree Celsius. Some places changed more than others, particularly the northern latitudes. The annual mean temperature in England was almost full degree C colder than it was in the following period of 1920–60. Elsewhere, glaciers expanded and overtook villages. In addition to the cold and ice, sickness and famines also claimed lives.\n\nScientists do not fully understand what caused the Little Ice Age. Data from tree and ice cores suggests that there was a drop in solar energy—incoming radiation from the sun. The sun’s activity, including the radiation it gives off and the number of sunspots, fluctuates periodically. It could have been volcanoes too. There is evidence that there were more volcano eruptions after 1200; the ash volcanoes spew into the air cools the earth by blocking out the sun.\n\nSo, it would seem that a natural trend towards warming after the 1900s accompanied by the warming effect of anthropogenic climate change is what makes frost fairs in 2018 impossible. But that’s not the whole story. The freezes in fact had as much to do with fluctuations in Earth’s climate as they did with architecture.\n\nThe London Bridge you see today—the location of the old frost fairs—is not the same bridge you would have seen as one of the fair’s attendees. You would have seen the original Old London Bridge, made up of almost 20 narrow arches and capped with homes, shops and a bustling street. The many narrow arches of the Old London Bridge—compared with the three wide arches of the current London Bridge—help explain why the Thames used to freeze over. The arches caused the water to flow at a much more glacial pace, and still water freezes easier than flowing water. Additionally, when less water made its way up from the sea, the lowered salt content raised the water’s freezing point. Sometimes ice floes would get stuck in the arches and block the river entirely.\n\nSpeaking in terms of vast geological time, the ways in which Earth’s climate has historically regulated itself and the ways in which it has so dramatically shifted can make the concept of anthropogenic climate change seem insignificant and pointless. Why bother thinking about climate change when another ice age could pop in any day now? Why bother thinking about climate change if we have no control over our broader planetary fate?\n\nBut the situation surrounding the Thames frost fairs offers a compelling reimagining of scale, particularly regarding how we contextualize human actions in relation to the cosmos. Those Londoners were trapped in a particularly cold era. They were, like all living things, at the mercy of the elements. And so, the fairs stand as a testament to the human capacity for joy, resilience and creativity in defiance of troubled times.\n\nHowever, the freezing of the Thames was a largely human-made environmental condition. I like to imagine that if the people of London had known about the bridge’s role in the freezing of the Thames, they would have considering rebuilding. Because no matter how grand the fair was for those with the privilege to enjoy it, the city could have been altered in a way that saved the lives and livelihoods of those worst affected by the freeze.","title":"\"Frost Fairs,\" the Little Ice Age and Climate Change","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/102052E4-9F63-4533-B6C16C6DE6A0B83A_source.jpg?w=590&h=800&0A862965-0C2F-4297-B13D9BEC00C8301F","link":"https://blogs.scientificamerican.com/observations/frost-fairs-the-little-ice-age-and-climate-change/"},{"authors":"Brian Switek","pub_date":"April 30, 2019","abstract":"Who looks like a paleontologist? Spend enough time in the field, or wandering the conference halls of a Society of Vertebrate Paleontology annual meeting, and you’ll likely come away with the impression that a paleontologist can look like just about anybody. Granted, there is unevenness when it comes to representation in the field, but, even at a glance, the volunteers, students, preparators, curators, professors, and various other people who make this field run represent a growing array of genders, ages, national and cultural backgrounds, and more. To follow a maxim from my own profession, to be a writer, write. To be a paleontologist, paleo.\n\nBut this isn’t the picture that’s conveyed to the public. The image of Paleontologists in the public imagination continues to almost exclusively be enthusiastically-bearded white males in fedoras, waving their arms about on dusty outcrops about finds that will change everything we thought we knew. Take the focus a step back and the picture doesn’t improve much. The nature of modern science coverage has forged an image of paleontologists that is almost entirely academic – the professor or the curator, and to a lesser extent the postdoc and the graduate student. More than once I’ve been told that “You’re not a real paleontologist until you publish your first academic paper,” although even then the sense of gatekeeping about who is and who is not a paleontologist can shift according to entirely subjective categories of who’s doing the judging.\n\nI’ve had my role in this, too. I’ve built my career on talking about studies published in peer-reviewed journals, riding the flood of papers coming out of the field. As a body of work, that means I am primarily highlighting career academics and paleontology coming out of professional institutions with occasional reference to historical figures in the field. And that creates a false image. It’s a pinhole view into how paleontology is carried out, glossing over and even ignoring the thousands of people who have discovered fossils, prepared them, and otherwise contributed to the field.\n\nThe point struck me while listening to paleontologist Matt Brown speak at this month's Pop Palaeo workshop in Raleigh, North Carolina. His presentation focused on community outreach, finding ways to connect paleontology to a public who is largely unaware of the fossils that are sometimes literally in their own back yards. But what especially struck me was the opening context about how a fantastic collection of Cenozoic fossils – just described as the Lapara Creek Fauna by paleontologist Steven May – were excavated by people employed by the Works Progress Administration during the Great Depression. Without their efforts, the fossils might still be in the ground, or could have crumbled to shards.\n\nWPA crews excavated fossils at various places around the country as part of America’s New Deal startup, and, if I’m honest, I haven’t always written about them in the most flattering way. These amateur crews rarely had any training in paleontology and sometimes destroyed fossils in the process of excavating them. But emphasizing this point overshadows the fact that many of our early paleo heroes did the same, accidentally destroying specimens, not taking good field notes, selling or trading important specimens to other institutions, and head-hunting skeletons to leave the rest to crumble away. It ends up reinforcing academic authority and creates another division to shore up scientific superiority through misplaced emphasis.\n\nSo let’s look at this another way.\n\nThe excavation of the Lapara Creek fauna is a fitting example. Between March 1939 and September of 1941, May recounts, a WPA-run field crew called the State-Wide Paleontologic-Mineralogic Survey excavated fossils from Bee and Live Oak counties in southern Texas. At a high point, in June of 1939, 97 field workers assisted seven supervisors to run digs split into 23 field units. All of which to say, this was a major operation, with the training done in the field. In a photo of field workers from a spot called the Buckner Ranch Site 1, May notes “The majority of these workers had no geologic or paleontologic experience and were paid $0.20/hour through the WPA.”\n\nThese crews uncovered an entire menagerie of fossil beasts and other creatures that roamed Texas over 10 million years ago. They dug up alligators, turtles, rhinos, 12 species of horses, and, as May points out, a new species of fossil elephant now dubbed Blancotherium buckneri. About 50 species of fossil vertebrates have been identified from several sites uncovered during the project, from fish to prehistoric dogs. And their work didn’t stop there. A laboratory in Austin employed 50 preparators to clean and attempt to restore the fossils so that they would look more complete – a practice frowned on today, but common at the time. The field and lab crews were kept busy through 1941, when May notes that mineralogical goals became more important, but the WPA work created an enormous collection that otherwise would have remained in the ground. Academic paleontologists oversaw the projects, identified the fossils, curated them, and have described them, but the collection wouldn’t have existed without the amateur employees.\n\nPaleontology still relies on crews like this. Full-time curators, professors, and lab technicians are far outnumbered by students and volunteers who are vital to the field. (Later at the same Pop Palaeo meeting, a museum-employed paleontologist wryly noted that he and his professional colleagues had to carry jacketed fossils out themselves as they didn’t have any undergrads to do the grunt work on a prestigious trip.) Finding a fossil or spending hours in the prep lab as a volunteer might get a new species named in your honor, but these hard-working crews are largely invisible to the public. They often get less thanks than the dreaded, anonymous Reviewer 3 in the acknowledgements sections of scientific papers.\n\nPerhaps we’ve been totally focused on the wrong question. Who is and is not a paleontologist is often treated as a binary option whose accuracy depends on the authority of who's doing the asking. Coming down with a universal common denominator is likely to reinforce the image that only academics contribute to the field, further making broader communities invisible. Perhaps the question we should be asking is “How do you contribute to paleontology?” That paleontology is a process of understanding, and there are various ways to foster the field. The dinosaur fan who gives up their vacation week and $20 a day for camp meals is vital to the science just as the tenured professor, their roles being different but complementary. No matter how you define “paleontologist,” uncovering the past is a community effort.","title":"Who Is a Paleontologist, Really?","origin":"Laelaps","image":"https://static.scientificamerican.com/blogs/cache/file/02664920-B0B9-4C52-8F6376AA18F94918_source.jpg?w=590&h=800&FA676126-DE29-4D97-89EACB66C13E7CD8","link":"https://blogs.scientificamerican.com/laelaps/who-is-a-paleontologist-really/"},{"authors":"John Horgan","pub_date":"May 2, 2019","abstract":"Last month my school, Stevens Institute of Technology, hosted a “debate” called “Souls or Selfish Genes?” The Stevens Christian Fellowship, which organized the event (along with Veritas), billed it as “a discussion between two professors (a Christian and non-Christian) in search of truth about what makes us human.” I was the non-Christian and David Lahti, a biologist at City University of New York, the Christian. The moderator and most of the audience (according to a show of hands) were Christian too. Lahti and I had a hard time finding things on which to disagree. I nodded along when he objected to the “souls or selfish genes” dichotomy, arguing that faith and evolutionary theory are compatible. I didn’t oppose religious belief so much as I defended disbelief, toward scientific as well as religious explanations of who we are. Below are things I said, or wanted to say, at the event.\n\nFor as long as I can remember, the world has struck me as improbable, inexplicable, just plain weird. I have felt estranged from everything, including other people and myself. Psychiatrists call these feelings derealization and depersonalization. I yearned for a revelation that could dispel the weirdness and make me feel at home in my own skin.\n\nAs a boy I took comfort in my parents’ religion, Catholicism. Priests, nuns and my parents assured me that I am a child of God with an immortal soul. If I obey the Ten Commandments, confess my sins and go to church, I will ascend to heaven, where I will hang out with God, Jesus and the Holy Spirit (which a mural in my church depicted as a dove emanating laser beams).\n\nBy the time I was 11 or so Catholicism stopped making sense. Why, if God loves us, would He inflict hell on us, just for skipping mass now and then? That doctrine, which hard-eyed nuns taught in catechism, seemed awfully harsh. Also, I couldn’t imagine how heaven could fail to be boring.\n\nLike lots of young people in my generation (I graduated from high school in 1971), I began checking out more exotic religions. I became intrigued by enlightenment, the goal of Hinduism and Buddhism. I envisioned it as a state of supreme bliss and wisdom. It’s like heaven, except you don’t have to die to get there.\n\nSeeking enlightenment, I learned meditation and yoga and ingested psychedelics, and I read Doors of Perception by Aldous Huxley and Siddhartha by Hermann Hesse. Far from enlightening me, my forays into mysticism deepened my sense of weirdness.\n\nEventually I decided that science represents our best hope for understanding ourselves. In the mid-1980s, when I started writing for Scientific American, Stephen Hawking and other big-shots were proclaiming that science was on the verge of solving the riddle of existence and revealing “the mind of God,” as Hawking put it.\n\nThis possibility thrilled me, but eventually I concluded that science, for all its power, cannot give us a genuine theory of everything. Science is bumping into what may turn out to be absolute limits, and it will never tell us why there is something rather than nothing. So I argued in my book The End of Science.\n\nHoping to be proved wrong, I kept tracking efforts to answer big questions, and especially the biggest of all, the mind-body problem. Narrowly speaking, the mind-body problem focuses on how matter generates mind, including consciousness and free will, but in a broader sense it asks what we are, can be and should be. Prophets, philosophers and poets have peddled answers to this question for millennia. Only recently have scientists gotten in on the action.\n\nThe trouble is, scientists can’t agree on a solution to the mind-body problem, or even on an approach to a solution. Theorists I interviewed for my most recent book, Mind-Body Problems (which I dedicated to my students), advocate dizzyingly diverse mind-body models. We are nodes of information, clusters of Bayesian algorithms, egos trying to keep a lid on our ids, genes blindly striving to replicate, wave functions in an infinite quantum field.\n\nSome researchers defend their views by citing Buddha. That’s like physicists citing the ancient Greek hypothesis that the world is made of earth, water, air and fire. Prominent theorists are even challenging materialism, the assumption that matter is the foundation of reality. They argue that consciousness may be as fundamental as matter, or more fundamental.\n\nSo where does this leave me, in terms of my search for answers? I’ve given up hope that science can give us a single, objectively true solution to the mind-body problem, one true for everyone. Disbelief, I’ve decided, is the only rational stance to take toward alleged solutions, whether religious or scientific.\n\nI no longer crave a revelation that will dispel my sense of weirdness, because I’ve accepted that we really are weird. The weirdness isn’t just a function of our ignorance, it is intrinsic to reality. \n\nAs much as I love some mind-body ideas (like Douglas Hofstadter’s self-generating strange loops), I don’t really believe any of them, not like I believe in the atomic theory of matter or the genetic code. I think of mind-body theories as stories, works of imagination, of art. Some are more compelling than others—more meaningful and comforting--but none really solves the mind-body problem, any more than The Inferno or War and Peace do.\n\nThose who yearn for certainty about who we really are might find disbelief unsatisfying, even frightening. You have no ground on which to stand, no assurance that God or science will take care of us, that everything is going to be okay.\n\nBut if history teaches us anything, it is that our craving for certainty can get us into trouble. It has led to genocide, slavery, crusades, inquisitions, wars. This is true not only of religious mind-body solutions but also of supposedly scientific ones, like Marxism and social Darwinism. We are never more dangerous than when we know what we really are, can be and should be, and we insist that others share our belief.\n\nDisbelief can protect us from our desperation for answers. And in exchange for certainty, you get the exhilaration of confronting the unknown with no preconceptions. You get the freedom to be whatever you imagine yourself to be, to create your own identity and destiny. You can see yourself as a pack of selfish genes, bundle of algorithms, immortal soul or all the above. You just can’t insist that your answer to the mind-body problem is The Answer.\n\nMy main advice to people of faith, whether Christians or hard-core scientific materialists, is to doubt yourself. Be open-minded. Consider the possibility, even probability, that your beliefs are a matter of taste, not truth. And remember that if we cannot solve the mystery of ourselves, we can keep exploring ourselves forever.\n\nFurther Reading:\n\nMind-Body Problems (free online edition, Kindle e-book and paperback)\n\nMeta-Post: Posts on the Mind-Body Problem","title":"In Defense of Disbelief: An Anti-Creed","origin":"Cross-Check","image":"https://static.scientificamerican.com/blogs/cache/file/ADD703D6-06CF-41F8-B615D0EE56FB22D3_source.jpg?w=590&h=800&EC1341FC-8609-4AF9-A25305FB354A2E4A","link":"https://blogs.scientificamerican.com/cross-check/in-defense-of-disbelief-an-anti-creed/"},{"authors":"Abraham Loeb","pub_date":"May 6, 2019","abstract":"Last month I was invited to a radio interview about a recent meteor over the Bering Sea which was spotted off Russia’s Kamchatka’s peninsula on December 18, 2018 after producing a blast with 10 times the energy of the atomic bomb over Hiroshima. In preparation for the interview I searched the literature online and came across a catalog of all meteors over the past three decades, ordered by the strength of the fireball they produced.\n\nThese objects were discovered by a classified system of detectors owned by the United States government, which used the sound waves and light they produced in the Earth’s atmosphere to determine the three-dimensional components of their velocity and position at the time of impact.\n\nImpressed by the most powerful fireballs, I asked an undergraduate student, Amir Siraj, who has been working with me on ‘Oumuamua—the first interstellar object discovered in the solar system—to calculate the past trajectory of the fastest meteors in the catalog, starting from their detected position and velocity at impact, and taking account of the gravity of the Earth, the sun and all distant planets within the solar system.\n\nThe trajectory of the fastest object ended up being bound to the sun since it involved a head-on collision with the moving Earth. The second fastest was definitely unbound to the sun. The third fastest was possibly bound within uncertainties. The surprising result about the second fastest meteor in the catalog was conveyed through a surprising e-mail from Amir saying: “We might have discovered the first meteor which originated outside the solar system!” A few days later, we posted our paper on the arXiv and simultaneously submitted it for publication in Astrophysical Journal Letters.\n\nThere is an important moral to this story. Media appearances are not a waste of time. They can raise questions that inspire scientific breakthroughs.\n\nIn retrospect, meteoric fireballs offer an ideal opportunity for learning about interstellar objects. The traditional search method uses the sun as a lamppost and looks for objects based on the light they reflect. This is how ‘Oumuamua was detected by the Pan-STARRS telescope. The limitation of this survey technique is that it is limited to objects larger than a hundred meters, since smaller objects are too faint to be detected by Pan-STARRS.\n\nNaturally, one would expect smaller objects to be more abundant—so much so, that some of these might hit the Earth at a noticeable rate despite its small cross-sectional area. And so, the Earth's atmosphere ends up serving as a detector for meter-size interstellar objects. And since meteors burn up in the atmosphere, spectroscopy of the gases they emit can be used to infer their composition even if they leave no relic behind.\n\nThe second fastest interstellar meteor in the catalog was spotted just north of Manus Island, off the coast of Papua New Guinea. It produced a blast of merely a percent of the Hiroshima bomb, implying a meter-size object with a mass of about 500 kilograms. Given the inferred impact rate of roughly once per decade, we concluded that there should be roughly a million such objects inside the orbit of the Earth around the sun. These meter-size objects carry as much mass per volume of space as ‘Oumuamua-like objects that are a hundred times bigger.\n\nThe reported meteor entered the solar system with a speed of 60 kilometers per second relative to the local standard of rest, the frame of reference obtained by averaging the motion of all stars in the vicinity of the sun. Such a high speed can be produced through ejection from the innermost region of a planetary system, where the characteristic orbital speeds are of comparable magnitude. In the solar system, the relevant region is interior to the orbit of Mercury.\n\nBut for dwarf stars like our nearest neighbor, Proxima Centauri, the required ejection region overlaps with the habitable zone, which is 20 times closer in since the star is much fainter than the sun. Since dwarf stars are most common, the detection of this meteor offers new prospects for “interstellar panspermia,” or the transfer of life between planets that reside in the habitable zones of different stars. This might be possible as long as the meteor is big enough so that it does not burn up completely in the Earth’s atmosphere. To seed life, the required impact rate is merely once per billion years, allowing for a much bigger object that could survive entry to the target planet.\n\nTo make the inferred population of meter-size interstellar meteors, each star needs to eject about 1022 objects, totaling about an Earth-mass worth of material.  This requirement is at tension with the expected mass in planetesimals within the orbit of Mercury in the early solar system.\n\nUsing the Earth’s atmosphere as a detector for interstellar objects offers new prospects for inferring the composition of the gases they leave behind as they burn up in the atmosphere. In the future, astronomers may establish an alert system that triggers follow-up spectroscopic observations to an impact by a meteor of possible interstellar origin. Alert systems already exist for gravitational wave sources, gamma-ray bursts or fast radio bursts at the edge of the universe.\n\nEven though interstellar meteors reflect the very local universe, they constitute a “message in a bottle” with fascinating new information about nurseries that may be very different from the solar system. Some of them might even represent defunct technological equipment from alien civilizations, which drifted towards Earth by chance, just like a plastic bottle swept ashore on the background of natural seashells.","title":"'Oumuamua's Cousin?","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/973F84E7-55C5-48A6-8F79D5E19353D35B_source.jpg?w=590&h=800&CE177167-E6B4-4AC4-ACDE7C4EA29D429F","link":"https://blogs.scientificamerican.com/observations/oumuamuas-cousin/"},{"authors":"Savvy Psychologist Ellen Hendriksen","pub_date":"May 4, 2019","abstract":"Like a tomato, which could arguably be a fruit or a vegetable, burnout can arguably be a diagnosable disorder or not. While it’s not recognized as a disorder in the U.S., it is in Sweden and it makes an appearance in the International Classification of Diseases (ICD-10) as a “state of vital exhaustion.” \n\nRegardless of whether you call it burnout or vital exhaustion, it’s a state known to many of us, and it ravages the body, contributing to everything from hypertension to substance abuse. Therefore, while we’ve talked about burnout on the podcast before, namely here and here, it demands another look.\n\nTo review, burnout has three hallmark symptoms. First, there is emotional exhaustion, which also bleeds over into physical exhaustion. With this symptom, dragging yourself to work takes heroic effort and being productive is out of the question. \n\nNext is reduced personal accomplishment, which is exactly what it sounds like. It takes more effort to get less done, and you wonder what the point is anyway. Even successes feel like the equivalent of a dead-eyed, slack-jawed sarcastic confetti toss. \n\nThe last symptom, depersonalization, is being cynical, critical, and resentful with co-workers and clients. If you frequently mutter, “What is with these people?”, “Idiots!”, or any number of NSFW labels, you may be on your way to depersonalization.\n\nAll of this may sound eerily similar to depression, but burnout is distinct in that it’s constrained to the domain of work. Folks who are depressed will still be depressed sitting on a tropical beach, but those with burnout often feel better once they’ve taken time off and are surrounded not by demanding customers and autocratic supervisors, but by palm trees, a stack of novels, or woodworking tools—whatever floats your boat. In other words, in depression, the little black raincloud follows you everywhere, but in burnout, it stays squarely over your work station. \n\nAnd while it’s normal to have ambivalent feelings about work, look at job listings over your lunch break, or fantasize about taking a baseball bat to the unruly printer (“PC load letter?!”), you know you’ve crossed a line if burnout symptoms interfere with your best efforts to function.\n\nSo what causes burnout? Some of the contributors are intuitive: a never-ending avalanche of tasks, a toxic work environment, or all work and no life. It makes sense that you’d feel drained by a boss who tells you to work through pain, a coach that sprays angry spittle in your face, or a colleague who has loud phone conversations about her sex life while you pick up the slack.\n\nBut the other factors aren’t so clear. This week, we’ll walk through 5 surprising causes of burnout. \n\n\n\n»Continue reading “5 Surprising Causes of Burnout” on QuickAndDirtyTips.com","title":"5 Surprising Causes of Burnout","origin":"Mental Health","image":"https://static.scientificamerican.com/sciam/cache/file/F5764F25-34B6-48A4-8B9216ADCB1C3783_source.jpg?w=590&h=800&E2E2762B-999F-45A7-9BF7B7FFE5C6C93D","link":"https://www.scientificamerican.com/article/5-surprising-causes-of-burnout/"},{"authors":"Evelyn Lamb","pub_date":"May 1, 2019","abstract":"My favorite genre of tumblr post is “humans are adorable.” We like to decorate ourselves with shiny things! We burn things for fun even though fire is inherently very scary! We bond with nonhumans and even feel empathy for spaceships and robot vacuum cleaners! We are ridiculous creatures, but at least we're cute. I think the way we do math is no different. Here are five of the most adorable things about our relationship with numbers and other aspects of math.\n\n1. Between 2003 and 2013, U.S. universities awarded almost 15,000 doctorates in mathematics, statistics, and biostatistics, so there are tens of thousands of mathematicians running around the country. But not a single one of them knows whether π+e is rational or irrational! I mean, odds are in favor of it being irrational because almost all number are, but they don’t know for sure. And it’s not just π+e. Combine some irrational (or even transcendental) numbers in pretty much any way you want, and chances are no one can figure out whether it’s rational or irrational! They don’t know whether π×e is rational either, but they do know π+e and π×e cannot both be rational. What a silly amount of knowledge to have!\n\n2. Even though most numbers—in a sense, 100% of them—are irrational, humans are strangely obsessed with rational numbers. The fewer and further between the numbers are, the more they care about them. They love integers more than non-integer rational numbers, and they seem to love prime numbers the most of all, even though a smaller and smaller proportion of integers are prime the further out on the number line you look! They are especially fascinated with twin primes, which are rarer still and may even peter out entirely at some point.\n\n3. Humans define π to be the ratio of a circle’s circumference to its diameter and then say with a straight face that it is irrational! Even though the word irrational is based on the word ratio!\n\n4. Despite the fact that numbers are largely constructed in their own minds, humans have favorite numbers and numbers they are afraid of or believe to be unlucky. Seven is the most favorite number, according to a poll conducted by Alex Bellos, even though that is the wrong answer because six is clearly the best number. Many people are afraid of four or 13. Some hotels and other tall buildings avoid labeling a floor as the 13th floor and then pretend there’s nothing wrong with the 14th floor! Humans will use their favorite numbers when they play games of chance even though they confer no extra benefit and in some sense could even lower their payout on a win.\n\n5. When they’re working on a problem, humans sometimes stop when they learn that a solution exists and not bother to find the actual solution! Sometimes they don’t even know how to find the solution—they just know that it exists—and they still give themselves credit for solving the problem. Only a few mathematicians, known as constructivists, insist on being able to provide examples of quantities or objects asserted to exist, and they are considered a bit odd.\n\nDo you have any favorite cute human math quirks? Share them with me on Twitter or Facebook.","title":"I Can Has Numberz?","origin":"Roots of Unity","image":"https://static.scientificamerican.com/blogs/cache/file/00FCE5E7-6F11-4BDF-BB38942B132A836C_source.jpg?w=590&h=800&661E147B-3CB2-4C7B-BAF9AD63E454B460","link":"https://blogs.scientificamerican.com/roots-of-unity/i-can-has-numberz/"},{"authors":"Oliver Whang","pub_date":"May 3, 2019","abstract":"Of the many ways to die in the modern world, few seem as perverse as this: you walk into the hospital with a minor illness and leave in a casket. \n\nYet such a fate is becoming an increasingly prevalent reality, even as medical technologies advance. This is mainly due to drug-resistant infections—infections caused by strains of bacteria and fungi that have evolved to be immune to the majority of modern medicines. More and more of these organisms have developed over the past decades as a result of a systemic overuse and misuse of antibiotics and antifungals. They cause infections hospitals don’t have the know-how or resources to deal with, and can spread rapidly between patients within health care facilities, and between health care facilities themselves, sickening and killing many.\n\nDrug-resistant infections are part of a larger problem the American health care system is facing: that of health care–associated infections (HAIs). Every year around 72,000 people die from infections contracted in American hospitals and health care facilities. An additional 615,000 people treated in these institutions—3 percent of all hospitalized patients—find themselves sicker than if they had not sought out help at all, according to the Centers for Disease Control and Prevention (CDC). \n\nThis issue is more than just a matter of life and death; it is also one of dollars and cents. The total annual cost of HAIs has been estimated to run up to 45 billion dollars—more than the GDP of most small countries, and a couple of large ones as well. This cost is shouldered by health care facilities and hospitals, bleeding out onto the government, and taxpayers, via programs like Medicaid and Medicare. \n\nThe $45 billion cost analysis doesn’t even include indirect costs, like a hesitancy among prospective patients to take part in the health care system. Why would you go to a hospital if you thought you would only come out sicker? And why would you want to pay for services at that hospital? \n\nThe good news is that most health care–associated infections are preventable. Simple things like proper hand sanitization, waste management and careful sterilization at skin injection sites have cut the rates of HAIs—and the costs they incur—by more than half over the past decade. All the same, these steps have not been enough. The problem remains, looming, and threatens to get worse in the future. \n\nAs the American population ages one of the greatest dangers hospitals face is the potential of HAIs—and specifically drug-resistant infections—to spread with patients as they move from facility to facility. Current cost calculations don’t take this movement into account, but as health care facilities bend to accommodate a growing number of high risk patients, transfers between hospitals will become more frequent, and the probability of a new deadly infection being introduced will increase. \n\nTo illustrate this problem, in a 2014 TED Talk, Ramanan Laxminarayan—senior research scholar and lecturer at Princeton University—said: “We don't consider—and we, including individuals, patients, hospitals, entire health systems—do not consider the costs [imposed] on others by the way antibiotics are actually used.” \n\nEach time an antibiotic is misused and a new strain of bacteria evolves with an immunity to that antibiotic, scientists must develop a different antibiotic to treat the new infection caused by the new bacteria. Then they must distribute the newly developed antibiotic to hospitals, by which time patients could have gotten sick and died. It’s a race to evolve, essentially. A race against nature. And we’re going to lose. \n\n“This is clearly not a game that can be sustained, or one that we can win by simply innovating to stay ahead,” said Laxminarayan. “We've got to slow the pace of coevolution down.” \n\nRecognizing this, the government has recently focused more resources towards the prevention of health care–associated infections. In 2013 the U.S. Department of Health and Human Services (HHS) implemented a “national action plan” to educate health care providers about prevention measures. A large part of this plan, which was added in 2018, includes an “antimicrobial stewardship” program to purvey information on how to most safely use antibiotics. \n\nThe government has also turned towards economic incentives as a possible way to decrease rates of HAIs. Because the emergence of a hospital-born infection in one hospital is a problem for all other hospitals, punishing health care facilities with the highest number of infected patients could have a positive impact on the entire American healthcare system. \n\nThe Centers for Medicare and Medicaid Services (CMS) started a program in 2014 under the Affordable Care Act to do just that—take money away from hospitals with the highest rates of HAIs. The program reduces Medicare reimbursement for these hospitals by 1 percent. This may not sound like a lot, but it adds up. Total Medicare spending on patients in hospitals was $142 billion in 2016. \n\nThe CMS’s program has been criticized for drawing an arbitrary line between which hospitals get punished and which don’t. The bottom 25 percent of hospitals—determined by a holistic metric—get their Medicare payments reduced, while those above that line are fully reimbursed by the program. \n\nSome questions are then raised: How should we incentivize hospitals to increase infection prevention measures? Whose behavior should be punished, and whose behavior should be rewarded (if at all)? Does the CMS have it right? \n\nA recent paper co-authored by Sarah Drohan, Simon Levin (James S. McDonnell Distinguished University Professor in Ecology and Evolutionary Biology at Princeton University) and Laxminarayan, published in the Proceedings of the National Academy of Sciences (PNAS), found that although subsidies may be able to reduce cases of HAIs, the current government approach does not do so in a productive way. Government subsidies work, but not as they are implemented now. \n\nThe paper compares, in a mathematical economic framework, the effect different types of subsidies have on the number of infections acquired in hospitals. The current CMS policy—to essentially tax hospitals with high numbers of infected patients—was shown to be only a fraction as effective as a subsidy that gives an additional dollar to hospitals for each dollar spent on infection control. \n\n“The fundamental reasoning for that is somewhat simple,” says Drohan. “It’s just that [a dollar for dollar subsidy] is a more effective way of tying the amount hospitals receive from the policymaker to their own spending. It's the most effective way to encourage them to spend more, because the more money they spend, the more money they get.” \n\nInstead of punishing hospitals that have a high rate of HAIs, Drohan, Levin and Laxminarayan advocate for the opposite: rewarding hospitals with the fewest HAIs. It’s a simple perspective shift—from a negative incentive to a positive one—but it could make all the difference. \n\nIn fact, Drohan and Ramanan go even further in the paper, suggesting that government subsidies should be, paradoxically, given almost entirely to hospitals with the lowest rates of HAIs. “What we found was that the subsidy, or the majority of the policymaker money—in fact all of it for the best possible outcome—should be given to the institution with the lowest transmission rate, which means the least infectious institution,” Drohan says. \n\nIn other words, all money should be given to the hospitals that least need it. Hospitals with the lowest numbers of infected patients. \n\nThis seems, to put it bluntly, unfair. It stands to reason that hospitals with the fewest infected patients would need the least help from the government. What Drohan, Levin, and Laxminarayan found, though, was that when they factored in the movement of patients between hospitals they were no longer trying to minimize the number of infected patients in each hospital but minimize the number of infected patients as a whole. It became a different kind of economics problem—a question not about individual patients, but of the common good.\n\n“From a simple economics point of view, the marginal return on your subsidy dollar is a lot greater when you give it to the hospital with the lower transmission rate,” Drohan says. Each dollar given to the hospitals with the fewest infected patients, because of the way patients move between facilities and the economic forces behind the health care system, goes further in reducing the overall rate of HAIs. Looking at the entire health care system, that is the best way to use the government’s money. \n\nAnd although giving more support to those facilities with the fewest people in need is totally counterintuitive, perhaps it is just something we need to try to wrap our heads around. We as a society are used to looking at the individual—the individual hospital, the individual family, the individual patient—but sometimes we need to look at the issue as a whole. \n\n\"When I was writing other drafts of this, I kept wanting to say it's the opposite of what we think our moral argument should be,” Drohan says. “But in a way I've sort of convinced myself that you have to sort of step above that, and think: But what's actually best in the long term?”","title":"How to Save $45 Billion on Health Care Costs Each Year","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/A0D7E0C4-A6B8-46DD-9CA55AF79CFCF094_source.jpg?w=590&h=800&DE403320-291B-4581-90A82127CD569B0C","link":"https://blogs.scientificamerican.com/observations/how-to-save-45-billion-on-health-care-costs-each-year/"},{"authors":"Thomas Frank, E&E News","pub_date":"May 6, 2019","abstract":"When Erica Kuligowski went to Puerto Rico three months after Hurricane Maria battered the island, she noticed conditions that others seemed to overlook.\nVital buildings such as hospitals and schools were still standing but unusable because rain had penetrated and swamped the interiors.\nThe water damage might have seemed unremarkable in a moonscape of destruction and power outages. But Kuligowski works for a federal agency that studies buildings.\nNow, the Commerce Department’s National Institute of Standards and Technology is undertaking a rare, comprehensive investigation into why Hurricane Maria ruined so many essential buildings in September 2017 and why systems such as emergency communications failed.\nThe broad inquiry is only the fourth that NIST has undertaken since Congress expanded the agency’s investigative powers after the 9/11 terrorist attacks. Its first investigation was into the collapse of New York City’s World Trade Center on Sept. 11, 2001.\nThe latest probe will answer some of the most crucial questions about why Puerto Rico suffered so much—and will guide the nation on how to avoid similar devastation. Investigators also will probe the death toll, which remains uncertain and disputed.\n“Our goal really is that we want to be able to identify findings that will lead to improvements,” said Kuligowski, who is leading the NIST investigative team.\nThe 17-member Hurricane Maria Team aims to revise international codes to incorporate new standards that make critical structures more resilient against the type of 140-mph winds that swept across Puerto Rico during Maria. Nonprofits such as the International Code Council and National Fire Protection Association publish and update model building and safety codes that jurisdictions often adopt as their own.\n“The whole goal here is to save lives and prevent damage in future disasters,” Kuligowski said. Her group, though, is not looking at the many homes that were extensively damaged by Hurricane Maria and were constructed without building permits.\nThe NIST investigation comes as private groups and government agencies increasingly focus on resilience—for buildings, infrastructure and services—to cope with climate change and the growing intensity of weather-related disasters such as hurricanes. In January, the National Institute of Building Sciences, a congressionally mandated research group, released astudyshowing that states and municipalities can save extensive money, property and lives by adopting the latest model building codes.\nLast Thursday, two former Federal Emergency Management Agency administrators wrote acolumninThe Hillsaying that Congress should require that any construction done through a massive new infrastructure package should follow modern codes.\nThe failures of buildings and emergency communications on Puerto Rico have been overlooked as other investigations have focused on the extensive power loss and FEMA’s struggle to assist the U.S. territory’s 3.2 million residents.\nIn a preliminaryreport, NIST’s Hurricane Maria Team found that buildings such as hospitals and schools had “good structural performance” but “suffered extensive nonstructural damage and loss of function.\" The buildings were deluged with rain because roofs, doors and windows were destroyed or damaged, while wind-driven rain penetrated undamaged doors and windows, the report found.\n“If a critical building loses its ability to function due to rainwater penetration, you’re not able to provide services” such as health care and education, Kuligowski said.\nThe preliminary report also found extended breakdowns in emergency communication with the public as conditions deteriorated.\n“The public wasn’t receiving information because a lot of the [communications] channels were lost due to power and other issues. Just a few radio stations were operating,” Kuligowski said. “When people needed to understand what’s going on, where recovery supplies are located—a lot of that was difficult to communicate.”\nIn addition to investigating failures of public buildings and emergency communications, the NIST team will examine hurricane-related deaths to determine how many were caused by windstorms and by failures of buildings and infrastructure. Death counts have varied wildly, ranging from an initial count of 64 to astudyone year ago by the Harvard T.H. Chan School of Public Health projecting as many as 8,498 deaths.\nThe team also will study hindrances to the resumption of businesses, supply chains and social functions. The investigation began in February 2018 and will take several years as the team interviews people in Puerto Rico and works with groups that write building and safety codes.\n‘Fact-finding’\nThe Maria investigation will be a major test for NIST, a small agency whose mission is to strengthen U.S. industry by setting standards for an endless array of products and services, from computer chips to power grids. A third of the agency’s roughly 3,000 employees have doctoral degrees.\nNIST has done dozens of post-disaster studies going back as far as a 1973 apartment building collapse in Virginia that killed 14 construction workers. But after the World Trade Center towers collapsed, Congress determined there were “serious flaws in how the federal government carries out investigations of major building failures” because no agency could conduct a “comprehensive and thorough investigation” immediately after 9/11.\nA law enacted in October 2002 authorizes NIST to undertake such investigations, following a model of the National Transportation Safety Board, which conducts exhaustive investigations of transportation incidents such as airplane crashes and recommends safety improvements. Thelawempowers NIST to investigate all aspects of a building failure, including meteorological conditions and emergency response, and gives the agency authority to subpoena witnesses and records.\n“Our technical report will be objective, scientific-based fact-finding—not fault-finding—that lays out recommendations to improve building codes, standards and practices,” Kuligowski said.\nNIST’s investigation of the World Trade Center yielded 30 recommendations, such as developing standards that prevent “progressive collapse” of a building and that ensure massive fires do not cause buildings to fall down.\nNIST’s other two comprehensive investigations were of a fire at The Station nightclub in Rhode Island in 2003 that killed 100 people and of a tornado in Joplin, Mo., in 2011 that killed 158.\nNIST decided to investigate Hurricane Maria in Puerto Rico after Kuligowski and three colleagues went to the island in December 2017 and saw building damage from the 140-mph winds as well as from flooding, storm surge and landslides.\n“We saw from one event so many conditions that were providing opportunities for NIST to study and recommend ways to improve the resilience of communities to windstorm events across the United States,” Kuligowski said.\nReprinted from Climatewire with permission from E&E News. E&E provides daily coverage of essential energy and environmental news atwww.eenews.net.","title":"Hurricane Maria Prompts Rare Investigation into Building Damage","origin":"Policy & Ethics","image":"https://static.scientificamerican.com/sciam/cache/file/46D18BF9-C515-42BF-895FA5E64974195B_source.jpg?w=590&h=800&0C709EC0-FB1E-4ADC-9749968B5FF9BE78","link":"https://www.scientificamerican.com/article/hurricane-maria-prompts-rare-investigation-into-building-damage/"},{"authors":"Daniel Cusick, E&E News","pub_date":"May 3, 2019","abstract":"Davenport, Iowa, was prepared for a big flood. Then it got the flood it hadn’t prepared for.\n\nThe city of 102,000 on the Iowa-Illinois border this week watched a fast-rising Mississippi River sweep over its postcard-view waterfront and inundate parks, pavilions and even a minor league baseball stadium that were all designed to absorb floodwater.\n\nBut no one anticipated the water would get past River Drive, the four-lane thoroughfare that marks the boundary between the Mississippi floodplain and downtown.\n\nThe river crossed the road Tuesday afternoon, breaching a temporary flood barrier and inundating several blocks of businesses and buildings on the city’s east side.\n\nAmong the damaged structures was the 95-year-old Union Station, where water lapped against the building’s foundation and then penetrated the interior, prompting utility workers to cut off electric and gas service to reduce the risk of fire.\n\nDavenport is Iowa’s fourth-largest city. It’s one of the most populated places along the Upper Mississippi not to have a flood wall or other permanent barrier separating higher ground from the river.\n\nDavenport’s leaders made that choice years ago when neighboring river cities were spending millions of dollars lining their banks with concrete and riprap.\n\nDavenport opted for a different flood control approach, one that allowed the river to take back the historic floodplain during high-water events. As with prior floods, the city’s near-escape from disaster is leading to second-guessing.\n\nMayor Frank Klipsch on Wednesday defended the city’s leaders, past and present.\n\n“This system works very well,” he said at a press conference. “The city of Davenport isn’t underwater. It’s a portion of it, and that’s very important to us.”\n\nExperts say the cresting Mississippi, which is expected to swallow dozens more river towns below Davenport over the coming days, will probably rank among the top five floods in history. In some areas, it could challenge the Great Flood of 1993, the most destructive on record.\n\nThe Midwest has already experienced a record-wet winter and early spring, and some communities in the Mississippi and Missouri basins have placed more sandbags in the first four months of 2019 than during the entire previous decade.\n\n“Many of these cities used up every inch of flood storage they had to make it through February, March and April,” said Colin Wellenkamp, executive director of the Mississippi River Cities and Towns Initiative based in St. Louis. “So it really doesn’t matter what kind of flood control system you have. At this point, everybody is going to get high water.”\n\nAnd it isn’t likely to get better.\n\nScientists have warned that one consequence of climate change for the Midwest will be more extreme precipitation, sometimes as snow, sleet and ice—as this year’s two winter bomb cyclones demonstrated—or in storm bursts like those that have beset the region over the last week.\n\nSince Sunday, the Chicago area has seen more than 5 inches of rain, sending already swollen creeks and rivers over their banks and into streets and neighborhoods. The Illinois Department of Transportation reported road closures yesterday across the city’s south, west and northwest suburbs, according to the Chicago Tribune.\n\n‘Don’t bode well’\n\nChicago’s urban floods will add insult to injury for the tiny city of Grafton, Ill., which sits just below the confluence of the Mississippi and Illinois rivers.\n\nGrafton Mayor Rick Eberlin described quickly deteriorating conditions in the city of 640 as local heavy rainfall meets rising water coming down the two upstream basins. By early next week, the Mississippi is expected to crest at 33 feet at Grafton, or 16 feet over flood stage. That would make it the third-largest flood on record, after 1993 and 1973.\n\n“We’ve just got a combination of events that don’t bode well for our city,” Eberlin said in a phone interview, adding he will not require evacuations “for the simple reason that everybody in town knows what’s going on.”\n\nHe will probably have to close Route 100, the Great River Road, on both sides of town since the highway is submerged at 29 feet above flood stage.\n\n“That’s a tough decision because we are a tourist town and every day is important to our business community,” Eberlin said.\n\nScott Ross, a spokesman for the Army Corps of Engineers’ St. Louis District, said officials are closely monitoring conditions in Grafton and that flood-fighting teams have been dispatched along the river from Clarksville to West Alton, Mo.\n\nNo federal levees had been breached as of yesterday, “but we’re keeping a close eye on all of them,” Ross said.\n\nMany of the most at-risk levees are nonfederal structures protecting agricultural areas.\n\n“We expect the whole district, from our part of the Mississippi all the way to Cairo, to reach major flood stage over the course of the next few days,” Ross added.\n\nReprinted from Climatewire with permission from E&E News. E&E provides daily coverage of essential energy and environmental news at www.eenews.net.","title":"Rising Mississippi River Tests a City’s Adaptation Plan","origin":"Natural Disasters","image":"https://static.scientificamerican.com/sciam/cache/file/CDED6FA0-AD4E-45D4-9C56DD367BEC10D6_source.jpg?w=590&h=800&A67AAD99-CCC8-410B-A9914DB532775211","link":"https://www.scientificamerican.com/article/rising-mississippi-river-tests-a-citys-adaptation-plan/"},{"authors":"Dana Hunter","pub_date":"April 30, 2019","abstract":"Humans have been captivated by diamonds, the planet’s hardest natural material, for nearly five thousand years. In Egypt, they were incorporated into the ankh, symbolizing the sun. People in ancient India thought they must be created by lightning, and attracted it; they also expected the stones could stave off danger (a belief somewhat at odds with the lightning-attractor thing). Greeks and Romans found the stones much more sacred, seeing them as tears of the gods. Rather poetically, they also thought the gems might be shards of fallen stars. At various times, people have thought diamonds to be capable of conferring invincibility, of healing, and of sealing the deal on romantic love.\n\nThe truth of diamonds is almost as extraordinary as all of those beliefs. They are formed of the stuff of life itself: carbon. Extraordinarily hard, they can withstand enough pressure to recreate the extreme conditions under which they were born; and yet, subjected to the right combination of heat and oxygen, they’ll vanish in a puff of carbon dioxide. They form naturally in only a few places on Earth: deep beneath continental cratons, or in the shock of a meteorite strike. Most of them are billions of years old, and we’re not even sure if they’re still being formed beneath the crust today. And they’re brought to the Earth’s surface by some of the most bizarre eruptions in our planet’s history.\n\nNo ordinary volcano would do. Most of them are too shallowly-rooted to get at the deep places, roughly 150 kilometers beneath the crust, under the keel of continents, where diamonds form. And even if they do, their magma is too hot, too full of oxygen, too slow to bring a diamond successfully to the surface. You need a magma that’s ultramafic, far more basic than basalt. It must be somewhat cool (for deeply-buried, iron and magnesium-rich magma), no more than around 1,300°C. It has to contain a high percentage of volatiles like water or carbon dioxide, in order to propel the stuff to the surface. And it needs to move like no eruptions humans have ever witnessed, in order to sweep diamonds up from the great depths where they originate up to the surface.\n\nThere are only three types of magma on the planet that can manage it, at least that we’ve identified so far: kimberlite, lamproite, and lamprophyre. All of them are ultrabasic. And most of them erupt in very odd ways. Kimberlite and lamproite especially like to power through the old, cold crust in and near cratons, expending vast energy in dashing to the surface and then off-loading their cargo in short-lived and small but very vigorous eruptions. Erik Klemetti has done a fabulous job describing how kimberlite volcanoes erupt. I couldn’t find a similar resource for lamproites, so I’m reading up on them and will have a post on them shortly.\n\nMost of the volcanoes that contain diamond-bearing rocks are old, but the diamonds that hitched a ride with them are far older. The vast majority are between one and three billion years old. So if someone gives you a natural diamond, as you admire its sparkle, marvel at the vast swath of time this little stone has witnessed.\n\nThey may not be tears of the gods (although given the right conditions, they will evaporate like tears) or shards of stars (although some stars may be made of them), but they are remarkable little stones nonetheless. They have a rich and complex history, plus eventful travels, that more than repay a day’s reading. Please check out the links below.\n\nAnd for those who aren’t wealthy enough to afford a diamond of their own: keep in mind that cubic zirconia is just as sparkly and will be happy to stand in.\n\nReferences and Further Reading:\n\nGeology of Diamonds\n\nDiamond! (This is a marvelous article that covers all the bases)\n\nRecent Advances in Understanding the Geology of Diamonds (More technical, but still marvelous)\n\nThe Crazy Eruptions That Spit Up Diamonds (Best post on kimberlite eruptions ever)\n\nMythology of Diamonds\n\nLord Shiva and the Hope Diamond\n\nDiamond Myths and Legends\n\nDiamond History and Lore\n\nHuman History of Diamonds\n\nThe Lustrous Legacy of Diamond Manufacturing","title":"Diamonds: Tears of the Gods","origin":"Rosetta Stones","image":"https://static.scientificamerican.com/blogs/cache/file/5FAA4754-92EA-41CA-8D3965064E4A9E46_source.jpg?w=590&h=800&A9A22E52-BC1E-472E-BC5858D9003121AE","link":"https://blogs.scientificamerican.com/rosetta-stones/diamonds-tears-of-the-gods/"},{"authors":"Joshua Rapp Learn, Sapiens","pub_date":"May 3, 2019","abstract":"During the last centuries of China’s Shang dynasty, which lasted from 1600 B.C. to 1050 B.C., ritual sacrifice was a well-oiled cultural phenomenon, rich and varied in its manifestations. Rulers and elites sacrificed animals and humans to appease spirits or the ancestors. Just as humans met their ends, dogs were often right beside them.\n\nNow a study in Archaeological Research in Asia, published in March, shows that people from the Shang dynasty relied heavily on sacrificial puppies to accompany them in death. “Although superficially it seems like a horrific thing to kill a puppy and put it into a tomb, it’s actually a window into the complex world of Shang human-animal relations,” says Roderick Campbell, an archaeologist at New York University and one of the co-authors of the study.\n\nResearchers have long known that people in the Shang dynasty sacrificed and buried canines alongside the elite. The assumption has been that these dogs were pets, ritually sacrificed after their owners’ deaths so that the canines would spiritually accompany them into the afterlife. But Campbell and his co-author Zhipeng Li, an associate professor at the Institute of Archaeology at the Chinese Academy of Social Sciences in Beijing, say that this explanation doesn’t fit their findings.\n\nThe team examined Chinese archaeological site reports and about 2,000 Shang-era graves at a site called Xiaomintun under the modern city of Anyang, China. The researchers discovered that the buried canines were predominantly juveniles. About a third of the graves contained a dead dog in a small pit dug under the coffin.\n\nThe puppies’ remains didn’t show any clear sign of death; someone likely drowned or suffocated the animal, or slit its throat (soft tissue damage doesn’t leave many long-term traces). Furthermore, many of the people buried appeared to be middle class rather than elite based on the quality and quantity of goods they were buried with. Campbell says that the sacrificial puppies could have come from the Shang-era equivalent of puppy mills or from litters around the city, whether strays or pets.\n\nhe reasons for the use of puppies as opposed to adult dogs aren’t yet clear, but Campbell places it within a larger tradition of miniaturization in grave offerings. Items such as ceramics left with the dead became progressively smaller on average throughout the Shang dynasty until they were little more than doll-sized cups. The change made offerings more affordable. Seen in this light, puppies may have been the cheaper alternative to dogs.\n\n“If you let dogs grow up, you have to take care of them,” says Angela Perri, a postdoctoral researcher in archaeology at Durham University in the U.K. who studies dog burials but was not involved in the Shang dynasty study. People therefore may have been less attached to puppies, an idea that counters the notion they were burying pets with their masters. And Perri notes that puppies may have had a symbolic significance, representing youthful innocence.\n\nIn addition to studying the graves and site reports, Campbell analyzed the inscriptions on engraved ox shoulder bones and turtle shell pieces, called “oracle bones,” used in divination during the Shang dynasty. The bones reveal that elite members of society likely sacrificed dogs in fire rituals—sometimes in the hundreds—to ancestors or to spirits representing weather or the cardinal directions. Due to the nature of their fiery demise, archaeologists haven’t discovered any of these canine remains yet.\n\nZhichun Jing, an archaeology professor at the University of British Columbia who studies the Shang dynasty but was not involved in this research, says that the zooarchaeological work on the dog remains from Xiaomintun “is indeed very exciting and revealing.” He believes the work could encourage a better understanding of the reasons behind ritual sacrifice.\n\nMany other societies sacrificed dogs at various periods of their history. For example, the Hittites ritually sacrificed puppies and not adult dogs; in one case discovered by archaeologists, they decapitated the puppy and placed its head between its hind legs.\n\nBut the new findings also enrich our understanding of sacrifice in the Shang dynasty. “This is a society that has human sacrifice as well as various kinds of animal sacrifice, so the idea of giving a life in ritual is a very familiar one for them,” Campbell says. One king, he notes, Wu Ding, who ruled from 1324 B.C. to 1266 B.C., may have sacrificed as many as 15,000 humans, as well as a number of livestock and dogs—an act that Campbell believes augmented the ruler’s power as an intermediary between the living and the dead.\n\nThis work first appeared on SAPIENS under a CC BY-ND 4.0 license. Read the original here.","title":"Uncovering the Sacrificial Puppies of the Shang Dynasty","origin":"Arts & Culture","image":"https://static.scientificamerican.com/sciam/cache/file/25B67196-B6A1-4341-8EA9A875916F73AF_source.jpg?w=590&h=800&B4197723-2357-48E2-A5138E6BED7BE2C8","link":"https://www.scientificamerican.com/article/uncovering-the-sacrificial-puppies-of-the-shang-dynasty/"},{"authors":"Anil Ananthaswamy","pub_date":"May 6, 2019","abstract":"One of the most mind-bending revelations of quantum physics over the past century has been that properties of particles are possibly not real until they are measured. Now a new thought experiment suggests that conclusion may be too tame: it seems that particles’ properties—their spin, for instance—may not even belong to them. This possibility is akin to saying that your personality does not belong to you.\n\nThe new study claims to demonstrate this paradoxical disconnect between particles and their properties via a new version of the so-called quantum Cheshire cat experiment. First performed in 2013, the experiment draws its name from the disappearing feline in Lewis Carroll’s Alice’s Adventures in Wonderland and involves the ostensible separation of a cat (actually, a particle) from its grin (some property of the particle).\n\nThe new version of the experiment starts with two grinning Cheshire cats and ends with the grin of one cat gracing the other cat’s face, and vice versa. In quantum terminology, it shows how two particles could end up exchanging their properties, or physical attributes.\n\n“Niels Bohr’s view [was] that until you do a measurement on a quantum system, you cannot say that the physical attribute actually exists. That questions the reality of physical attributes,” says Arun Kumar Pati of the Harish-Chandra Research Institute (HRI) in India, who co-authored the new work. “Our thought experiment takes that view a step ahead. Not only are the attributes not real, but they could not be yours. It questions the reality at a much deeper level.”\n\nThe Weak versus the Strong\n\nTo arrive at their conclusion, Pati and his HRI colleague Debmalya Das resorted to a technique known as weak measurement.\n\nIn standard quantum mechanics, examining the state of a quantum system—such as a particle or an atom—involves a so-called strong measurement, which can be something as simple as a detector registering the arrival of a photon. A particle is first prepared in some initial state, a process called preselection. Then the quantum state of the particle evolves over time, under the influence of external forces, and it can end up in a superposition of many states. The strong measurement randomly “collapses” the superposition into one of those many possible states—a process that is unavoidably destructive. For example, if you were measuring the position of a photon, a strong measurement would locate the photon but also destroy the superposition.\n\nWeak measurements, on the other hand, are not so heavy-handed. They represent an idea that goes back to 1988, to a theory devised by Yakir Aharonov, David Z. Albert and Lev Vaidman, all then at the University of South Carolina and Tel Aviv University. The trio asked, What if the measuring device interacted extremely weakly with the particle? While such a measurement would not destroy the quantum state (and thus the state would continue evolving), it would result in a value for the state with a very large uncertainty. If you performed the measurements over and over again, with an ensemble of identically prepared, or preselected, particles, then you would get a distribution of weak measurement values.\n\nOn its own, this distribution is not informative. But add one more stage to the process, and things get very interesting. After each weak measurement, you let the particle evolve and then perform a strong, destructive measurement on it. Repeat this action for every identically preselected, weakly measured particle. Each strong measurement will give a different value because of the random collapse of the superposition. Now select only those particles whose final positions have a certain value—doing so is called postselection. Then discard information about all of the particles that do not end up in this postselected stage. What Aharonov and his colleagues argued is that you can now take the weakly measured values for the subset of postselected particles and turn it into a “weak value” that tells you something about a property of the particles, such as their spin in a given direction.\n\nAn intriguing outcome of this approach to quantum systems has to do with the nature of time. According to the mathematics developed by Aharonov and his colleagues, weak values are influenced by both the initial preselected quantum state (the past) and the final postselected quantum state (the future). Time, in this way of thinking, flows both ways: the future influences the present.\n\nWhose Property Is It, Anyway?\n\nIn a first-of-its-kind experiment conducted in 2013 and published in 2014, Tobias Denkmayr of the Technical University of Vienna, Jeff Tollaksen of Chapman University and their colleagues used weak measurements to separate the quantum Cheshire cat from its grin.\n\nIn their experiment, preselected neutrons with a particular spin were sent, one by one, into a beam splitter, which is a device that splits a beam of particles into two. Each incoming neutron ended up in a superposition of two states: taking paths A and B. These two paths were recombined in a so-called interferometer, which caused the quantum states to interfere. The neutrons then headed toward output detectors. In one of the output paths of the interferometer, the experiment involved a strong measurement of a particular spin state of the neutrons. Neutrons that satisfied this criterion were considered to be postselected. The experimenters discarded all of the other neutrons for their analysis.\n\nFor these postselected particles, the team also performed two sets of weak measurements: one for the position of the particles, and the other for their spin. These dual measurements suggested that the particles were going through path B, while the weak value of their spin could only be measured in path A. The cat had been separated from its grin.\n\n\n\n\nCredit: Melissa Thomas Baum, Buckyball Design; Source: “Observation of a quantum Cheshire Cat in a matter-wave interferometer experiment,” by Tobias Denkmayr, et al, in Nature Communications, Vol. 5, No. 4492; July 29, 2014\n\n\n“From the old perspective of preparing a particle and then doing a strong measurement, it is impossible to separate a particle from its properties,” says Tollaksen, who wrote about the Cheshire cat paradox in his 2001 Ph.D. thesis.\n\nNow Pati and Das have extended the Cheshire cat experiment to their thought experiment, which not only separates a particle from its properties but also causes one particle to take on a property previously associated with another and vice versa.\n\nThe thought experiment involves putting two interferometers side by side, such that each particle first encounters a beam splitter. After going through the beam splitter, the particle enters into a superposition of two states: taking the left and right paths.\n\nThen comes a twist: The alignment of the setup is such that interferometer 1’s right path, which would normally be recombined with its corresponding left path, is instead recombined with interferometer 2’s right path. And the interferometers’ left paths are recombined as well. When recombined, the various quantum states interfere with one another. Then the two outputs from each interferometer encounter a series of beam splitters and detectors. These beam splitters are designed to make photons with one type of polarization go one way and the rest go the other way. (Polarization describes the orientation of a photon’s vibrating electric and magnetic fields.) The postselection involves choosing only those photons that cause a particular set of six detectors to click simultaneously. All other photons are discarded.\n\nAccording to Pati and Das, if one were to calculate weak values for the position and polarization of each pair of photons in the postselected ensemble, then the weak values would show that photon 1 went through the left arm of interferometer 1, whereas its polarization appeared in the left arm of interferometer 2. Similarly, photon 2 would appear in the right arm of interferometer 2, whereas its polarization would show up in the right arm of interferometer 1. At least, that is how the researchers interpret the weak values.\n\n\n\n\nCredit: Melissa Thomas Baum, Buckyball Design; Source: “Can Two Quantum Cheshire Cats Exchange Grins?” by  Debmalya Das, et al., in arXiv:1904.07707; April 14, 2019\n\n\nThis interpretation of the thought experiment suggests that after the particles and their properties are decoupled, and their paths are recombined and finally subjected to strong measurements, photon 1 ends up with the polarization of photon 2, and vice versa. The cats and their grins are first separated, and then the cats exchange grins. Also, the photons, despite being separated from their initial properties, all end up in one massive entangled state—meaning that they can only be described by a single global quantum state.\n\n“It doesn’t surprise me,” says Tollaksen, who is used to the seeming paradoxes thrown up by weak measurements. But “it’s very good work.”\n\nA Disagreement over Details\n\nExperimentalist Aephraim Steinberg of the University of Toronto is also not surprised but for different reasons. He points out that an interaction between particles results in those particles getting entangled (as happens in Pati and Das’s thought experiment), and this entanglement can lead to the particles swapping properties. Such swapping is the basis of a so-called SWAP gate, a well-studied operation used in quantum computing, Steinberg says. “It would indeed be interesting if they could swap their polarizations without ever interacting,” he adds.\n\nBut Steinberg is more surprised, even concerned, by the new experiment’s design. “It relies on two photons traversing a set of interferometers and then causing six different detectors to fire simultaneously. This is, of course, impossible,” he says. “The authors seem to be imagining one detector sensitive to where a photon is, while another detector (at another location) could simultaneously measure the photon’s polarization. In this sense, they seem to be trying to ‘build in’ the separation of the photon’s different properties from the start rather than devising an experiment to reveal it.”\n\nTollaksen also says that having six detectors firing simultaneously with only two photons is simply not possible. But he thinks Pati and Das’s conceptual idea is sound. “As far as I can tell, it seems to me that even the requirement of all six [detectors] could be boiled back down to two with the right optics and still produce the right postselection,” he says. “If so, the fundamental swapping idea they are playing with might be salvageable.”\n\nBut Pati says that his and Das’s experiment should work as designed—and with existing technology, too. “The detector clicks are for various degrees of freedom or attributes of the two photons,” he says, thus allowing for six simultaneous detections.\n\nThere is also a bigger issue of whether the weak values obtained via weak measurements are telling us something about what is real. “In general, I think too much is made of the quantum Cheshire cat being a ‘paradox,’” says theorist Michael Hall of the Australian National University. “Weak values are not the outcomes of individual measurements in general. They are only average values of many repeated measurements.” Hall argues that such average values cannot be accorded the same status as the outcome of individual strong measurements.\n\nNevertheless, weak measurements are already being used for seemingly impossible applications. For example, if one selects the initial state and the postselected state of particles such that there is very little overlap between the two, then the percentage of particles that have to be thrown away because they are not postselected becomes very large. But for the very few that remain, the weak values can be extremely useful. John C. Howell of the University of Rochester and his colleagues have already used such weak values to measure displacements of about 14 femtometers, which is roughly the size of a uranium nucleus.\n\nConcerns about Pati and Das’s thought experiment notwithstanding, the debate over the meaning of weak values is also one over the correct theory for describing the quantum world, particularly the role of time. “You need to start thinking about the relevance of the future on the present. The property [of a particle] at any given moment of time is influenced by the future,” Tollaksen says. “When you shift your thinking to that, then all of these things, like the Cheshire cat, are not at all surprising.”","title":"Spin-Swapping Particles Could Be “Quantum Cheshire Cats”","origin":"Physics","image":"https://static.scientificamerican.com/sciam/cache/file/F36141BC-F7F7-4746-98C20672E80D3735_source.jpg?w=590&h=800&6083DF17-3808-42C8-A8D07A01C42F714F","link":"https://www.scientificamerican.com/article/spin-swapping-particles-could-be-quantum-cheshire-cats/"},{"authors":"Brian Switek","pub_date":"April 30, 2019","abstract":"A good dinosaur is hard to find.\n\nI've previously laid blame on Jurassic Park for popularizing the idea that paleontologists regularly find stunning, intact, articulated dinosaur skeletons. The truth requires quite a bit more consolidant and frustrated queries along the lines of \"Where's the rest of it?\" But museums have their roles to play, too. We wish to see dinosaurs fully restored - not just the butt half of a skeleton - and this can leave us with the impression that finding a crisp, complete, and undistorted dinosaur happens all the time.\n\nPerhaps \"complete\" isn't a word we should even use in reference to dinosaur fossils. It's often difficult to tell exactly what complete is after taphonomy has taken its toll, discarding some hard tissues as well as the bonus soft tissue remnants we all hope for. And this is why the anatomical aspect of paleontology is always a comparative science, with new finds being compared to what's been seen before. Missing parts in one skeleton can be substituted with bones from a relative as a working hypothesis and placeholder. And every now and then, a lucky find helps provide more grist for the osteological mill.\n\nGobihadros mongoliensis, a duckbill dinosaur just described by paleontologist Khishigjav Tsogtbaatar and colleagues, is a great example. In technical terms, this roughly 100-million-year-old dinosaur is a \"non-hadrosaurid hadrosauroid.\"  That means it belongs to the greater family of shovel-beaked herbivores commonly known as duckbills, but lacks some of the features that would place this dinosaur closer to famous favorites like Parasaurolophus and Edmontosaurus. In other words, Gobihadros is in between the more Iguanodon-like ancestors of hadrosaurs and later species in both time and anatomy.\n\nThe wonderful set of remains described by Tsogtbaatar and coauthors drive home the point. Gobihadros is the first dinosaur of its kind to be found in the Late Cretaceous of central Asia from extensive remains, including an articulated skull, providing a rare picture of this veggiesaur and its relatives. Not only does Gobihadros seem to show some convergent characteristics with later duckbills, indicating features of the jaws and hips evolved more than once, but the hadrosaur also has a thumb spike similar to its more ancient relatives. This makes Gobihadros what experts sometimes call a mosaic - an animal that has prominent similarities to both earlier and later species from the same group, yet still on an evolutionary branch of its own instead of slotted onto a straight line of progress.\n\nReconstructed from snout to tail from multiple specimens, Gobihadros slides into the family tree alongside previously-named relatives like Bactrosaurus and Gilmoresaurus. And so Gobihadros becomes an important point of comparison, perhaps unlocking how hadrosaurs flourished and became iconic parts of Late Cretaceous life. The dinosaurs themselves couldn't have known this for a variety of reasons, but, thanks to the hindsight granted to us 100 million years later, we can perceive how this exquisite dinosaur fit into hadrosaur history.","title":"New Duckbill Dinosaur Looks Sharp","origin":"Laelaps","image":"https://static.scientificamerican.com/blogs/cache/file/2EFF77B1-5523-48D2-B7143802346EA62C_source.jpg?w=590&h=800&F61902C7-4BFB-4D05-8F83FF5D54DC3C70","link":"https://blogs.scientificamerican.com/laelaps/new-duckbill-dinosaur-looks-sharp/"}]