[{"authors":"","abstract":"","title":"","origin":"","link":"https://www.scientificamerican.com/article/trump-administration-to-issue-first-emissions-limits-for-planes/"},{"authors":"Karen Hopkin","pub_date":"May 29, 2019","abstract":"Some wild female bonobos introduce their sons to desirable females—then make sure their relations won’t be interrupted by competing males. Karen Hopkin reports.","title":"Bonobo Mothers Supervise Their Sons' Monkey Business","origin":"Evolution 60-Second ScienceSubscribe:Apple iTunesRSS","image":"https://static.scientificamerican.com/sciam/cache/file/28C04632-34E4-4612-BAC92807F560FEA2_source.jpg?w=590&h=800&D757B1E9-56B2-4378-9531CAE76CAFAA8C","link":"https://www.scientificamerican.com/podcast/episode/bonobo-mothers-supervise-their-sons-monkey-business/"},{"authors":"Abigail C. Saguy, Juliet A. Williams, Robin Dembroff and Daniel Wodak","pub_date":"May 30, 2019","abstract":"Two of us (Saguy and Williams) recently proposed that we should all use they/them pronouns. In a post in Scientific American, Hanna et al. disagreed with this proposal. This provides a helpful opportunity to clarify our idea, and to explain how it fits into broader debates about gendered language. To do so, we have collaborated on this response with Robin Dembroff and Daniel Wodak—philosophers who have independently been developing their own argument for why we should do away with gendered pronouns. Together, we seek to address some common misunderstandings about our proposal.\n\nOne thing Hanna et al. object to is our perceived failure to take “trans perspectives” into account. In fact, our work draws extensively on trans perspectives, including published work by trans authors and interviews with LGBTQ+ activists. The fact that we considered how the use of gendered pronouns may reinforce gender inequality does not mean our piece was not informed by trans perspectives.\n\nSuch an assumption risks reinforcing divisive rhetoric that on matters of gender justice, the needs of women and those of transgender and gender-nonconforming people are inevitably in conflict. We believe it is crucial to take into account multiple perspectives on gendered language and develop an intersectional approach; after all, research shows that women of color are disproportionately at risk of being misgendered.\n\nLeaving aside this methodological issue, Hanna et al.’s primary concern is that “a move toward gender-neutral pronouns ignores the important work that gendered pronouns perform in everyday life.” They contend that “avoiding the act of gendering manifests as another form of violence.”\n\nThis is a subtle issue; it needs to be handled with care. Most fundamentally, we disagree with the claim that “avoiding the act of gendering” is always a “form of violence.” It is wrong to misgender others. But it is not generally important to gender others correctly, as long as we do not gender others incorrectly. When a student refers to a university instructor as “Professor So-and-So,” this avoids misgendering by avoiding any act of gendering in the first place.\n\nThat said, selective avoidance of gendering can be wrongful. Consider an example from one of the interviews that informed this work. During an interview with a self-described gender-nonconforming person, we were told of a recent experience going through airport security. Every other passenger was addressed as “Thank you, sir” or “Thank you, ma’am.” When it came to this person, the TSA officer just said “Thank you,” which was experienced as stigmatizing because it was a glaring departure from a practice of showing respect via gendering.\n\nHanna et al.’s view is the equivalent of insisting that the TSA continue thanking “sirs” and “ma’ams”—that is, continue gendering passengers by default. Our view is the equivalent of insisting that the TSA should just say “thank you” to everyone and avoid gendering anyone. Hanna et. al. acknowledge the serious harms of misgendering but they offer no plausible solution to the pervasive problem of misgendering, especially for transgender and gender-nonconforming people and people of color.\n\nWe do. Our proposed solution is to use gender-neutral pronouns as the default, with the long-term goal of using they/them pronouns for everyone. That’s a far cry from advocating for the willful misgendering of a person who asks to be referred to using a specific pronoun or legitimizing the refusal to gender a trans person while gendering everyone else.\n\nHanna et al. further contend that our proposal is based on a “Western-centric view of language,” pointing out that there are languages like Bangla that don’t have gendered pronouns. True. There are also languages with less gendered grammar than Bangla and languages with more gendered grammar than English. This raises questions for Hanna et al. and anyone else who defends the status quo in English: if gendered pronouns are so valuable, should languages like Bangla introduce them? Are trans and gender-nonconforming persons wronged when they are not gendered (correctly or incorrectly) by the accepted pronouns in Bangla or Finnish or Malay or Kurdish? Presumably not.\n\nIt is worth reflecting on why this is so. One reason is connected to Hanna et al.’s observation that, without grammatical gender, people can still gender themselves. But crucially, without grammatical gender they have more autonomy over whether and when they are gendered: trans women can declare their gender when doing so is safe and significant, without risking misgendering by others’ use of gendered pronouns.\n\nAnother reason why avoiding gendering does no harm connects back to the discussion of equality. The preference for gendered pronouns among English-speaking trans and gender-nonconforming people is best understood as a preference for equality: just as our interviewee prefers to be treated like other passengers, a trans woman may understandably prefer to be called “she” in a world where cis women are called “she.” If cis women were all called “they”—as they are in the languages mentioned above—equality requires that trans women be called “they” too. This is particularly important in institutional contexts.\n\nAs Hanna, et al. point out (citing Dean Spade), institutions have long used gender categorization to marginalize trans and gender-nonconforming people. In light of this history, we think efforts to remove gendered language from legislation and institutional processes should be lauded. Certainly, they should not be opposed as a form of “administrative violence.”\n\nThe differences between grammatical gender systems in natural languages provide a fruitful basis for empirical research. This includes testing Hanna et al.’s hypothesis that a natural language without gendered pronouns “doesn’t have the effect of reducing gender inequality, nor does it reduce the cisnormative desire to categorize by gender.”\n\nIn fact, studies find that grammatical gender in natural languages does affect gender inequality and essentialism in gender categorization. Of course, there are confounding variables here: plenty of other social, cultural and political factors affect levels of gender equality. But a clear implication of this research is that gendered language affects gendered thought. That finding undermines Hanna et al.’s most important objection: “Getting rid of gendered pronouns as a means of addressing gender inequalities would be the equivalent of treating symptoms without treating the root of the illness.” On the contrary, evidence suggests that degendering English would reduce gender discrimination and gender essentialism. If this is right, it is an important tool of addressing gender injustices, root and branch.","title":"We Should All Use They/Them Pronouns ... Eventually","origin":"Voices","image":"https://static.scientificamerican.com/blogs/cache/file/554F6E00-D8EF-4A9C-9D641E8A3DF47318_source.jpg?w=590&h=800&DF2B7C67-3634-4FA7-B4250F965CD715D9","link":"https://blogs.scientificamerican.com/voices/we-should-all-use-they-them-pronouns-eventually/"},{"authors":"April Crow","pub_date":"May 29, 2019","abstract":"On June 6, a panel of experts convened by Scientific American and Nature Research—part of Springer Nature—will talk about the issue of plastic pollution, including possible solutions, in Washington, D.C. The author is one of those experts.\n\nThe plastic waste crisis has been growing and deepening for years. We’ve known it was happening, and too many of us have stood by and watched it escalate. Fortunately, we are starting to see the emergence of a broad and growing group of stakeholders who are coming together—in their own ways—to tackle this complex challenge. I became involved in this topic nearly 15 years ago through my environmental work at Coca-Cola. At that time, there was an emerging discussion about the Great Pacific Garbage Patch, which may now be more than twice the size of Texas. But beyond that fact, little was known.\n\nA few of us working in this area became concerned by the scale of the problem and started to ask for more science and data to help guide us to solutions. Fast-forward to 2012, when a National Center for Ecological Analysis and Synthesis (NCEAS) working group brought together, for the first time, leading academics to take a closer look at the existing science. The effort represented a turning point in understanding the problem and how we might go about devising solutions to tackle it. One of the papers the NCEAS group produced started to point at  the leaking countries—nations where the most plastic waste was flowing into our oceans—and brought into sharper focus where we could prioritize resources to begin to address the crisis.\n\nThe Ocean Conservancy followed with a groundbreaking report, Stemming the Tide, which indicated that focusing on waste management in these countries would be one of the most impactful levers to stop the flow of eight million metric tons of plastic entering the world’s oceans each year. The million-dollar question—or, in this case, the multibillion-dollar question—is where to find the resources to solve the problem. There are significant infrastructure needs in these countries, and currently, very little funding is flowing into the development of new waste and recycling solutions.\n\nOur mission at Circulate Capital is to catalyze billions of dollars to invest in such solutions by proving the case for financing for-profit enterprises that are developing in South and Southeast Asia (SSEA), the area currently responsible for 45 percent of the oceans’ plastic waste. By demonstrating the possibility to generate financial returns from these investments, we believe we can unlock the public and private institutional investors needed to address this challenge.\n\nAs we start to fund, and unlock other investments in, solutions to plastic pollution, it is important that we keep the science and data in mind to prioritize the dollars and their potential impact. That is why we are only looking to invest in companies with approaches that are scalable and replicable. In the Investment Handbook that we issued this past March, we identified disrupters at every level, from collection to aggregation and recycling, that need financial capital to scale solutions and laid out a variety of factors impacting the entire plastics value chain to help investors evaluate opportunities and deploy assets in the SSEA region.\n\nThe handbook is a first-of-its-kind guide to investment opportunities in SSEA’s municipal waste–management and recycling–infrastructure sectors, the two sectors in the region that Circulate Capital and the Ocean Conservancy identified as having the most solutions ready to scale. It lays out a variety of factors impacting the entire plastics value chain to help investors evaluate opportunities and deploy assets in SSEA.\n\nBut investing in companies such as these is not going to be enough. There are simply not enough options ready to scale. In addition to the financial capital required to help solve this problem, we also see a great need for human capital and innovation support The Incubator Network by Circulate Capital and SecondMuse, which we announced in September 2018, seeks to change that situation, The Incubator Network uses philanthropic and public funds and technical assistance to bolster and develop public and nonprofit entities to implement new approaches and build capacity that can support large institutional capital commitments.\n\nSolving the plastic-waste crisis will not be easy, and there is no silver bullet. The challenge demands the realignment of the ecosystem that generates ocean plastic pollution and the creation of one that remediates it—forming a circular economy that takes old plastic and turns it into a new, reusable resource for future materials. But we believe that the process we have begun will prove that investment in the resource-recovery sector can ultimately provide financial returns, completely change the system and start to solve the problem of ocean plastic.","title":"Science on the Hill: How to Make Recycling Profitable","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/88835330-1227-4FF9-A2FCA0CAD6CA3BA9_source.jpg?w=590&h=800&CA7D31FE-922D-4008-8E9A4EB0810DA9BA","link":"https://blogs.scientificamerican.com/observations/science-on-the-hill-how-to-make-recycling-profitable/"},{"authors":"Helen Shen","pub_date":"May 30, 2019","abstract":"As the world’s aging population grows rapidly, so has its appetite for health tips, tricks and products that could help guard against the ravages of time. Among countless dietary supplements—vitamins, minerals and other products—some people have pinned their hopes on a molecule called nicotinamide adenine dinucleotide (NAD), a key player in the cellular production of energy. Often written as NAD+, the name of its oxidized form, the molecule participates in a host of metabolic pathways and is involved in other important processes, such as DNA repair. NAD+ levels naturally decline as people and animals age, and this loss has been proposed as contributing to the underlying physiology of aging.\n\nStudies show that boosting NAD+ levels can extend life span in yeast, worms and mice. Animal research also indicates NAD+’s promise for improving several aspects of health. Raising levels of the molecule in old mice appears to rejuvenate mitochondria—the cell’s energy factories, which falter over time. Other mouse studies have demonstrated benefits such as improved cardiovascular function, enhanced muscle regeneration and better glucose metabolism with NAD+ supplementation.\n\nBanking on such results, multiple companies currently sell dietary supplements containing NAD+ precursors such as nicotinamide riboside (NR) or nicotinamide mononucleotide (NMN). NR supplements, in particular, have attracted buzz for the scientific star power associated with two major suppliers, ChromaDex and Elysium Health. The companies’ research advisers hail from institutions such as Stanford, Harvard and Columbia University. Elysium’s scientific advisory board currently boasts eight Nobel laureates.\n\nBut the NR business and some scientists involved have attracted their share of criticism as well. Unlike drugs, dietary supplements are lightly regulated by U.S. authorities, allowing them to be sold before research confirms their safety and effectiveness in humans. Recent clinical trials funded by ChromaDex and Elysium show that adults taking NR-containing supplements for six to eight weeks experience increased levels of NAD+ in their blood without serious side effects. But researchers are still working to prove that NR can actually improve human health—a sticking point for critics and an issue acknowledged by the companies themselves.\n\n“Not everything that works in mice works in humans, which is why it’s critical to do the rigorous human trials,” says Leonard Guarente, a co-founder of Elysium and its chief scientist. The company is studying the effectiveness of its NR-containing supplement for a number of conditions in people, including kidney injury and fatty liver. Early this year, Elysium published a small trial showing that its product could potentially slow the neurodegenerative disease amyotrophic lateral sclerosis. ChromaDex’s NR supplement is also the subject of many clinical trials, with the company recently sponsoring a study of its effects on cognitive function, mood and sleep in people older than 55.\n\nFor very different reasons, NAD+ has also attracted a wave of attention from cancer researchers. Recent studies suggest that cancer cells of many types depend on NAD+ to sustain their rapid growth and that cutting off the NAD+ supply could be an effective strategy for killing certain cancers. The data from these studies paint a more complicated picture of NAD+ and raise new questions about the diverse ways taking an NAD+-boosting supplement might influence health. “It might still slow down the aging part, but it might fuel the cancer part,” says Versha Banerji, a clinician-scientist at the University of Manitoba. “We just need to figure out more about the biology of both of those processes, to figure out how we can make people age well and also not get cancer.”\n\nIn a Nature Cell Biology study in February scientists reported a newly discovered role for NAD+ metabolism at the intersection of cellular aging and cancer—specifically, in a process called cellular senescence. Senescence occurs when aging, damaged cells stop dividing. The process can help suppress cancer, but it leads cells to produce inflammatory molecules that can also promote cancer growth under certain conditions. In the Nature Cell Biology study, Rugang Zhang of the Wistar Institute, and his colleagues found that in cells entering senescence, rising levels of NAMPT (a major NAD+-producing enzyme in mammals) encourage the release of inflammatory and potentially protumor molecules. Consistent with those findings, mice genetically predisposed toward pancreatic cancer developed more precancerous and cancerous growths when they consumed the NAD+ precursor NMN. Zhang says more research is needed to fully understand the role of NAD+ in cancer, but he adds that “we should be cautious and bear in mind the potential downside of NAD+ supplementation as a dietary approach for antiaging.”\n\nZhang’s work is part of a growing body of research that has drawn attention to NAD+ metabolism in cancer, particularly involving NAMPT. Compared with healthy tissues, elevated NAMPT levels have been reported in several human cancers including colorectal, ovarian, breast and prostate cancers. In studies in animals and cells, drugs that inhibit NAMPT have shown promise in killing cancer cells or enhancing the effectiveness of other cancer therapies.\n\nIn 2016 researchers at Washington University School of Medicine in St. Louis found that among people with glioblastoma—an aggressive form of brain cancer—tumors with higher NAMPT levels correlated with shorter survival times. When human glioblastoma cells were implanted in mice, the cells proliferated and established new tumors. But when researchers suppressed NAMPT in these cells before implantation, they later saw reduced brain-tumor formation and increased survival in the mice—suggesting that glioblastoma cells depend on NAMPT and NAD+ to thrive.\n\nWhat might this result say about NAD+-boosting supplements? “There’s a lot of buzz about taking NAD+ precursors for their anti-aging effects, which is based on a lot of great science,” said Albert Kim, senior author of the 2016 study, in a School of Medicine press release “I don’t know if taking NAD+ precursors makes existing tumors grow faster, but one implication of our work is that we don’t yet fully understand all of the consequences of enhancing NAD+ levels.”\n\nThese emerging questions are not ruffling makers of NR supplements. “I’m not losing sleep over this,” says Charles Brenner, chief scientific advisor for ChromaDex. Reports of higher-than-normal NAMPT levels in many cancers do not prove that high NAD+ levels actually promote cancer growth, he notes. He contends that studies that kill cancer cells by suppressing the NAD+-producing enzyme also do not properly address the issue. \"Whether low NAD+ would block cancer and whether high NAD+ would promote cancer are two separate questions,” he says.\n\nIndeed, Zhang’s study is one of the first to directly show that providing supplemental NAD+, via the precursor NMN, was associated with increased cancerous growths in mice. But Elysium’s Guarente is skeptical of the data, arguing that Zhang’s study showed a small effect in a small number of animals and that it has yet to be replicated by other groups. “I don’t think the evidence is there at all to say that raising NAD+ levels would favor cancer,” Guarente says.\n\nAt the moment, the idea that elevating NAD+ levels could fuel cancer growth remains a hypothesis, but it is one that has attracted considerable attention. Cancer cells have high metabolic needs, including processes requiring NAD+. And many types of cancer cells boost NAD+-making enzymes and then die when those enzymes are blocked by drugs. “We know that they like NAD+, but it’s too early to say, if you add NAD+, whether they will grow really fast,” says Shashi Gujar, a cancer immunologist at Dalhousie University. “Many labs are working to figure that out.”\n\nThe answer may not be a single or straightforward one. NAD+ is a ubiquitous and fundamental molecule, involved in many biological pathways and cellular operations. Its ingestion could lead to a mix of positive and negative outcomes, the balance of which might depend on context. NAD+ precursors, consumed orally, may be taken up by some tissues more than others. And different cell types are known to employ distinct metabolic programs, which could lead to tissue-specific responses to NAD+.\n\nLike the tissues from which they arise, cancers are diverse in their cellular ways—and at least some run counter to the “cancer fuel” hypothesis of NAD+. A 2014 study, for instance, reported that in a mouse model of liver cancer, inhibiting NAD+ production was a key step by which an errant gene caused DNA damage and tumor formation. In this case, feeding NR to the mice actually helped protect against these harmful effects.\n\nTogether these findings do not necessarily point to ready answers for consumers interested in NR or NMN supplements, so much as they highlight questions for scientists to address in the coming years. “I would say that given that many people are taking these supplements for health benefits, a study of what these do to cancer risk or existing cancer biology is warranted,” says Matthew Vander Heiden, a clinician-scientist at the Massachusetts Institute of Technology’s Koch Institute for Integrative Cancer Research.\n\nThe need for more evidence is a sentiment that is shared by others. “There is tremendous interest in the NAD+ field right now,” Gujar says. “And I’m pretty sure sooner or later, we will have the evidence to answer this.”","title":"Cancer Research Points to Key Unknowns about Popular “Antiaging” Supplements","origin":"Medicine","image":"https://static.scientificamerican.com/sciam/cache/file/15099623-E8AA-465F-81EBA8649E6ADFFC_source.jpg?w=590&h=800&38FDA6A9-0CCF-469B-A10D028487544964","link":"https://www.scientificamerican.com/article/cancer-research-points-to-key-unknowns-about-popular-antiaging-supplements/"},{"authors":"Diana Kwon","pub_date":"May 29, 2019","abstract":"Few things are more refreshing than enjoying a cool beverage after spending a day under the hot summer sun. But gulping down a drink does not always quench thirst. Seawater, for example, may look appealing to someone stranded in the middle of the ocean, but taking a swig of it will only worsen dehydration.\n\nScientists have now discovered that in rodents, signals from both the throat and gut control feelings of thirst. These distinct pathways may explain why consuming a beverage is typically refreshing but does not always sate one’s thirst, according to a study by Yuki Oka, a neuroscientist at the California Institute of Technology, and his colleagues at the California Institute of Technology, published May 29 in Neuron.\n\nLast year, Oka’s team reported that the simple act of gulping activated a circuit in the lamina terminalis, a region near the front of the brain, which ultimately led to the suppression of activity in neurons responsible for generating feelings of thirst. This throat-brain pathway, which the researchers identified in mice, switched on regardless of what an animal consumed—water, saline solution and oil produced similar effects. But the fact that all of these substances were able to inhibit the brain’s “thirst” neurons indicated that there was something missing. After all, if any liquid could satisfy an animal’s thirst, it might not consume enough water to remain hydrated.\n\nAccording to Oka, behavioral studies in animals dating back decades suggested that there was an additional mechanism in the gut that signaled the presence of water to the brain. So in their latest investigation, Oka’s team set out to map the brain circuits responsible for receiving these signals. By injecting fluids directly into the guts of mice, the researchers discovered that in order for the rodents to feel fully hydrated, this second gut-based circuit needed to be activated. Without these gastrointestinal signals—which, unlike ones from the throat, selectively responded to the presence of water—the brain’s “thirst” neurons quickly revved up again, driving the animals to drink more.\n\nThe throat sends an immediate but temporary thirst-quenching signal to the brain once an animal starts gulping. At that point, the body is unaware of what it is consuming, and “you don’t want to make a mistake and keep drinking something that’s dehydrating,” Oka explains. The second signal from the gut then acts as a “checking mechanism” that makes sure to hold the brakes on thirst only if what the body consumed was actually water. \n\nIn another study, published earlier this year in Nature, Zachary Knight, an associate professor of physiology at the University of California, San Francisco, and his colleagues also reported that the gut sensed the water content of an ingested fluid and sent a signal that suppressed thirst in the brain. “[It] is rather cool that two separate groups reach the same conclusion at about the same time,” says Charles Bourque, a neuroscientist at McGill University, who was not involved in either investigation. \n\nOver the past few years, a handful of labs have identified two parallel stories explaining the neurobiology of thirst and hunger. While these processes are mediated by distinct populations of cells in the brain, “the logic is very similar in both systems,” Knight says. After drinking and eating, the brain receives a very fast signal from either the throat or mouth that it uses to estimate how much an animal has ingested or what it is going to ingest. Then, after a delay of a couple of minutes, it gets a slower signal from the gut that confirms what was consumed (in the case of hunger, calories, and in the case of thirst, water).\n\nOka’s team also wanted to know whether both the oral and gastrointestinal signals were rewarding—driving the underlying motivation to drink. To find out, the researchers measured the levels of dopamine, a neurotransmitter involved in generating feelings of pleasure, in the rodents’ brain. Doing so revealed that while the act of drinking both water and saline led to a release of dopamine, gut infusions of those liquids had no effect. In addition, water-deprived mice were willing to press a lever to receive a spurt of water into the mouth but not directly into the stomach. “If the animals like something, they would work for it,” says study co-author Vineet Augustine, a graduate student in Oka’s lab. This finding, he adds, “clearly shows that [water in the gut] is not rewarding even though it is satiating.” \n\nInterestingly, other groups have found that unlike water, nutrients in the gastrointestinal tract do stimulate the brain’s pleasure centers. Ivan De Araujo, a neuroscientist at the Icahn School of Medicine at Mount Sinai, says that this result is not too surprising because work by his lab has shown that water in the gut does not influence behaviors in the same way nutrients do. One reason for this difference, according to De Araujo, may be that water concentrations need to be tightly controlled, while excess energy from food can be stored for later use. In addition, unlike water, there is a wide diversity of nutrients that an animal can consume—and work by De Araujo and others has shown that animals can develop, independent of taste, a set of flavor preferences that hinge on the caloric content in the gut.  \n\nBut these differences raise some interesting questions. For one, De Araujo wonders what happens when nutrients are consumed in liquid form. “If you are really thirsty, and you buy a can of Coke to quench your thirst, you are putting a lot of sugar in the gut at the same time,” he says. “[This] may somehow mix the two drives and form ambiguous rewards.”\n\nMore research is needed to confirm why nutrients in the gut are rewarding, while water is not. Another uncertainty is how, exactly, signals travel from the throat or gut to the brain. Knight’s group began to address this topic in its recent paper by demonstrating that the vagus nerve, a bundle of fibers connecting the brain stem to the major organs in the body, was involved in the process. But there are still open questions about what kind of information is being sent and whether other signals, such as hormones, also play a role.\n\nWhile most of these studies have been conducted in rodents, researchers think that similar circuits exist in humans. There is some evidence for this idea, such as neuroimaging work in people that have identified activity in the lamina terminalis in response to thirst. In addition, according to Knight, the brain areas under investigation are thought to be evolutionarily highly conserved because they are involved in basic behaviors needed for survival.  \n\nFurther probing these circuits could help scientists understand what happens when they go awry. For example, elderly people tend to feel less thirst, which can make them vulnerable to dehydration. On the other hand, some individuals experience polydipsia—a condition that causes excessive, unquenchable thirst. This research could also shed light on the mechanisms that underlie many of our behaviors. “Basic drives like hunger and thirst are the reason we do a lot of the things that we do,” Knight says. “So understanding how they’re regulated provides insight into the origin of our own motivations.”","title":"Does Thirst Start in the Mouth or the Gut?","origin":"Neuroscience","image":"https://static.scientificamerican.com/sciam/cache/file/C508712C-52A5-4A19-B9B5D4D196033A8A_source.jpg?w=590&h=800&084F9D85-3CE5-4079-80E4B56807E63748","link":"https://www.scientificamerican.com/article/does-thirst-start-in-the-mouth-or-the-gut/"},{"authors":"Elizabeth Howell, SPACE.com","pub_date":"May 29, 2019","abstract":"Scientists scouring old Kepler Space Telescope data have tracked down 18 more relatively small exoplanets imaged by the famed planet-hunting observatory.\n\nWhile most of the planets orbit close to their parent stars and have scorching surface temperatures of up to about 1,830 degrees Fahrenheit (1,000 degrees Celsius), one world orbits a small red dwarf star in an area called the “habitable zone.” That term is usually defined as the area around a star where a rocky planet could host liquid water on its surface. However, life is never a slam-dunk, and on this world, it would be particularly tricky because red dwarfs put out killer X-rays that could make living on nearby planets a challenge, even for microbes.\n\nA new computer algorithm flushed out the hidden planets from data gathered by K2, Kepler’s late-in-life observing program. K2 was developed after several of Kepler’s gyroscopes (devices that allow a telescope to maintain a consistent orientation in space) had ceased working by 2013 after four years of operations in space, well exceeding their design lifetime. \n\nScientists figured out how to stabilize the telescope’s pointing using the constant pressure of particles streaming from the sun, hopping around from time to time to protect its sensors from solar light. Kepler found its planets using the “transit method,\" which notices when a planet passes in front of its parent star and produces a drop in brightness. \n\nK2 allowed Kepler to observe 100,000 more stars before the telescope ran out of fuel in 2018, including 517 stars that scientists had already spotted planets orbiting. The researchers behind the new study decided to revisit those stars with a new data-processing algorithm.\n\n“Standard search algorithms attempt to identify sudden drops in brightness,” lead author René Heller, an astrophysicist at the Max Planck Institute for Solar System Research, said in a statement. “In reality, however, a stellar disk appears slightly darker at the edge than in the center. When a planet moves in front of a star, it therefore initially blocks less starlight than at the mid-time of the transit. The maximum dimming of the star occurs in the center of the transit just before the star becomes gradually brighter again.\"\n\nThe new algorithm attempted to plot a more realistic “light curve,” or pattern of dimming as the planet moves across the face of a distant star. This made it easier to find small planets in the data: The new planets Heller and his colleagues found range from 70% the size of Earth to double our planet’s size. The research team says their new algorithm also makes it somewhat easier to spot small planets amid natural brightness fluctuations of a star, such as those caused by sunspots, and other variables in observation.\n\nMore Earth-size exoplanets might be lurking in the data. Planets that orbit more frequently around a star have a greater chance of being spotted, because they pass in front of the star more often. But planets that are farther away might have gone undetected in the data, since their crossings are less frequent.\n\nThe researchers plan to apply their algorithm to the rest of the Kepler data, and say they may yield up to 100 new Earth-size worlds. \n\nTwo papers based on the research were published this month in the journal Astronomy and Astrophysics. \n\nCopyright 2019 Space.com, a Future company. All rights reserved. This material may not be published, broadcast, rewritten or redistributed.","title":"18 Small New Worlds Found in Old Planet-Hunting Data","origin":"Space","image":"https://static.scientificamerican.com/sciam/cache/file/63D32C5E-2032-40C9-AAD14F528DA7702C_source.jpg?w=590&h=800&4D18750C-6D26-4A55-9ECF17D99D987AFD","link":"https://www.scientificamerican.com/article/18-small-new-worlds-found-in-old-planet-hunting-data/"},{"authors":"Rick Hohner","pub_date":"May 29, 2019","abstract":"Several years ago, a patient of mine was suffering from intrusive thoughts and associated rituals, stemming from a case of obsessive compulsive disorder (OCD). One day, she took a significant risk and shared her distress with her primary care provider, who realized that she needed a behavioral health clinician (BHC). Fortunately, one was \"embedded\" at the same healthcare provider where I work—a concept that is gaining in popularity.\n\nSoon, she eliminated 90 percent of her ritualized behaviors and regained control. If she had not spoken up, and her primary care provider was not equipped to input data and integrate with a BHC, the story might have ended differently.\n\nEvery day, patients visit a health care provider or hospital in order to discuss one or more physical symptoms, when in reality what they may really need is to talk about their mental health. In a Collaborative Care Model (also called Integrated Care or Health Homes), the opportunity for patients to receive comprehensive care for all of their physical and behavioral health needs is greatly enhanced.\n\nUntreated behavioral health issues can have significant downstream repercussions on physical health, and can occasionally cause an alteration to treatment protocols. Whether it is heart disease or cancer or psoriasis, physical ailments are closely connected to mental health conditions such as depression and anxiety.\n\nWhen psychological problems lead to or exacerbate physical symptoms in a traditional “compartmentalized care” setting, it is more likely that the patient’s overall health (mind and body) will be impacted by disconnected treatment. Cardiologists are not skilled psychiatrists, and vice versa, but by coming together in a Collaborative Care Model, they can each be more confident in knowing that their patient is receiving holistic treatment.\n\nThe collaborative care “team” may include a primary care physician, a mental health specialist (social workers, psychiatric nurse practitioners, counselors, psychologists, and psychiatrists), and other physical medicine specialists who may be treating the patient. All team members agree to hold each other accountable and to work in sharing their knowledge of the patient with the overall goal of using this information to ensure the best possible outcome.\n\nEach team member is expected to set goals with the patient that are aligned with and supportive of those from other team members. This model implies that team members share ideas, outcomes (positive or otherwise) and recommendations with each other in a respectful and supportive manner. Finally, the team evaluates the outcomes using validated measures and makes adjustments to the collaborative plan accordingly.  \n\nGaining Traction\n\nResearch around collaborative and integrated care models began in the 1970s, so this is not a new idea, but adoption has been tepid. Fortunately, with more than 80 trials completed, there is greater acceptance of this approach as a well-supported model of care. In true collaborative fashion, many of these trials involved multiple specialties, including OB-GYN, pediatrics and pain management, and studied patients with a range of complex needs. As recently as 2016, the American Psychiatric Association and the Academy of Psychosomatic Medicine (since renamed the Academy of Consultation-Liaison Psychiatry) issued a report and a public statement recommending collaborative care.\n\nOne case study, tracking the impact of collaborative care in a hospital system, demonstrated a 57 percent reduction in depression among primary care patients. Results like these can nudge other health care systems and leaders to adopt technologies and software that facilitate the collaborative care approach.\n\nThe Road Ahead\n\nThe benefits of patient-centered collaborative care go beyond better physical and mental health. This approach takes into account the patient’s values, beliefs and preferences while encouraging him or her to actively participate in an individualized treatment plan. Additionally, the economic benefits of collaborative care must not be overlooked. Depression costs employers an estimated $44 billion annually in lost productivity.\n\nAcross all disciplines, we need to find ways to integrate our knowledge and skills so that everyone can engage in such a dynamic and effective mode of care. Despite the positive results from collaborative care models, there is still work to be done to successfully adopt these practices across our health care landscape. I recently joined NeuroFlow, a health care technology company whose goal is to bridge the gap between mental and physical health in all care settings. But currently, fewer than 3 percent of psychiatrists and psychiatric nurse practitioners work with primary care physicians in designing and implementing treatment plans for their shared patients. Increasing that percentage would allow many more patients to reaping the benefits.","title":"The Case for Collaborative Care","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/8BDE57FC-E1A6-4CB0-A8767E4A905BC518_source.jpg?w=590&h=800&70476F77-DD2A-4270-815BA95FA71D2763","link":"https://blogs.scientificamerican.com/observations/the-case-for-collaborative-care/"},{"authors":"Everyday Einstein Sabrina Stierwalt","pub_date":"May 29, 2019","abstract":"Imagine if we could hop over to our nearest massive galaxy neighbor Andromeda and check out what our Milky Way looks like from the outside.\n\nOr, want to know if the recently discovered Earth-like exoplanet is habitable? Send a probe over to check! The main challenge to space exploration has never been our imagination, or even our ability to come up with new technologies to make space flight possible, but the vastness of space.\n\nIt took 9.5 years for the New Horizons spacecraft just to get to Pluto which is still within our solar system. The nearest star (after our Sun) is more than 26 trillion miles away! Wouldn’t it be convenient if the universe offered us a shortcut?\n\nWhat Is a Wormhole?\n\nTheoretical physicists have hypothesized the existence of such shortcuts through spacetime since the 1930s, originally calling them white holes and eventually Einstein-Rosen bridges. A white hole acts like the reverse of a black hole by emitting energy while not allowing anything to enter. (Black holes, of course, allow matter and energy to enter but, much like the Hotel California, once you enter, you can never leave.) Since the name “Einstein-Rosen bridges” is a bit dry, they became more commonly known as wormholes.\n\nYou can picture a wormhole as a kind of tunnel that connects two points in spacetime. That tunnel could be a straight chute or take a more winding path. If the wormhole is \"traversable\" it acts as a shortcut through spacetime, connecting two points that would otherwise be far apart. Wormholes could connect different spots within a single universe or they can connect different universes.\n\nThe most common way wormholes are depicted is to imagine you are holding a piece of paper that represents normal space. Think of traveling through space as traveling along the sheet of paper. Now mark a point at each end and bend the piece of paper in half, bringing those two points together but without letting them touch. If you were to travel in normal space (i.e. along the sheet of paper) the trip from one of your marks to the other would be longer than if there were a tunnel or a \"wormhole\" connecting the two points on the paper through the empty space between them.\n\nNow the big question—do wormholes really exist?\n\n\n\n»Continue reading “Are Wormholes Real?” on QuickAndDirtyTips.com","title":"Are Wormholes Real?","origin":"Space","image":"https://static.scientificamerican.com/sciam/cache/file/D26FF170-7269-4BCE-A8685B30C003B93D_source.jpg?w=590&h=800&6AF60700-17C6-4B39-A2F4315712F89E98","link":"https://www.scientificamerican.com/article/are-wormholes-real/"},{"authors":"Robin Verble","pub_date":"May 29, 2019","abstract":"As summer approaches and wildfire season kicks into high gear, we can once again expect a series of heart-wrenching tales about injured animals fleeing from the flames. Accounts of endangered species threatened by these disasters and descriptions of koalas with burned paws tug at our collective heartstrings. But such articles tend to miss the big picture: Fires represent an ecological trade-off. While some species are threatened, others benefit.\n\nIn fact, closer analysis of the scientific literature reveals that koala mortality tends to be highly localized, and resource depletion from intense wildfires only poses short-term effects for koalas: populations will recolonize burned areas a few months after a fire.\n\nIn my work as a fire ecologist who studies the effects of fire on animal populations and community structure, one of the most common questions I am asked is about the risks that such fires pose to wild animals. It is important to remember that for many species, fires aren’t necessarily harmful and, in many cases they may be the key to preserving and restoring habitat. Many animals have evolved techniques to avoid, survive and even thrive on fire-prone areas.  \n\nAdditionally, fires have existed on our landscape for millennia, and these animal species have adapted in the presence of periodic fires. It is only recently, in the face of a multitude of human activities (e.g., deforestation, urbanization, development, increasing carbon emissions, fire suppression) that some species became endangered and that fires became an imminent threat.\n\nIn fact, in many ecosystems, our aggressive suppression of fire degrades habitat and threatens species more than allowing fires to burn; considerations about economics and livelihood obviously are a key part of this decision, but many fires occur in relatively remote areas and are not imminent human threats. Ultimately, your views on wildfire may largely be determined by which animal species you value most\n\nAnd for some species, fires are essential. Saproxylic insects rely on fires to create the best habitat for nest sites. One such deadwood specialist, Melanophila acuminata (known as the fire beetle), has evolved chemical receptors to sense smoke on its antennae and thermal receptors under its wings to detect heat. These adaptations increase its competitive advantage over other early postfire colonizers.\n\nFor the federally endangered American burying beetle (Nicrophorus americanus), fires create the open savanna habitat that it needs to thrive and reproduce. Likewise, the red-cockaded woodpecker and Delmarva fox squirrel both require habitats that are maintained by periodic fires. Large herbivores such a bison maximize their time in recently burned areas, because the regrowing vegetation has a higher nutritional content and increased palatability. Pollinating insects benefit from increased flowering plant growth postfire. These examples are only a few of the many ways in which fires directly benefit wildlife populations.\n\nPerhaps the best case for the importance of fires to wildlife is recent evidence that some animals intentionally spread them. Research by geographer Mark Bonta, then at Pennsylvania State University, and his colleagues documents fire spreading by three species of raptors in Australian savannas. Collectively called the firehawks, these birds have long been known to the indigenous communities in the areas where they occur. The firehawks transport burning sticks in their talons and beaks to unburned areas to ignite fires there. As a result, small mammals and insects fleeing the fires become accessible prey for the birds.\n\nThese birds are distributed on three continents, and there are limited accounts of other species (including the crested caracara in the southern U.S.) that exhibit similar behaviors. The potentially widespread occurrence of this behavior led Bonta and his colleague to speculate that “humanity’s acquisition and manipulation of fire may be a result of the observation of intentional avian pyrophilic behaviour rather than solely from some relationship with lightning-caused fire”.\n\nMost animals are likely to be just fine this fire season, and some will probably welcome the flames. But with rapidly changing fire regimes, in part because of climate change—namely, increases in the length of the fire season and in fire frequency, acreage burned and overall fire intensity—the future impacts on wildlife should be closely monitored.","title":"A Bit of Wildfire Isn’t Necessarily a Bad Thing","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/C02761E0-7823-4072-AC082043E8E8F16B_source.jpg?w=590&h=800&7497EF95-D44C-49C8-AE5A246EC80A9C30","link":"https://blogs.scientificamerican.com/observations/a-bit-of-wildfire-isnt-necessarily-a-bad-thing/"},{"authors":"Annie Sneed","pub_date":"May 29, 2019","abstract":"A kaleidoscopic diversity of Earth’s plants and animals underpins human existence but is under major threat from the environmental degradation wrought by human activities from mining to agriculture. A million species face extinction—many within decades—without major changes to the way we interact with nature, according to a United Nations–backed report released earlier this month.\n\nBut there is a bright spot: this decline is happening at a slower rate on indigenous peoples’ lands, according to the report, which was compiled by a panel called the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES). Its authors and other conservation experts say the world should not only draw lessons from those and other local communities’ environmental stewardship but that scientists and policy makers need to support and partner with them in order to stem the tide of biodiversity loss.\n\n“On average, they are doing a better job of managing natural resources and environmental hazards like species decline and pollution,” says Pamela McElwee, one of the report’s lead authors and an associate professor of human ecology at Rutgers University. “This is a watershed moment in acknowledging that indigenous and local communities play really important roles in maintaining and managing biodiversity and landscapes that the rest of us can learn from.”\n\nStewards of Biodiversity\n\nThe report says at least a quarter of our planet’s land is owned, used, occupied or managed by indigenous peoples. And that includes 35 percent of terrestrial areas with very low human impacts, as well as approximately 35 percent of lands under formal protection. The numbers would rise even higher if groups the report designates as “local communities”—considered nonindigenous, but with strong ties to the land through livelihood and other factors—were included. “We have always been saying that if you really look at it, indigenous peoples manage very large areas of biodiversity. But to have governments accept that, and to make it one of the major findings of the report, is quite significant,” says Joji Carino, who is Ibaloi-Igorot from the Philippines’ Cordilleras Highlands and a senior policy adviser of the Forest Peoples Programme. This nonprofit human rights organization works with indigenous peoples, particularly in tropical forest countries.\n\nThe report found that indigenous and local communities contribute in many significant ways to biodiversity. By combining wild and domestic species in gardens, for example, they have created habitats that are much more diverse and species-rich than typical agricultural landscapes—which are often vast fields with acre upon acre of the same crop. “In some cases, there are 300 or 500 species in a garden,” says Zsolt Molnár, a coordinating lead author of the IPBES global assessment and an ethnoecologist at the MTA Center for Ecological Research in Hungary. Many indigenous and local communities actively manage their lands, such as through traditional burning practices that promote biodiversity in places including Australia. They also carry out ecological restoration of degraded lands, such as in the U.S. Pacific Northwest, where indigenous communities have been involved in restoring shellfish populations and native plant species.\n\nIndigenous peoples and local communities also play an important role in long-term monitoring of ecosystems. This is critical, especially because some of these groups live in remote, hard-to-reach areas, such as the Arctic or Amazon forest. “It’s really [these communities] that are collecting the data, often through everyday experiences, so they can report back trends for species, population numbers over time, interactions between species, noticeable declines,” McElwee says. “That monitoring role can be really important, particularly where we don’t have a long-term scientific presence.” For example, indigenous communities in the semipolar regions of the U.S. and Canada have collaborated with those countries on the Local Environmental Observer network, which collects observations on everything from temperatures to wildlife sightings.\n\nOften, though, scientists and governments have not recognized—or have even denigrated—the contributions of indigenous and local communities to biodiversity conservation and ecosystem health. In Hungary, for instance, traditional herders had long allowed livestock to graze grasslands, which helped promote biodiversity by maintaining the balance of plant species. But when the country established national parks several decades ago, government authorities often discouraged, restricted or even outright banned traditional grazing on grasslands. “The problem was that science had no understanding of traditional herding and its impacts,” Molnár says. It is only over the last couple decades that government authorities and scientists have recognized herders’ crucial role in grassland management and have started reintroducing and supporting traditional grazing in the parks.\n\nA Different View of Nature\n\nIndigenous and local communities tend to succeed at conservation for a number of reasons, say experts such as Eduardo Brondízio, co-chair of the IPBES global assessment and an anthropologist at Indiana University Bloomington. These communities have long histories with their lands, which have provided sustenance in a very direct and intimate way. “When you understand the potential uses and the values of hundreds of species, you see a forest differently than if you don’t recognize that,” he says. Social norms and rules can also help communities regulate their natural resources. “It’s about [viewing] the landscape not only from the perspective of just agriculture or ranging,” Brondízio notes. “Instead of focusing on a single management issue, they look at the function of landscapes and what is important to keep in terms of connectivity, how different habitats can be managed to complement each other.”\n\nThey also tend to have a deeper understanding of local ecosystems and their dynamics, and this can help make better-informed management decisions. “Community-based institutions are often more successful than government policies or institutions (like formal protected areas) simply because they are closer to the ground and can respond more quickly to changes or threats,” McElwee says.\n\nIn addition, many indigenous and local communities tend have a reciprocal relationship with nature, rather than viewing nature as existing to serve humans—as much of Western culture has historically regarded things. “The institutions, the cultural values, the way of living and the way you see nature itself—as [inseparable] from your social life and identity—that creates a different view of what to use, how to use and how to deal with the tradeoffs of use,” Brondízio says. As McElwee notes, “Even if we don’t acknowledge it, the water we drink, the air we breathe, the food we produce—it all depends on healthy ecosystems. That is a lesson we can learn from indigenous peoples and local communities who know this already, and who are actively conserving and managing lands.”\n\nExperts say indigenous and local knowledge is—and will be—a critical part of protecting the planet’s biodiversity and the overall health of our ecosystems. This means governments and scientists need to be allies with these communities by amplifying their voices, including them in scientific assessments, recognizing territorial rights and creating partnerships between scientists and indigenous and local communities. “One of the big points is that governments really have to respect our knowledge, values and innovations,” Carino says. “As well as recognize land tenure systems, access rights, and so on.”\n\nAt the same time, many of these communities and their lands face immense threats. They are dealing with pressures from encroaching infrastructure, agriculture, mining, logging and other activities that also endanger biodiversity. There are internal pressures as well, Brondízio says. “Poverty is a major issue among indigenous and local communities,” he explains, adding that this can put pressure on their natural resources or allow outsiders to exploit communities.\n\nStandard conservation practices can present a threat too, experts including Molnár and Carino say. “There are many bad examples where indigenous territories were appropriated by the government, declared a protected area, and indigenous peoples were translocated or just simply killed,” explains Molnár, pointing to the example of the Ogiek, who were evicted from their homes on Mount Elgon in Kenya. “If we want indigenous peoples to become allies in the protection of biodiversity, then we have to respect their rights,” he says. Carino agrees. “Really, the whole way of conservation in the future needs to be rethought,” she says. “It has to be conservation with respect for human rights of the peoples who are living there, who are managing these areas.”","title":"What Conservation Efforts Can Learn from Indigenous Communities","origin":"Conservation","image":"https://static.scientificamerican.com/sciam/cache/file/05442AF1-C7D1-4275-A0FA4B76C7156C68_source.jpg?w=590&h=800&946CA797-B9F4-490D-B11D754633CCA1EC","link":"https://www.scientificamerican.com/article/what-conservation-efforts-can-learn-from-indigenous-communities/"},{"authors":"Jacob Carter, Anita Desikan, Gretchen Goldman","pub_date":"May 29, 2019","abstract":"Does it feel like science has been under attack in federal policy lately? It isn’t your imagination. As of this month, the Trump administration has attacked science more than 100 times—more than any other administration since the Union of Concerned Scientists (UCS) began tracking.\n\nBefore Donald Trump came along, George W. Bush’s administration posed the biggest threat to public health and safety, with a documented 98 attacks on science, according to our count. But while those attacks happened over eight years, the Trump administration topped them in a mere two and a half years. The Bush administration was no friend to federal scientists or their work. Indeed, for a long period of time, it was widely regarded by many to be the most antagonistic administration toward science and science policy in modern history.\n\nOnly three years into Bush’s first term, his administration’s assaults on science prompted a group of senior scientists who had advised every Republican and Democratic administration dating to President Dwight D. Eisenhower’s to pen a letter protesting these actions., which was entitled “Restoring Scientific Integrity in Policy Making The statement accumulated signatures by around 15,000 scientists, including 20 Nobel laureates, over the next four years. The antiscience actions taken by the Bush administration are largely why science-based federal agencies now have scientific-integrity policies in place to deter political interference in federal science.\n\nThe 98 political attacks that UCS documented on the federal science apparatus during the Bush years run the gamut from censoring the ability of federal scientists to speak about their work, to altering scientific information in reports and government Web sites, to downplaying the effects of climate change, to disbanding scientific advisory committees.\n\nThese strategies are not new. We have seen nearly every administration in modern history politicize science to promote its own political agenda. This politicization has occurred because science is powerful. It is easier to win political arguments if science is on your side. But that benefit makes science vulnerable to interference and misuse for political purposes. President Barack Obama’s administration, for example, failed to set a science-based ozone standard and defied its own Food and Drug Administration scientists’ judgment by restricting access to emergency contraceptives.\n\nEven though we expected the Trump administration to be destructive to federal science, we had no idea that the size of the wrecking ball would be this big. While we have now documented 100 attacks on science under the Trump administration, it is difficult to know if Presidents Bush and Obama were subject to the same level of scrutiny and whether the exact same lens was applied for all administrations with different political contexts and decisions in our count, it is clear that President Trump and his political appointees have escalated their attacks on science by making them in a mere fraction of the time it took the Bush administration.\n\nAnd the worst is probably yet to come. The administration is likely to sideline science and push antiscience actions through the federal government’s bureaucratic process at an even greater rate as President Trump’s first term comes to an end in January 2021.\n\nThe Trump administration’s unprecedented record on science will harm people across the country, especially the most disenfranchised. While the sheer number of attacks on science is shocking, what a lack of science-informed policy means for our country is even more shocking. The administration’s rollback of protections from exposure to dangerous chemicals means that more people will become ill, develop chronic diseases or die from encountering these hazardous substances.\n\nThe safety of workers is at risk as the administration ignores scientific evidence showing things such as the fact that making people conduct work faster on poultry lines results in more workplace injuries. More children are likely to suffer from asthma and respiratory illnesses as the administration sidelines epidemiological evidence that air pollution has negative effects on our health.\n\nIt’s no surprise that millions of scientists and supporters have taken to the streets to push back against these attacks on science. The scientific community has stood up to the Trump administration’s antiscience actions with much success. A good example comes from an early 2018 issue when Trump administration officials blocked the Agency for Toxic Substances and Disease Registry (ATSDR) from publishing a draft toxicology report on a class of potentially hazardous chemicals commonly found in drinking water and groundwater. The chemicals, known collectively as PFAS (per- and polyfluoroalkyl substances) are linked with numerous adverse health effects, including liver damage, kidney cancer and increased cholesterol.\n\nWe obtained e-mails showing that the White House purposefully blocked the report’s release because of concern about a “potential public relations nightmare.” The report found that the current standards of risk for this class of chemicals was outdated. After the story broke, UCS and other nonprofits organized scientists and supporters to deliver 18,000 messages to their representatives to put a spotlight on the issue. Thanks to this pressure, the ATSDR released the report in June 2018. The report suggested a much more stringent standard for PFAS exposure, citing health concerns for even low levels of the chemicals.\n\nThis unprecedented milestone means there is more work to do. Congress should join with scientists and their supporters to stop the Trump administration’s antiscience actions. One way for congresspeople to do so is to place their support behind the Scientific Integrity Act, which would codify many provisions within scientific integrity policies maintained by federal agencies with science-based missions, such as the Environmental Protection Agency and the Department of the Interior. The act could prevent many of the attacks on science that we have seen from the administration to date, meaning it could stop many unforeseeable ones from this and future administrations. This safeguard will help us better protect the health and safety of people and our environment.","title":"The Trump Administration Has Attacked Science 100 Times ... and Counting","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/8506F3EA-D151-48D1-851F742AB82555DB_source.jpg?w=590&h=800&0EF67B1C-C77E-4FE9-8F9C5C4A4892F59A","link":"https://blogs.scientificamerican.com/observations/the-trump-administration-has-attacked-science-100-times-and-counting/"},{"authors":"Chelsea Harvey, E&E News","pub_date":"May 30, 2019","abstract":"In October 2016, a tiny island in the frigid waters between Russia and Alaska was the site of a morbid mystery. Dozens of dead seabirds began suddenly washing up on the shore. The bodies continued to arrive for months.\n\nIt was a jolt to the local residents of St. Paul Island, northernmost of a group of four volcanic formations known as the Pribilof Islands, clustered in the icy Bering Sea. While dead animals might occasionally wash up under normal circumstances, the daily bombardments of sodden carcasses were clearly the mark of a mass die-off. More remarkably, most of the birds were tufted puffins, a species that rarely washes up dead on the island at all.\n\nPerhaps most disturbingly of all were the birds’ emaciated bodies; they likely starved to death.\n\nShortly after the bodies began to appear, scientists started speculating that the die-off might be linked to climate change. Now, new research published this week in the journal PLOS ONE reiterates this theory—and suggests the event may have killed thousands of puffins across the Bering Sea.\n\nLed by Timothy Jones of the University of Washington, the researchers used special models to simulate the way winds and waves carry objects through the Bering Sea and deposit them on the shore of St. Paul. These simulations helped highlight the number of carcasses that were actually found washed up on the beach, versus the number that were swept back out to sea before they could be counted—and the number that never made it onto shore at all.\n\nMore than 300 carcasses were recovered on the shoreline. Using their simulations, however, the researchers estimate that anywhere from 3,150 to 8,800 birds likely died in the event. Most of them were probably puffins.\n\nAnalyses on a handful of the bodies found that toxins, often the suspected culprit in mass animal die-offs, were not to blame. Instead, it appeared most of the birds starved to death. Many of them were molting, the researchers note—an energy-intensive process that can make birds more vulnerable to stressors like food shortages.\n\nThe food stress itself is likely being driven by changes in the Arctic related to global warming, scientists say.\n\nIn their new paper, Jones and his colleagues point out that climate-driven changes in the Bering Sea, from rising water temperature to vanishing sea ice, are affecting the abundance and distribution of fish and other marine organisms. Those are important food sources for puffins and other seabirds. They suggest these changes triggered the 2016 die-off.\n\nThe Arctic as a whole is one of the most rapidly changing regions of the Earth. It’s warming about twice as fast as the rest of the planet on average. And the Bering Sea, in particular, is undergoing one of the most noticeable transformations. Over the last few years, water temperatures have been higher than usual and its sea ice cover has declined dramatically—even in winter, when the ice should be at its greatest extent.\n\nThat decline is already having a significant effect on the Bering Sea ecosystem, according to Robert Foy, director of NOAA’s Alaska Fisheries Science Center, who commented on the study for E&E News. There are fewer fish than there used to be, and many of the fish that remain are in poorer condition, with lower fat content, making them less nutritious for the bigger animals that eat them.\n\n“We have found substantial movement patterns in some of the pollock and cod stocks from the southeastern Bering Sea to the northeastern Bering Sea, where upwards of 50% of the cod stock may have shifted distribution towards the north,” he said.\n\nIn general, Foy said, it’s “very plausible” that changing conditions in the Bering Sea are to blame for the 2016 mass mortality event. And it’s probably not the only die-off linked to recent climate change in the Arctic and the northern Pacific.\n\nDuring the winter of 2014 through 2015, for instance, thousands of Cassin’s auklets washed up dead on the shores from California up to British Columbia. Scientists also believe that starvation, linked to warm ocean temperatures and a recent change in prey availability, was to blame. Between 2015 and 2018, scientists also recorded several seabird die-off events, affecting multiple species, in the Gulf of Alaska and the Chukchi and Bering seas.\n\nPuffins, in particular, have begun to garner increasing concern, and not only off the coast of Alaska. The New York Times reported in 2018 that puffin colonies in Iceland and other parts of the North Atlantic also seem to be suffering recent declines linked to food shortages, and scientists are working to determine how much of it is linked to the impact of climate change.\n\nIn the Bering Sea, the most dramatic trends have occurred within the last five years or so, meaning “the question remains what will happen if that colder water returns in future years,” Foy noted.\n\nBut in the long term, models project a continued increase in water temperatures and marine heat waves, as well as continued declines in Arctic sea ice. And there’s concern that as Arctic ecosystems continue to adjust, seabirds and other animals may become more vulnerable.\n\n“Experts around the world expect that we will see more events such as these heat wave events,” Foy noted. “And the likely effect is, as the authors [of the new study] state, the potential for these large scale die-offs.”\n\nReprinted from Climatewire with permission from E&E News. E&E provides daily coverage of essential energy and environmental news atwww.eenews.net.","title":"Mass Puffin Die-Off May Be Linked to Climate Change","origin":"Climate","image":"https://static.scientificamerican.com/sciam/cache/file/E39F14D2-8D2E-4B1E-A842D821EF32BE9E_source.jpg?w=590&h=800&B4F2371C-7645-4B8C-8F3C6F119C1A8A3F","link":"https://www.scientificamerican.com/article/mass-puffin-die-off-may-be-linked-to-climate-change/"},{"authors":"David Z. Hambrick, Alexander P. Burgoyne","pub_date":"May 29, 2019","abstract":"If you’re single and looking to change that status, perhaps you have been pondering the mysteries of romantic attraction. It will come as no surprise to learn that research has established that physical appearance is one factor that predicts attraction. This is starkly evident in online dating. A study of thirty-somethings found that being rated in the top (versus bottom) ten percent on physical attractiveness roughly tripled the probability of receiving a “first-contact” message (Hi! How are you?). Like it or not, looks matter in the dating game.  \n\nBut do not despair if you don’t have movie-star good looks. Intelligence predicts attraction, too—and not only in humans, it turns out. In a study just published in Science, a team of scientists in China and the Netherlands showed that female parakeets showed greater preference for males that they observed demonstrating problem-solving skill. This finding provides the most direct support to date for the evolutionary hypothesis, first advanced by Charles Darwin himself, that mate selection is one way that human intelligence has evolved.\n\nAt the beginning of the experiment, male and female parakeets (also known as budgerigars) were randomly assigned to either a problem-solving group or a control group. In both groups, the birds were tested in a specially designed cage in triads consisting of a “focal” female and two “demonstrator” males. The cage was divided into three compartments by transparent screens: a main compartment, where the female was placed, and side compartments, where the males were placed. The female had perches in the middle of her compartment (the “neutral zone”) and on either end (the “preference zones”). Over four days of observation, the male near which she spent more of her time was identified as the “preferred” male and the other one as the “less-preferred” male.\n\nNext, the birds were taken out of the testing cage. In the problem-solving group, over the course of a week, the less-preferred male was given training in two “puzzle box” tasks in which the goal was to access birdseed. The first puzzle box required the bird to open a petri dish; the second one required him to open the lid to the box, pull open a door, and pull out a drawer. (The preferred male received no training.) All three birds were then put back into the testing cage for an “observing phase” in which both males attempted to open the puzzle boxes in view of the female. In the control group, the focal female simply observed the less-preferred male eating out of a regular food dish. Finally, the scientists once again measured the female’s preference for the males.\n\nThe results supported Darwin’s hypothesis. In the problem-solving group, the females’ preferences shifted. Most of the females now preferred the skilled (and previously less-preferred) male, all but ignoring the unskilled (and previously preferred) male. (Intelligence was, to borrow from the psychologist Timothy Leary, the “ultimate aphrodisiac” for these lady birds.) By contrast, in the control group, the females’ preferences were unchanged. This finding indicates that it was the demonstration of problem-solving skill related to foraging, and not mere access to food, that caused females’ preferences to shift. As a follow-up, the researchers repeated the experiment with all female birds. There was no preference shift to the skilled female. Thus, the key result was specific to males and was presumably driven by mate selection.\n\nHumans are obviously more complex creatures than parakeets, but mate selection may be a factor in the evolution of human intelligence, as well. Intelligence predicts people’s ability to survive and thrive in their environments. Smart people tend to live long, healthy, productive lives. Consequently, we may have evolved to become attracted to people whose behavior signals high intelligence. For the single among us, there’s practical message here for dating: make yourself look nice, but don’t forget about the intelligent conversation.","title":"Looks Matters, but so Does Smarts","origin":"Behavior & Society","image":"https://static.scientificamerican.com/sciam/cache/file/32920834-8CBF-4170-94E746035674357C_source.jpg?w=590&h=800&F555B73F-8D9B-4618-82F678592F452041","link":"https://www.scientificamerican.com/article/looks-matters-but-so-does-smarts/"},{"authors":"Jim Daley","pub_date":"May 27, 2019","abstract":"Millipedes are hard to tell apart. Different species of the many-legged creatures often share the same dull colors and tend to blend in with the gloom of the forest floor. But under ultraviolet light, some millipedes display a striking characteristic: their genitals glow brightly.\n\nStephanie Ware, a research assistant at Chicago’s Field Museum, and her colleagues have used this strange fluorescence to help identify the leggy arthropods. Ware rigged up a camera with inexpensive UV flashlights to capture images of millipedes’ glimmering “gonopods,” specialized appendages used for copulation. The camera took multiple pictures that Ware stitched together to create a composite image. In visible-light photographs, “it’s really hard to pick out different structures” on the millipedes, she says. “But under UV, there were different patterns and colors that made them really pop out.”\n\nThis technique makes it easier to distinguish between similar-looking species, according to Petra Sierwald, a zoologist at the Field Museum She and Ware and their colleagues co-authored a study on the topic, published online in April in the Zoological Journal of the Linnean Society. Using the UV technique, the researchers identified eight species—which had previously been miscategorized as 12—within the North American genus Pseudopolydesmus. Sierwald says this kind of imaging could have applications in soil science and conservation, helping researchers quickly assess whether certain millipede species are present in a habitat. “Millipedes are very good indicators for soil health because they recycle rotting leaf litter,” she says.\n\nYet scientists still have no idea why these animals’ genitals fluoresce. “The order Polydesmida can’t even see—they don’t have eyes,” Sierwald says. M. Gabriela Lagorio, a chemist who studies photobiology at the University of Buenos Aires and was not involved in the study, says the feature may or may not have an evolutionary purpose. She notes that it may be “simply a nonfunctional consequence of the chemical structure of a substance present in the tissue.”","title":"Millipede Genitalia Glow in Ultraviolet Light","origin":"Biology","image":"https://static.scientificamerican.com/sciam/cache/file/CD8ECFB7-DB46-483A-B6C4E73D343EA14C_source.jpg?w=590&h=800&B1D783E5-71B7-4430-BCA079E109869A0C","link":"https://www.scientificamerican.com/article/millipede-genitalia-glow-in-ultraviolet-light/"},{"authors":"Ron Cowen","pub_date":"May 28, 2019","abstract":"Can science exist apart from politics? On May 29, 1919, British astronomer Arthur Eddington was doing his best to find out. Eddington had been preparing for this day throughout much of the Great War, trying to keep the prejudices of the moment separate from the pursuit of knowledge. Less than a year earlier, Eddington had finalized plans to test of a new theory of gravity proposed by a German-born scientist, even as the German army was shelling Paris and waging one of the bloodiest campaigns of World War I.\n\nThat scientist was Albert Einstein, not yet famous enough to serve as Eddington’s get-out-of-jail-free card—and staying out of jail was exactly what Eddington had been trying to do.\n\nIn 1918, the British military, desperate to replenish its ranks after more than three years of fighting, started reviewing all cases for which it had granted exemptions from the draft. Eddington’s exemption was based solely on his research as director of the Cambridge University Observatory, but on June 14, a military tribunal told him that the reason was no longer sufficient. A lifelong Quaker, Eddington tried pleading his case as a conscientious objector. The court refused to consider that argument, calling the case “a very hard one—hard against Prof. Eddington,” but gave him a deadline of July 11 to convince them otherwise.\n\nEddington had never been shy about expressing his Quaker beliefs, even while living and working on a Cambridge campus where five undergraduates and fifteen graduate students had been arrested for refusing military service. At a time when a Cambridge professor proclaimed that “the Germans are congenitally unfit to read our poetry; the very structure of their organs forbids it,” and the prestigious British journal Nature published articles decrying the inferiority of German science, Eddington publicly urged British astronomers to keep the wartime horrors separate from their work.\n\nIn particular, he championed Einstein’s radical new work on gravity. If Einstein’s theory was correct, it would replace Isaac Newton’s view of gravity as a force that acts across space with the revolutionary idea that gravity is space. Einstein asserted that space and time, instead of being stiff and unchanging, can buckle or sag due to the presence of a massive body, much the way a heavyweight sleeper sags a mattress. A marble rolls toward a heavy body not because of a force but because the body has dimpled the space-time through which the marble must travel.\n\nAs counterintuitive as Einstein’s theory seemed, Eddington knew there was a way to test the theory. Einstein himself had suggested the method.\n\nIf a body is massive enough—like the Sun—then it should be possible to observe the curved or bent path of all objects traveling in its vicinity, even particles of starlight. The bending of starlight would show up as a change in the apparent position of the star compared to its position when the Sun was in another part of the sky.  Under ordinary conditions, the blinding light of the solar disk would completely swamp the much fainter light from surrounding stars. But during those rare times and places when the clockwork motion of the solar system places the Moon directly between the Sun and Earth—a total solar eclipse--the stars pop into view.\n\nWhen the military tribunal reconvened on July 11, 1918, to decide his fate, Eddington presented a letter from Frank Dyson, Britain’s Astronomer Royal and chair of the Joint Permanent Eclipse Committee of the Royal Society and the Royal Astronomical Society. The carefully crafted note highlighted the pursuit of science but also played on the emotions of wartime Britain. Dyson asserted that the total eclipse of the sun in May 1919, visible across Africa and Brazil, was of exceptional importance and that Eddington was the best person to lead a British expedition to observe it—an experiment, Dyson added, that would counteract “a widely spread but erroneous notion that the most important scientific researches are carried out in Germany.”\n\nThe letter did the trick. The local tribunal said it was convinced that Eddington was a true conscientious objector and that his work was of vast importance “not only to this country but to the world—to knowledge generally.” And so it was that during some of the fiercest fighting of the war, Eddington got the official go-ahead to test Einstein’s theory.\n\nThe 1919 event had several things going for it. The eclipse would last more than six minutes, one of the longest in the twentieth century. What’s more, the sun would be sitting against a rich background of stars, the Hyades cluster, providing a bounty of objects with which to test Einstein’s light-bending prediction. Another plus: these stars were relatively bright.\n\nBy the time Eddington and his colleagues set sail to observe the eclipse, the war was over. Eddington and a collaborator journeyed to Principe, a Portuguese-owned island off the west coast of Africa, while a second British team traveled to Sobral, in northern Brazil. At Principe, clouds nearly obscured the observations. At Sobral, the lens of the main telescope had become distorted, apparently due to the sun’s heat, producing photographic plates that were out of focus.\n\nYet when Eddington and his colleagues went back to England and compared the positions of the stars they were able to image during the eclipse with the positions when the sun was elsewhere in the sky, they found the amount of light bending that Einstein had predicted (more recent assertions that Eddington had fudged the data in favor of Einstein have been shown to be incorrect).\n\nThe day after Eddington announced the results at a meeting of Royal Society in London on November 6, 1919, the front page of the Times of London was full of stories about war and remembrance. It was only a few days before the first anniversary of the armistice, and King George V had just issued an invitation for all workers to take two minutes of silence out of their day to remember and honor “the glorious dead.” But to the right of these stories appeared an article about rebirth and renaissance. In a triple-decker headline, the normally staid Times wrote: “Revolution in Science / New Theory of the Universe / Newtonian Ideas Overthrown.” \n\nThe news set off a chain reaction around the globe. The New York Times followed suit with a front-page story on November 10: “Lights All Askew in the Heavens . . . Einstein Theory Triumphs.” \n\nA century later, Einstein’s theory of gravitation continues to open new and unexpected windows on the birth and life of the cosmos. The discovery of gravitational waves—invisible undulations in space-time that provide a new way of learning about exploding stars and some of the most violent collisions in the universe—and the existence of black holes and their shadows, one of which was recently imaged for the first time--were both predicted by Einstein’s theory.\n\nInternational collaborations are now commonplace and an expedition like Eddington’s would almost certainly include scientists from a multitude of countries. But in today’s polarized climate, the question may be important than ever: Can science remain separate from politics?\n\nAll scientists can do is keep on trying.\n\nThis essay is adapted from the author’s new book Gravity's Century: From Einstein's Eclipse to Images of Black Holes, published by Harvard University Press.","title":"The 1919 Solar Eclipse and General Relativity's First Major Triumph","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/7D73CC70-B23C-4D06-A90BA54AA1F6F30E_source.jpg?w=590&h=800&DAA32367-758B-487B-A211AB0A6EA8D55A","link":"https://blogs.scientificamerican.com/observations/the-1919-solar-eclipse-and-general-relativitys-first-major-triumph/"},{"authors":"Shirin Panahi","pub_date":"May 27, 2019","abstract":"\"Eat less and move more.\" Oh, such simple advice, but is maintaining a healthy weight really that simple? We live in an era of nutritional misinformation and opinions galore. These days, it seems that everyone feels qualified to offer expert advice on diet, exercise and weight loss. With rising obesity rates all around the world, we are constantly searching for approaches to better manage our weight and our health.\n\nFor decades, the main strategy for losing weight has been to cut back on calories; what nutritionists call an “energy-restricted diet.” Although this often works in the short term, it rarely produces long-term success. It backfires because it can lead to greater feelings of hunger after the weight is lost, more obsessive thoughts about food and eating, and a greater risk of overeating due to negative emotions and stress. These complicate the bodily mechanisms that control appetite and partly explain why most people regain the weight in the long term.\n\nOther types of restrictive diets, such as the popular high-fat, no-carbohydrate ketogenic regimen, have some of the same problems. Like low-calorie diets, they are difficult to follow over a long period of time, which can lead to feelings of frustration and failure. The challenge for researchers has been to find a strategy that is not restrictive and that can reduce feelings of hunger and improve eating habits and overall health without causing some of these negative side effects.\n\nThe answer, it turns out, may be a diet constructed from healthy foods that are especially satiating; that is, foods that create feelings of fullness and satisfaction. Nutrition researchers have discovered many such foods, which improve appetite control and decrease food intake, conditions necessary for sustained weight loss. A satiating diet includes foods that are high in protein (such as fish),; high in fiber (whole grains, for example) and high in fruits and vegetables. It contains healthy fats, such as the polyunsaturated fats found in avocados, and includes dairy products such as yogurt. Perhaps surprisingly, it might also include capsaicin, the substance that makes jalapenos and other peppers so hot.\n\nWhat’s so special about these foods it’s that each of them possesses specific characteristics that benefit our health either by decreasing hunger, reducing body fat, lowering blood sugar, improving blood pressure or increasing metabolism. For instance, yogurt contains protein, calcium and lactic acid bacteria, which are the live and active bacteria that help with the growth of good bacteria in the gut. A healthy gut microbiome has been found to help control body weight and improve other aspects of health. In some specific populations, such as people with obesity or type 2 diabetes, these food components have been found to help control appetite and positively impact overall health.\n\nBut what if we took all of these key components and combined them into one diet as an approach to manage weight? That’s exactly what our team did at Université Laval in Quebec City, Canada. As a nutritional researcher there, I have spent years trying to understand how what we eat affects appetite, body weight and metabolic health over the lifespan. We took foods containing most of these components and created what we called a \"highly satiating diet.\"\n\nIn a 2017 study, 34 obese men were placed on this regimen, which was 20–25 percent protein, for 16 weeks. Another 35 obese men followed a standard diet: 10–15 percent protein, and based on Canadian national guidelines for healthy eating. The men who followed the highly satiating diet significantly reduced their weight and body fat and had greater feelings of fullness compared to men who followed the standard diet. They were also better able to stick to the highly satiating diet: only 8.6 percent quit the diet, compared to 44.1 percent of the men following the standard diet.\n\nThese are very promising findings, but is it possible to maintain the weight lost over the long term with this strategy? What about metabolic and mental health; can it prevent cravings and negative emotions? What about the role of other lifestyle factors on body weight such as sleep, physical activity and prolonged sitting? We don`t have the answers yet, but we are planning further studies that we hope will address these questions. If the highly satiating diet proves to have the benefits we saw in our study and if it proves to be sustainable, it could be a realistic and potentially powerful dietary solution to the problem of weight control.","title":"Have We Found a Diet That Truly Works?","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/248563CA-E911-4BF2-844D1B8B2E1F54BB_source.jpg?w=590&h=800&8DF53AAF-E77F-4FBA-920D3A5D6B1EEB76","link":"https://blogs.scientificamerican.com/observations/have-we-found-a-diet-that-truly-works/"},{"authors":"Halley E. Froehlich","pub_date":"May 27, 2019","abstract":"Demand for seafood is increasing across the globe, and the United States is no exception. Aquaculture, or aquatic farming, is increasingly meeting this demand and now supplies just over 50 percent of all seafood globally. In fact, it has been one of the world’s fastest growing food sectors for years.\n\nThe U.S. is the largest importer of seafood in the world, and some of Americans’ favorites—including shrimp, salmon and tilapia—are predominantly farmed these days. Yet, we contribute less than 1 percent of the world’s total aquaculture production. This means we rely heavily on other countries to satisfy our appetites for seafood.\n\nIf the U.S. does not increase its domestic production of farmed shellfish, seaweed and finfish, the divergence between what we consume and what we contribute to the global seafood market will continue to widen. This gap may make it harder for our seafood diets to be sustainable. It also means the U.S. won’t have a hand in shaping the standards or economies that contribute to the seafood sector as a whole in the future.\n\nA brand-new bill that proposes a moratorium on commercial permits for marine finfish aquaculture facilities in U.S. waters could serve to widen this gap, and it represents another divergence: between public wariness about domestic aquaculture operations and the science showing aquaculture’s potential for sustainable growth.\n\nWhile wild-caught fisheries have hit “peak fish” domestically and globally, with limited potential for additional sustainable growth, there is mounting scientific evidence that the U.S. could dramatically increase domestic aquaculture production and do so sustainably, as we did with our fisheries before they peaked. And this growth does not have to come at the cost of our wild-caught fisheries or other priorities for our oceans, especially under careful management and planning.\n\n\n\tTotal United States seafood consumption and production (fisheries and aquaculture) over time. Data from FAOSTAT. Credit: Halley E. Froehlich.\n\n\nThe oceans, including around the U.S., have a lot of space to put sustainable aquaculture operations, and the amount of space required to farm a lot of seafood is miniscule compared to land-based farming. In fact, farming aquatic species instead of livestock could spare a lot of land because we wouldn’t have to grow as much feed, even as crops replace more limited aquatic feed resources, like sardines and anchovies.\n\nSome aquaculture species, like oysters and seaweed, don’t even require farmers to feed them, and they can actually improve environmental conditions by filtering nutrients and mitigating some climate change impacts.\n\nLike all food production systems, aquaculture will have an impact, with challenges around minimizing disease, contaminants, pollution, escapes, and disturbing wild species. The alternative to open ocean farms—closed, land-based farms—are part of the solution, but have their own trade-offs, like limited places to put them, greater water demands and greater green-house gas emissions.\n\nImportantly, not all aquaculture is created equal. The U.S. has the opportunity to employ clear and strong regulatory standards that also consider our wild fisheries and marine ecosystems.\n\nThe perceptions of some policymakers and the public could determine the rejection or adoption of aquaculture in the country. Social research shows Americans are willing to eat farmed seafood but are concerned about the development of aquatic farms in their “backyards.” Fears about escapes, like one in Washington State that resulted in a statewide ban on Atlantic salmon, and worries based on a history of poor farming practices and pathogens, are not baseless. People are fallible, but good management and monitoring can reduce negative impacts.\n\nLegislators at federal and state levels are considering and constructing policies that could facilitate some types of domestic aquaculture production. In Alaska and California, for example, marine shellfish and seaweed aquaculture are taking center stage. There are continued attempts to create a clear federal law governing marine aquaculture nationally, but there is still a long way to go.\n\nIf the U.S. wants a future with more local and sustainable seafood production, part of the solution is better communication of what sustainable aquaculture looks like and who are the people working in that space, including chefs, scientists, conservationists and farmers. This can help raise awareness of the realities of seafood production and demand, and of the potential for sustainable aquaculture. Already, there is evidence that improved communication works.\n\nAt the policy level, we need more evidence-informed conversations and policies around the future of domestic aquaculture that address important philosophical questions—namely, where do we want our food to come from and what, if any, role should we play in its production? Can aquaculture have its own Magnuson-Stevens Act, which changed the game for the sustainability of our wild fisheries?\n\nThe reality is Americans, and the growing global population, won’t stop eating seafood any time soon, and our reliance on it may continue to grow if we are going to feed the world sustainably. Aquaculture is part of our diets now and will be into the future.\n\nThe narrative needs to move away from “wild versus farmed” and toward “wild and farmed.” Let’s have serious, informed conversations about the state of seafood and how the U.S. can both contribute to and benefit from sustainable Blue Growth.","title":"It's Time to Be Honest about Seafood","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/B0008766-9FC4-4EA1-97134AA99356B2E8_source.jpg?w=590&h=800&757D10C5-F3CE-486A-A7A228C6F248359A","link":"https://blogs.scientificamerican.com/observations/its-time-to-be-honest-about-seafood/"},{"authors":"Harini Barath","pub_date":"May 28, 2019","abstract":"Scientists have successfully tested a heartbeat-powered pacemaker in living pigs, whose hearts are similar to humans’ in size and function. Researchers say this is an important step toward developing battery-free implantable medical devices. Current pacemaker batteries have a life span of seven to 10 years, and replacing them entails expensive surgery. \n\nThe new “symbiotic pacemaker” consists of three components: a wafer-sized generator attached to the heart that converts the organ’s mechanical energy into electrical energy; a power-management unit that has a capacitor to store that energy; and the pacemaker itself, which stimulates and regulates the heart muscle. \n\nZhou Li of the Beijing Institute of Nanoenergy and Nanosystems and Zhong Lin Wang of the Georgia Institute of Technology and their colleagues implanted their device in two adult male pigs. In the first animal (which had a healthy heart), the team tested how well the generator harvested energy; it powered the pacemaker for a total of nearly three and a half hours. The pig’s heart generated more than enough energy to power a human version of the pacemaker, the scientists reported in April in Nature Communications. In the second pig, they induced an irregular heartbeat (arrhythmia) to test the pacemaker’s therapeutic function. When the device—which had been charged by the pig’s heart for more than an hour—was turned on, the animal’s heartbeats promptly became regular and remained so even after it was turned off. \n\nHuman testing is unlikely in the near future because the device’s size, safety and efficiency must still be optimized. “The technology described is a significant achievement,” says Patrick Wolf, a biomedical engineer at Duke University, who was not involved in the study. But he cautions that the size and efficiency hurdles are significant, and the pacemaker’s effectiveness in a less dynamic, diseased heart is yet to be determined. \n\nAnother drawback is that the unit must be attached directly to the heart’s surface and could interfere with the organ’s functions. A group at Dartmouth College and the University of Texas at San Antonio previously designed a pacemaker that instead harnesses kinetic energy from its own lead wire, which moves when the heart pulses. The team is currently testing it in dogs. \n\n“The development of these battery-free technologies will revolutionize implantable devices,” says Ramses Martinez, a researcher in industrial and biomedical engineering at Purdue University, who was not involved in either study. “Soon traditional rigid implants will evolve into conformable systems capable of harvesting the energy they need to function from the patient.”","title":"New Pacemaker Harvests Energy from the Heart","origin":"Medical & Biotech","image":"https://static.scientificamerican.com/sciam/cache/file/6E3A517B-3E94-4DF9-905429A42658118E_source.jpg?w=590&h=800&2A204C5D-F364-49A6-91043E5E0725DD49","link":"https://www.scientificamerican.com/article/new-pacemaker-harvests-energy-from-the-heart/"},{"authors":"Tamily Weissman","pub_date":"May 28, 2019","abstract":"Peering into a tiny living animal, I watch the motion of its beating heart and see hundreds of individual blood cells march through its arteries. The skin of the zebrafish is transparent for its first few days of life, beginning with its development in a clear egg, so the egg can be placed under a microscope, and the growth of the entire nervous system can be viewed in real time. I can see directly into the growing brain, which, amazingly, only takes about 40 hours to fully develop. This observation is not possible in any other animal, and it makes the zebrafish a powerful tool for studying development and disease. \n\nScientists use these unique critters to probe a plethora of previously unanswered questions. In my research laboratory at Lewis & Clark College, for example, we watch the very cells that are affected by the mosquito-borne Zika virus, which tragically causes causes microcephaly—in which the head and, typically, brain are significantly smaller than normal— in newborn babies. The stem cells in an embryo’s nervous system are busy dividing over and over again to produce all of the neurons needed for the growing organism. If those cells are infected by the virus, however, they don’t make enough neurons, and the brain will remain underdeveloped. \n\nWe use a tool called Brainbow to label the stem cells with different colors and watch them divide inside the living zebrafish. Beyond generating beautiful images that have been featured previously by Scientific American, Scientific American Mind and other outlets, the multicolor Brainbow technique illuminates many dividing cells all at once. Now able to clearly track cells nestled deep inside the brain, we can pursue specific questions about how and when the cells divide and die. \n\n\n\tCells dividing in a zebrafish's brain, illuminated with the \"Brainbow\" technique. Credit: Image by Zachary Tobias in the lab of Tamily Weissman.\n\n\nThe zebrafish, native to South Asia and first harnessed for modern scientific use in the 1970s, is now also helping us to understand the brain’s complex response to the visual world. For the first time, scientists are able to observe and record the firing of every single neuron in an entire brain, allowing us to watch how a whole circuit of nerve cells responds to stimuli. Fish are placed in a recording chamber, where they view different shapes that mimic approaching predators or other objects. As they respond to each stimulus, a computer tracks the electrical activity in their brain. \n\nPrecisely how the brain generates behaviors like these is still largely a mystery. From work in zebrarish, we are beginning to piece together a functional map that was impossible to see before using this type of holistic imaging technique. That map, in turn, informs us about how the neural circuits in our own brain may be organized and how we generate our own behaviors. \n\nSurprisingly, zebrafish research can also help us to understand human deafness. Along the outer edge of the fish, there is a specialized organ used for detecting movements in the water. This sensory system, called the lateral line, uses delicate “hair cells” that are very similar to the ones found in our own ears. In humans, these cells sense pressure waves in the air, sending signals to our brain that represent sound. \n\nWe know that those cells can become damaged over time, leading to age-related and other kings of hearing loss. In fish, these same hair cells are activated when nearby objects cause movements in the water. Researchers can model hair-cell loss in fish by damaging them with a particular drug and then testing their ability to regenerate. Using zebrafish, researchers have learned about the specialized stem cells and genes that are required for this critical process—insights that may pay off some day in efforts to restore hearing in humans. \n\nThe fish are also a powerful model for studying numerous genetic diseases. The precise mutations that arise in humans can be mimicked in a zebrafish, while the affected cells are followed in real time under the microscope. Drug treatments that may eventually cure human diseases such as cancer are then added to the fish’s water to test how the cells respond inside the animal. \n\nThough small, the transparent zebrafish is making important contributions to our understanding of human biology. Beyond its direct impact on medicine, the fish inspires young people across the globe. The model organism is used in elementary schools and college classrooms to demonstrate the wonder of embryonic development. In my own lab, I have witnessed a powerful impact on students when they look directly into the zebrafish brain for the first time. It opens a window of curiosity that can inspire undergraduate scientists, who will ultimately develop new approaches and become the next generation of cutting-edge researchers","title":"How a See-Through Fish Is Helping Answer Big Questions about Biology","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/6F27BBF6-EB3D-4A42-9424613EE9D7C0F7_source.jpg?w=590&h=800&6BA5FF08-DFC3-4BDC-8C4E31E907EA84B8","link":"https://blogs.scientificamerican.com/observations/how-a-see-through-fish-is-helping-answer-big-questions-about-biology/"},{"authors":"Michael Painter, David Wilkie, James Watson","pub_date":"May 23, 2019","abstract":"Increasing household income and a human population expected to grow from seven to nine billion people in the next 20 years have driven up global demand for food, fiber, fuel and other natural resources. Rapid urbanization means that more people are consuming in a way that puts untenable pressure on nature.\n\nOne way to conserve viable populations of many animal and plant species is to create areas that are off-limits to industrial-scale development. But reducing human pressures does not mean that an area must be entirely free of people. In fact, evidence shows that many of nature’s remaining strongholds have maintained high natural values precisely because of the stewardship of local people over generations.\n\nA recent article in PLoS Biology found that unsustainable extraction of natural resources and large-scale habitat destruction threatens terrestrial species across 84 percent of the earth’s surface. Of the 5,457 terrestrial vertebrates included in the study, about a quarter face impacts across more than 90 percent of their range and may become locally extinct in many areas. Even where populations persist, their numbers are often so small that it is unlikely they will remain viable in the long term. Continuing species loss is both an indicator and a driver of the degradation of natural ecosystems upon which humans depend.\n\nThe PLoS Biology study complements an article published last year in Science by many of the same authors, which found that human pressures are undermining the ability of many of the world’s protected areas to achieve the conservation goals for which they were created. Nearly a third of the world’s six million square kilometers of protected areas are under intense human pressure, the researchers reported.\n\nThese two assessments highlight a threat to the integrity of our natural ecosystems and the critical functions they provide. While protected areas play a necessary role in conserving biodiversity and intact ecosystems, they are an insufficient response to the challenges we face if not strengthened and supplemented by other conservation actions.\n\nWe must proactively plan our use of the earth’s remaining natural areas to reflect their significance for people living there and the quality of life on this planet more generally. This is especially true in the case of indigenous lands. Globally, the lands of indigenous peoples are twice as likely as non-indigenous lands to have high natural values. Indeed, about 37 of the world’s terrestrial surface where conservation values persist have indigenous populations.\n\nMoreover, species loss and ecosystem degradation are often outcomes of indigenous peoples having been dispossessed of their lands and resources and deprived of the opportunity to effectively govern the areas that remain under their control.\n\nSupporting the efforts of indigenous peoples to secure and exercise their legitimate rights over areas of high natural value represents a cost-effective means of reducing and redirecting human pressures while redressing injustices associated with the misappropriation of indigenous lands and resources. Lands managed by indigenous peoples tend to be especially effective at avoiding deforestation.\n\nAs we look to a post-2020 global framework for preserving biodiversity, effectively managed protected areas cannot alone achieve our global conservation goals. Even if we reach the ambitious target of placing 30 percent of land and oceans under protection, human pressure on nature will continue across the remaining 70 percent. Investing in protected areas will be effective only if we simultaneously support the sustainable management of adjacent areas with high natural values.\n\nIn any effort to conserve and effectively manage nature’s strongholds, we need to think about supporting the legitimate rights of indigenous peoples and traditional communities to govern access to and use of their territorial resources. By ensuring that local communities continue to have the incentives and authority to be effective stewards, we can likewise ensure that our consumption remains sustainable long into the future.","title":"Indigenous Peoples Have a Critical Role in Conserving Nature","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/24F60EFA-6D8E-494A-B2A6ACC6BF66D5B9_source.jpg?w=590&h=800&7F25F84C-B4F3-4B87-BE5946B059546B76","link":"https://blogs.scientificamerican.com/observations/indigenous-peoples-have-a-critical-role-in-conserving-nature/"},{"authors":"Lee McIntyre","pub_date":"May 22, 2019","abstract":"Two years ago at the March for Science in Boston, I saw signs that said \"Keep Calm and Think Critically,\" \"Extremely Mad Scientist,\" \"No Science, No Twitter\" and \"It's So Severe, The Nerds Are Here.\" If anything, since then things have gotten worse.\n\nVirtually every day we find an assault on science in the pages of our newspapers and on TV, the radio and the Internet. Seven hundred measles cases in 22 states due to anti-vaxxers. Climate change legislation stalled in Washington, D.C., due to willful ignorance over the difference between climate and weather. Even the flat-earth movement is on the rise.\n\nThe attacks target not only the findings of science, but the very process by which well-corroborated scientific theories are produced. Perhaps few if any of the lay public ever really understood how science works, but in decades past they at least trusted scientists as experts. In today's hyperpoliticized, do-your-own-research-on-Google era, a good deal of that trust has vanished.\n\nIn a recent survey cited in Scientific American, only 35 percent of respondents had \"a lot\" of trust in scientists, while the number of people who \"do not at all\" trust scientists increased by over 50 percent from 2013 to 2017.\n\nThe question of what accounts for the special epistemic authority of science is no longer purely academic. If we cannot do a better job of defending science—of saying how it works and why its findings have a privileged claim to believability—we can only expect things to get worse.\n\nIn my new book The Scientific Attitude: Defending Science from Denial, Fraud, and Pseudoscience (MIT Press, 2019), I propose that the best way to defend science is to reconceive what is most special about it. This entails abandoning several longstanding myths about science that have given aid and comfort to deniers.\n\nIn brief, my claim is that what is most distinctive about science is not any \"scientific method\" or \"logic of science\" that philosophers and others have tried to use as a criterion of demarcation between science and nonscience. Instead, the key feature is the \"scientific attitude,\" which consists of two theses: (1) scientists care about evidence and (2) they are willing to change their beliefs based on new evidence. Rather than any method or logic, the scientific attitude is an essential value of science. It is embodied in the scientific community’s practice of testing and checking one another's work, without which science could not move forward.\n\nAt this point, some of you who regularly read these pages might wonder, \"Why do philosophers always think we need help from them?\" Perhaps the most diplomatic answer is that this post-truth era is not the time to refuse help from your allies. A more provocative answer is to turn the question around: How have things been going in your own defense of science to a lay public who do not really understand what scientists do?\n\nThe typical way that scientists defend science is to present their evidence and then, if it is not rationally evaluated or their integrity is questioned, walk away. Perhaps it is understandable that people who have devoted their lives to rigorously testing empirical theories would become exasperated with faux-skepticism or conspiracy theories and would dismiss these ideas as irrational.\n\nIndeed they are irrational. But at a time when even the most patent nonsense gets amplified on the Internet—sometimes reaching all the way to Congress and the White House—I implore scientists not to walk away (nor merely to protest), but to engage in more public education not just about the results of science but about how science works. Remember that every lie has an audience. If we do not fight back, we lose the chance not just to try to convince the liar, which may be impossible, but more importantly to provide a counter-argument to the liar's audience.\n\nHow, then, to proceed? You cannot convince someone who does not believe in evidence by showing them more evidence. You succeed, if at all, by showing them that their reasoning is faulty. And one of the most important things to do here is to dispel the myth that science, like its cousins math and logic, can produce \"proofs\"—or that the point of scientific inquiry is to reach certainty.\n\nUnfortunately, when backed into a corner by some know-nothing who claims that climate change \"isn't settled science\" or that evolution \"is just a theory,\" scientists themselves have given oxygen to these myths. The desire to blurt out \"But it's been proven!\" can be almost irresistible.\n\nWhy do scientists sometimes do this? Because they know that if they admit any uncertainty it will be exploited by science deniers to claim that \"until the rest of the evidence comes in\" their own theory is just as likely to be true. Of course, this is a ridiculous standard. Just because certainty cannot be reached does not mean that likelihood and probability go out the window.\n\nWhen certainty is the standard, science deniers may feel justified in calling themselves skeptics and retreating into their weird silos of confirmation bias and disinformation, where any slim possibility can seem like victory. So let's show them that this is an unreasonable standard. Let's show them, at least, that this is not the way scientists reason based on evidence.\n\nI wish scientists would stop being so embarrassed by uncertainty and instead embrace it as a strength rather than a weakness of scientific reasoning. The scientific attitude allows us to be open to new ideas, even while we must remain skeptical of them until they have been thoroughly tested. That is a model of true skepticism.\n\nOne virtue of embracing the scientific attitude is that it would allow scientists to say what in their hearts they know to be true: that science cannot, technically speaking, ever prove a theory true—even that gravity exists or that electrons are real. That is just not how inductive reasoning works. In studying the empirical world we must always hold out the possibility that some future evidence may come along to refute even the best-corroborated theories. Just as Newton’s theories gave way to Einstein's, every scientific theory is fallible.\n\nBut in the next breath, we must shut down the false idea that uncertainty allows any wild-eyed theory to be believable. The strength of scientific explanation is based on warrant—on justification given the evidence—and although this may not be a matter of proof, it is a matter of (overwhelming) likelihood and probability. While it is true that, until it has been disproven, any theory may be correct, this does not mean it is rational to believe a theory with insufficient evidence.\n\nAnother way to proceed might be to pick a case of uncertainty and talk about that. For instance, it was recently reported that the evidence for anthropocentric climate change has now reached the \"five sigma\" level. This is the gold standard of corroboration, indicating there is now only a one-in-a-million chance that the climate change deniers are right. Perhaps this might feel like enough for them. But should it?\n\nIn the Jim Carrey movie Dumb and Dumber there is a scene where the protagonist is trying everything he can to get a woman to go out with him. No matter how many different ways he asks, she just keeps rejecting him. Finally, in exasperation, he asks her to assess the probability that she would ever date him, to which she replies, \"One in a million.\" This film being the comedic gem that it is, he grins and says, \"So you're tellin' me there's a chance.\"\n\nInstead of supporting myths about certainty, let's make the science deniers own that.","title":"How to Reverse the Assault on Science","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/B1D21034-62A8-45B2-AD986CBB4E14720C_source.jpg?w=590&h=800&43E5857A-B980-42AE-B7F6EE257069BA9B","link":"https://blogs.scientificamerican.com/observations/how-to-reverse-the-assault-on-science1/"},{"authors":"Nutrition Diva Monica Reinagel","pub_date":"May 24, 2019","abstract":"The FDA recently announced that it plans to increase its oversight of the multi-billion dollar supplement industry. This would include everything from the calcium and multivitamins at your local drug store, to those questionable weight loss and virility supplements pitched on late night cable TV stations.\n\nAccording to the FDA, “Three out of every four American consumers take a dietary supplement on a regular basis. For older Americans, the rate rises to four in five. And one in three children take supplements.”\n\nAll of these are currently regulated under guidelines known as DSHEA—the Dietary Supplement Health and Education Act. According to the regulations, manufacturers are responsible for ensuring that their products are safe and correctly labeled.\n\nHowever, unlike drug makers, supplement manufacturers do not have to submit proof of safety or efficacy before bringing their product to market. It’s sort of an honor system. If you get caught doing something wrong, you’ll be punished. But for the most part, there’s an assumption that people are following the rules.\n\nIn the 25 years since these regulations were enacted, the supplement industry has grown ten-fold—from about 4,000 products in 1994 to 50,000 different products now.  With this explosive growth has come an increasing number of what FDA commissioner Scott Gottlieb calls “bad actors,” aka companies that are either intentionally or accidentally breaking the rules. As a result, there’s a greater chance that consumers will be exposed to products that have undeclared or even illegal ingredients or contaminants.There’s also a greater chance that products may include unapproved or inaccurate health claims.\n\nIn response, Gottlieb intends to step up enforcement of the regulations. Hopefully, this will result in fewer people going to the emergency room due to adverse effects from dietary supplements. (In 2015, there were 23,000 such visits).\n\nBut, to tell you the truth, even when manufacturers are strictly obeying the rules, there’s still a lot of potential for consumers to be wasting their money on supplements that simply aren’t doing anything for them.\n\n\n\n»Continue reading “Are Nutritional Supplements a Waste of Money?” on QuickAndDirtyTips.com","title":"Are Nutritional Supplements a Waste of Money?","origin":"Medicine","image":"https://static.scientificamerican.com/sciam/cache/file/1957E3C8-5CC5-423B-84DE249D1D829521_source.jpg?w=590&h=800&A77F596C-705E-4BD5-8BF1ED7F825C6BC4","link":"https://www.scientificamerican.com/article/are-nutritional-supplements-a-waste-of-money/"},{"authors":"Clara Moskowitz","pub_date":"May 23, 2019","abstract":"We could have been living in an antimatter universe, but we are not. Antimatter is matter’s upside-down twin—every matter particle has a matching antimatter version with the opposite charge. Physicists think the cosmos started out with just as much antimatter as matter, but most of the former got wiped out. Now they may be one step closer to knowing why. \n\nResearchers at the Large Hadron Collider Beauty (LHCb) experiment at CERN near Geneva have discovered antimatter and matter versions of “charm” quarks—one of six types, or flavors, of a class of elementary matter particles—acting differently from one another. In a new study, which was presented in March at the “Rencontres de Moriond” particle physics conference in La Thuile, Italy, the physicists found that unstable particles called D0 mesons (which contain charm quarks) decayed into more stable particles at a slightly different rate than their antimatter counterparts. Such differences could help explain how an asymmetry arose between matter and antimatter after the big bang, resulting in a universe composed mostly of matter. \n\nMatter and antimatter annihilate each other on contact, and researchers believe such collisions destroyed almost all of the antimatter (and a large chunk of the matter) that initially existed in the cosmos. But they do not understand why a relatively small excess of matter survived to become the stars and planets and the rest of the cosmos. Consequently, physicists have been looking for a kind of matter that behaves so differently from its antimatter version that it would have had time to generate this excess in the early universe. \n\nThe newly discovered mismatch in decay rates between charm quarks and antiquarks turns out to be too small to account for the universe’s excess of matter. The result, however, “does bring us closer to finding the answer because it shows one of the possible answers may not be the right one,” says theoretical physicist Yuval Grossman of Cornell University, who was not involved in the new work. “I am also excited because it’s the first time we’ve ever seen this [phenomenon in charm quarks].” \n\nPhysicists previously found similar variations in two other quark flavors, but those were also too tiny to account for our matter-dominated universe. Scientists are holding out hope of finding much larger matter-antimatter differences elsewhere, such as in ghostly particles called neutrinos or reactions involving the Higgs boson—the particle that gives others mass—says LHCb team member Sheldon Stone of Syracuse University: “There are lots of different searches going on.”","title":"What Happened to All of the Universe’s Antimatter?","origin":"Physics","image":"https://static.scientificamerican.com/sciam/cache/file/B6C7C111-134B-411A-ABA8F2A5774F58D6_source.jpg?w=590&h=800&5AC94598-A8D4-4D47-B9382E346C0AC550","link":"https://www.scientificamerican.com/article/what-happened-to-all-of-the-universes-antimatter1/"},{"authors":"Daniel B. Poneman","pub_date":"May 24, 2019","abstract":"Sixty-five years ago, President Eisenhower took the first concrete steps toward implementing his “Atoms for Peace” initiative, presenting Soviet leaders with a detailed outline of the safety and nonproliferation rules that should guide the peaceful development of civilian nuclear energy.\n\nThree more years of determined U.S.-led diplomacy culminated in the establishment of the International Atomic Energy Agency, which continues to be pivotal in maintaining, monitoring and enforcing global nonproliferation safeguards—so that, in Ike’s words, “this greatest of destructive forces can be developed into a great boon, for the benefit of all mankind.”\n\nThe existential threat of nuclear annihilation did not go away when the Cold War ended, and now we face a second existential threat from climate change. In the face of these twin threats, American nuclear leadership is as critical in 2019 as it was in 1954. \n\nNuclear energy is the largest source of carbon-free energy in the U.S. by a huge margin and it has a major role to play in confronting the global climate challenge. But we must also be vigilant about the prospect of nuclear weapons falling into the hands of terrorists or rogue regimes. \n\nThe threat of nuclear proliferation abroad should not lead us to abandon nuclear energy at home. Indeed, American nuclear leadership has always been critical to guiding the safe, responsible use of civilian nuclear energy around the world. \n\nFor example, a number of American companies are developing advanced generation-reactor technologies that offer a host of safety and nonproliferation advantages. These advanced designs would have “walk away” safety, meaning they do not need any backup power or external cooling systems in the event of an accident. And since many of the new reactor designs would rarely if ever need to be refueled, the risk of diversion of fuel from uranium-enrichment or plutonium-reprocessing plants to a bomb program would be greatly diminished. \n\nThe U.S. should lead the way in the development of these reactors so they can be deployed at home and abroad over the next decade. As a growing number of countries around the world turn to nuclear power as a source of carbon-free electricity, it is strongly in our interest that they do so with safe, American-made technology. Countries that adopt the new U.S. reactor designs will also be subject to U.S. nonproliferation requirements, which are second to none. \n\nWe must also confront the challenge posed by countries like North Korea, which has nuclear weapons, and Iran, which has sought to develop them. There is no substitute for tough diplomacy, backed by a unified international community willing to exercise its leverage—through sanctions or ultimately military means, if necessary—to persuade these nations to give up their weapons in a transparent and verifiable way. Here again, America’s technical expertise in building, operating and fueling reactors informs and strengthens our ability to design enforceable nonproliferation agreements and effective verification measures to detect and respond to violations. \n\nAmerican leadership in nuclear technologies is equally important when it comes to the climate challenge. It has been three years since the Paris Climate Agreement and the world is already falling far short of its collective commitments to reduce carbon emissions. Even if all nations achieved 100 percent of the reductions they pledged in Paris, the world would not come anywhere near the goal of limiting temperature rise to 2 degrees Celsius over preindustrial levels, much less the 1.5-degree target that scientists say we must achieve if we are, for example, to save the earth’s coral reefs. Projected increases in renewable power and plans to invest in carbon-capture technologies, efficiency measures, reforestation and other steps are important but will not get us there.\n\nThat is why the International Energy Agency has concluded that meeting the goal of 2 degrees C will require doubling nuclear power’s contribution to global energy consumption by mid-century. Late last year the Intergovernmental Panel on Climate Change reached a similar conclusion: in most scenarios consistent with the target of 1.5 degrees C, nuclear energy would have to more than double.\n\nIn the end, we do not need statistical models to know that nuclear must be on the table. Common sense tells us that if we are facing an existential crisis, every available zero-carbon technology must be called into service. Scaling the mountain in front of us is daunting enough without tying one hand behind our back. \n\nThe 98 reactors in our nuclear fleet are the workhorse of the clean-energy sector. They provide one fifth of our electricity. Unfortunately, over the past few years six reactors have been prematurely shut down, and another 12 are set to close in the next seven years.\n\nThe problem is that the rules governing wholesale electricity markets do not allow the unique advantages of nuclear power to be reflected in the wholesale price, effectively putting new and existing nuclear plants at a disadvantage. These rules were written decades ago to deliver some things we want (low prices and excess capacity to meet spikes in demand) but not other things we want (clean air, lower carbon emissions and grid reliability). \n\nNuclear plants are not only emissions-free and carbon-free, they are by far the most reliable assets in our power generation mix, operating 93 percent of the time—even during extreme weather events when some fossil fuel plants may be forced to shut down or curtail their operations. Under current rules, electricity markets are not allowed to value these attributes, even though they are clearly valuable. \n\nRepublicans and Democrats in states like Illinois, New York and New Jersey have taken action to establish “zero-emission credits” so the markets better reflect the value of carbon-free energy like nuclear and renewables. But state solutions are an imperfect substitute for what should be federal, nationwide action to reform these markets. \n\nPreserving existing reactors may not sound exciting, but it is a critical first step if we take the climate challenge seriously. Consider that for every reactor that prematurely shuts down, our carbon dioxide emissions rise by about 5.8 million metric tons per year. According to the Environmental Protection Agency’s Greenhouse Equivalencies Calculator, that equals the emissions from burning more than 648 million gallons of gasoline—the equivalent of filling up an NFL stadium with gasoline and setting it on fire. To offset those carbon emissions, we would need to plant over 95 million trees. Or we could install solar panels on one million homes and figure out a cost-effective way of storing the electricity so it is available day and night. \n\nBut that is just to break even and does not move us past the starting line. Instead of swapping one source of zero-emission power for another, wouldn’t it be better to combine all available sources of low- and no-carbon energy to maximize our emissions reductions? The only way to do that is through a public-private partnership. \n\nThis kind of partnership can succeed only over a sustained period, which requires a strong foundation of support across a broad political spectrum. The good news about nuclear energy is that those who care about climate change may support it on environmental grounds while those who care about U.S. global influence may support it for other reasons. Remember that apart from generating power to light homes, drive industrial manufacturing and reduce carbon emissions, the U.S. needs a robust nuclear industry to support its national security. This includes building, operating, sustaining and fueling our aircraft carriers and fast-attack and nuclear-armed submarines. \n\nIn the 1950s, Admiral Hyman Rickover’s redoubtable efforts to establish a nuclear navy led directly to a commercial nuclear power industry in the U.S., beginning with the Shippingport reactor in 1957. Today the Pentagon’s need for reliable power can help drive demand for nuclear energy and defray its costs. \n\nIt is telling that despite the polarized politics of the day, two bills promoting U.S. leadership in nuclear energy passed Congress last year and were signed into law (the Nuclear Energy Innovation Capabilities Act, or NEICA, and the Nuclear Energy Innovation and Modernization Act, or NEIMA). \n\nTogether, public and private partners can drive a new generation of smaller, cheaper, safer nuclear reactors that satisfy the world’s growing energy demands while lowering carbon emissions and reducing proliferation risks.","title":"We Can’t Solve Climate Change without Nuclear Power","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/CE426778-71A5-4F6F-853ED61D9AF572C1_source.jpg?w=590&h=800&9088A599-9E11-4F58-9D2C1CFF7A30A6D5","link":"https://blogs.scientificamerican.com/observations/we-cant-solve-climate-change-without-nuclear-power/"},{"authors":"Get-Fit Guy Brock Armstrong","pub_date":"May 25, 2019","abstract":"The 2019 Canadian guideline for physical activity throughout pregnancy was recently published in the British Journal of Sports and Medicine—and it has some surprising recommendations for pregnant women.\n\nTo get into the nitty-gritty of the new guidelines, I recently talked to one of the authors, Dr. Margie Davenport. Margie is an Associate Professor of Kinesiology, Sport, and Recreation at the University of Alberta. Her graduate work focused on the use of prenatal exercise to promote cardiometabolic health in women diagnosed with, or at risk for, gestational diabetes.\n\nDr. Davenport developed target heart rate zones for pregnant women, which are included in the guidelines, and her postdoctoral fellowship investigated the impact of exercise on cerebrovascular function in older adults with a focus on post-menopausal women. So, as you can tell, she is no slouch when it comes to how exercise can benefit the human body.\n\nDuring our chat, Margie outlined six recommendations that were formulated through an extensive systematic review, expert opinion, consultation with end-users (pregnant and postpartum women), and then balanced her research with the recommendations' feasibility, acceptability, costs, and equity.\n\nIn a nutshell, the main recommendations are that pregnant women should:\n\n\n\nperform at least 150 minutes of moderate exercise per week\n\n\nspread that exercise over at least three (hopefully more) days a week\n\n\ninclude a variety of aerobic and resistance-training activities\n\n\nand finally, there’s no bad time to start physical activity during pregnancy\n\n\n\n\n\n»Continue reading “How Much Should You Exercise While Pregnant?” on QuickAndDirtyTips.com","title":"How Much Should You Exercise While Pregnant?","origin":"Wellness","image":"https://static.scientificamerican.com/sciam/cache/file/FA43D700-AD2A-4491-A704C39E56A2F5EE_source.jpg?w=590&h=800&34CC128E-A10D-4DBC-8649E65DA7F3A176","link":"https://www.scientificamerican.com/article/how-much-should-you-exercise-while-pregnant/"},{"authors":"Evelyn Lamb","pub_date":"May 24, 2019","abstract":"Prime numbers are often described as the “atoms” of mathematics, or at least of numbers. A prime has exactly two distinct factors: itself and 1. (Hence 1 is not considered a prime number.) All whole numbers greater than 1 are either primes or products of primes.\n\nOne of the first questions a curious human could ask about prime numbers is how many there are, and one of the earliest proofs that there are infinitely many primes is a lovely argument from Euclid’s Elements.\n\nEuclid’s proof starts with a finite list of prime numbers and describes a way to generate a prime that is not on the list. If your primes are p1, p2, p3,…,pn, take the product of all of them: p1×p2×p3×…×pn and add 1. This number, p1×p2×p3×…×pn +1, is not divisible by any of the primes on our list; the remainder when we divide it by one of those primes is 1. Hence, the finite list of primes is incomplete.\n\nA common misconception about this proof is that the number p1×p2×p3×…×pn +1 itself has to be prime. That is not necessarily the case. To see why, we can start thinking about what numbers we would get if we started with the first prime, 2, and used it to find new primes using the process from Euclid’s proof. The first one is easy: 2+1=3, and 3 is prime. To find the next number, we multiply 2×3 and add 1 to get 7, which is prime. To continue: 2×3×7+1=43, also prime. 2×3×7×43+1=1807, which is 13×139.\n\nThe Euclid-Mullin sequence is the sequence that starts 2, 3, 7, 43, 13, and so on: the first term is 2, and each subsequent term is the smallest prime factor of 1 plus the product of all previous terms. It is sequence A000945 in the Online Encyclopedia of Integer Sequences.\n\n(Side question: Is the Euclid-Mullin sequence the poly-eponymous term in math with the greatest chronological gap between eponyms? Euclid, or the person or group of people who wrote Euclid’s Elements, lived around 300 BCE in Alexandria; American mathematician and engineer Albert Mullin was born in 1933 and died in 2017. If you know of a term named for people who lived more than 2200 or so years apart from each other, let me know on Twitter.)\n\nKen Ribet mentioned the Euclid-Mullin sequence when my cohost Kevin Knudson and I talked with him for our podcast My Favorite Theorem, and it’s been rattling around in my brain ever since. The sequence has infinitely many terms, but we know only 51 of them. As the number of terms grows, we run into very large numbers that take a long time to factor, in part because the sequence bounces around a lot. The seventh term is the number 5, but the ninth term has 14 digits! Finding the 52nd term of the sequence requires the factorization of a 335-digit number.\n\nWe do not know whether every prime number appears in the Euclid-Mullin sequence. The smallest prime not known to appear on the list is 41. The similar sequence that chooses the largest instead of smallest prime that divides 1 plus the product of the previous terms avoids an infinite number of primes. If the Euclid-Mullin sequence does avoid some primes, why? And could we look at a prime and tell whether it is in the sequence or not? \n\nI am drawn to adorable and amusing math, so I hope the Euclid-Mullin sequence contains all prime numbers. I love the fact that it would give us a new natural order to put the prime numbers in. It would be like putting the letters of the alphabet in the alphabetical order of their phonetic spellings:\n\nHRABDWEFXLMNSIJGKQOPCTVYUZ (phonetic spellings being somewhat debatable, your mileage may vary).\n\nOn the other hand, wouldn’t it be funny if somehow we could figure out that the sequence did not contain all prime numbers, but we couldn’t figure out which ones never showed up? The least amusing option, I suppose, would be for someone to figure out a simple rule that would determine whether a number was in the Euclid-Mullin sequence or not. But hey, the silver lining would be that we might actually gain some insight into prime numbers as a result!","title":"A Curious Sequence of Prime Numbers","origin":"Roots of Unity","image":"https://static.scientificamerican.com/blogs/cache/file/EFD689BF-F519-45A7-BDCA2FAC981E9C30_source.jpg?w=590&h=800&7767BB8A-61AD-4C83-AE74F062C8E1640C","link":"https://blogs.scientificamerican.com/roots-of-unity/a-curious-sequence-of-prime-numbers/"},{"authors":"Stephanie Pappas","pub_date":"May 23, 2019","abstract":"The foundation of massive, flashy and dazzling coral reefs may be a group of fish almost too small to see.\n\nNew research suggests a group of fish species called cryptobenthics are the fuel that feeds coral reef ecosystems. Most cryptobenthic fish weigh just a fraction of a gram each—but they make up more than half of all fish flesh consumed on reefs each year, says study leader Simon Brandl, a postdoctoral researcher in marine ecology at Simon Fraser University. Millions of humans rely on bigger reef fish for food, but how reef ecosystems sustain such a bounty of species in tropical oceans that are low in plant nutrients has been a longstanding mystery that the new work could help explain.\n\n“It’s actually frustrating how little we know as scientists about coral reef ecosystems,” says Julia Baum, a marine ecologist at the University of Victoria in British Columbia, who was not involved in the new research.\n\nBrandl began to suspect cryptobenthics were key to the bounty of coral systems when he was combing through decades of data on newly hatched larval fish that hang around the periphery of reefs. Within about five kilometers of reefs worldwide, Brandl noticed, roughly two thirds to three quarters of the larvae present belonged to cryptobenthic species.\n\nThese fish grow to just a few centimeters long at most, and live near the seafloor or in the nooks and crannies of rocks and corals on reefs. Even experienced divers often fail to notice them, says Gabby Ahmadia, the director of marine conservation science at the World Wildlife Fund, who was also not involved in the new study.\n\nBut Brandl and his team found the diminutive cryptobenthics to be extremely productive—they can generate a lot of themselves in a very short time. Most reef fish larvae start their lives by heading to the faraway open sea, a trip that enables them to disperse to new reefs, but that has a high mortality rate. Cryptobenthics have a different strategy: First, many species go to great lengths to protect their eggs, carrying them around in their mouths or in special pouches near their fins, which boosts the proportion of eggs that survive to the hatching stage. Then, larvae stay nearer the reef where they hatched for the first few weeks of their development, avoiding becoming lunch to fish in the open ocean. But when they hit adulthood they boomerang back home, where most are then rapidly eaten by other reef denizens (but are quickly replaced by new larvae).\n\nAlthough they make up only around 2 percent of the total biomass on a reef at any given moment, they account for 57 percent of the fish biomass consumed within reefs each year, Brandl and his colleagues report in a study published Thursday in Science. In essence, these rapid breeders provide a constant influx of calories to their fellow reef-dwellers.\n\nOther scientists have previously suggested that tiny cryptobenthic fish might be important in the reef food web. But the fish had been easy to dismiss because they were so small and hard to study, says Giacomo Bernardi, a professor of ecology and evolutionary biology at the University of California, Santa Cruz. “It’s a very impressive paper,” he says of the new work, which he was not involved with. “It’s going to be a really important one.”\n\nQuantifying components of reef food webs is indeed difficult and essential to understanding reef dynamics, says Jacob Allgeier, a marine ecologist at the University of Michigan who was not part of the research. Yet he says the new study leaves him curious about cryptobenthics’ overall contribution to total fish production. These fish could be very important at the base of the reef food web, he says—however, he adds that other potential food sources, such as small invertebrates, might be similarly crucial and are just as challenging to study.\n\nThe new research also raises questions about the health of coral reefs as the oceans warm and acidify. One possible downside of the cryptobenthic fishes’ small size and shy lifestyle is that it might be hard to detect problems in their populations that could in turn affect the larger reef ecosystem, Bernardi says. Also, the fishes’ limited range likely means that they have few options for moving to cooler areas as the oceans heat up, Baum notes.\n\nThe sheer number of cryptobenthic species, however—there are an estimated 4,000, many yet unknown to science—might offer hope for some resilience under changing conditions. A 2012 study involving a portion of the Great Barrier Reef found that 10 years after a major coral-bleaching event in 1998, the mix of cryptobenthic populations had shifted compared with before the bleaching—but the ascendant species still had roles in the ecosystem similar to those of their predecessors. Research on environmental disturbances in other reefs would help clarify the role cryptobenthics play in coral recovery from such events, Ahmadia says.\n\nA pressing concern, Brandl says, is that many cryptobenthic fish dwell in the extremely complex branching corals that are often the first to suffer from human environmental pressures. “If reefs are in fact changing as rapidly as it looks like they are right now,” he says, “losing this kind of complexity might be a really hard hit for a lot of these species.”","title":"Tiny, Snackable Fish Are Linchpins of Coral Reef Ecosystems","origin":"Biology","image":"https://static.scientificamerican.com/sciam/cache/file/6E779A0F-0B22-429F-A5EBB5CC665B8398_source.jpg?w=590&h=800&2E818074-E8E0-42AF-90C52956D6F8336D","link":"https://www.scientificamerican.com/article/tiny-snackable-fish-are-linchpins-of-coral-reef-ecosystems/"},{"authors":"John Horgan","pub_date":"December 17, 2013, 16","abstract":"Physicist Murray Gell-Mann, one of the greatest physicists of the second half of the 20th century, died on Friday, May 24 at the age of 89. This post was written in 2013, on the 50th anniversary of his first paper on quarks, which he proposed in 1963.\n\nTo commemorate the 50th anniversary of Murray Gell-Mann's first paper on quarks, Gell-Mann biographer George Johnson has written several terrific posts about one of the truly great theorists—and characters—of modern physics. See here, here and here.\n\nI had the good fortune (and Gell-Mann, perhaps, the bad) to interview Gell-Mann twice: in 1991, for a profile in Scientific American; and in 1995, when I was researching an article on \"chaoplexity\" (my term for complexity and its antecedent, chaos). The latter interview took place at the Santa Fe Institute, a leading center of complexity studies, which Gell-Mann helped found. To my surprise, Gell-Mann disparaged the hope that complexity research would yield profound new laws of nature, such as a force that counters the tendency of all systems to become more disordered.\n\nIn my 1996 book The End of Science, I cited Gell-Mann's views to advance my thesis that science would not yield insights into nature comparable to natural selection, the double helix, quantum mechanics, relativity. Gell-Mann was not amused. Last summer, he suggested that I receive an Ig Nobel Award for advancing the \"ridiculous theory that science is mined out.\" But his main beef with me is my criticism of string theory, not of complexity.\n\nRead George Johnson's posts on Gell-Mann. Then, if you'd like a different perspective, take a look at what follows, an edited version of my profile of Gell-Mann in The End of Science:\n\nMurray Gell-Mann is a master reductionist. He won a Nobel Prize in 1969 for finding a unifying order beneath the alarmingly diverse particles streaming from accelerators in the 1950s. He called his particle-classification system the Eight-fold way, after the Buddhist road to wisdom. (The name was meant to be a joke, he emphasized; he is not one of these flakey New Age types who thinks physics and Eastern mysticism have something in common.)\n\nHe showed the same flair for discerning unity in complexity--and for coining terms--when he proposed that neutrons, protons and a host of other shorter-lived particles consist of triplets of more fundamental entities, which he dubbed \"quarks.\" Gell-Mann's quark theory has been amply demonstrated in accelerators, and it remains a cornerstone of the standard model of particle physics.\n\nGell-Mann is fond of recalling how he stumbled on the neologism quark while perusing James Joyce's gobbledygookian masterpiece Finnegans Wake. (The passage reads, \"Three quarks for Muster Mark!\") This anecdote serves notice that Gell-Mann's intellect is far too powerful and restless to be satisfied by particle physics alone.\n\nAccording to a \"personal statement\" that he distributes to reporters, his interests include not only physics and modernist literature but also nuclear arm-control, natural history, human history, population growth, sustainable human development, archaeology and linguistics. Gell-Mann has at least some familiarity with all the world's major languages, and he enjoys telling people about the etymology and correct native pronunciation of their names.\n\nGell-Mann is unquestionably one of this century's most brilliant scientists. (His literary agent, John Brockman, once said that Gell-Mann \"has five brains, and every one is smarter than yours.\") He is also one of the most annoying, because of his fondness for dwelling on his own talents and belittling those of others.\n\nGell-Mann displayed this trait almost immediately after we met in 1991, when I interviewed him in a New York City restaurant. I had barely sat down when Gell-Mann began to tell me—as I set out my tape recorder and yellow pad—that science writers are \"ignoramuses\" and a \"terrible breed,\" who invariably get things wrong; only scientists are qualified to present their work to the masses.\n\nAs time went on, I felt less offended, since Gell-Mann obviously held many of his scientific peers in contempt as well. After a series of demeaning comments about other physicists, Gell-Mann said, \"I don't want to be quoted insulting people. It's not nice. Some of these people are my friends.\" [*See the end of this post for a story about how our meeting ended.]\n\nI interviewed Gell-Mann again in 1995 at the Santa Fe Institute, a small but influential research center dedicated to the study of complex systems. Gell-Mann was one of the first major scientists to climb aboard the complexity bandwagon. He helped found the Santa Fe Institute and became its first full-time professor in 1993, after decades of teaching at Caltech.\n\nFor a putative leader of chaoplexity, Gell-Mann espoused a worldview remarkably similar to that of the arch-reductionist Steven Weinberg--although Gell-Mann did not see the convergence. \"I have no idea what Weinberg said in his book,\" Gell-Mann replied when I asked if he agreed with Weinberg's comments on reductionism in his 1992 book Dreams of a Final Theory. \"But if you read mine you saw what I said about it.\"\n\nGell-Mann then reiterated themes of his 1994 book The Quark and the Jaguar. (See George Johnson's discussion of the book's tortuous genesis.) Gell-Mann (like Weinberg) views science as a hierarchy. At the top are theories that apply everywhere in the known universe, such as the second law of thermodynamics and his own quark theory. Other theories, such as those related to genetic transmission, apply only here on earth, and the phenomena they describe entail a great deal of randomness and historical circumstance.\n\n\"With biological evolution we see a gigantic amount of history enters,\" he said, \"huge numbers of accidents that could have gone different ways and produced different life forms than we have on the earth, constrained of course by selection pressures. Then we get to human beings, and the characteristics of human beings are determined by huge amounts of history. But still, there's clear determination from the fundamental laws and from history, or fundamental laws and specific circumstances.\"\n\nGell-Mann's reductionist predilections can be seen in his attempts to substitute his own neologism, plectics, for complexity. Plectics \"is based on the Indo-European word plec, which is the basis of both simplicity and complexity. So in plectics we try to understand the relation between the simple and the complex, and in particular how we get from the simple fundamental laws that govern the behavior of all matter to the complex fabric that we see around us,\" he said. (Unlike quark, plectics has not caught on. I have never heard anyone besides Gell-Mann use the term—except to deride Gell-Mann's fondness for it.)\n\nI asked if Gell-Mann agreed with what his Santa Fe colleague and fellow Nobel laureate Phil Anderson said in his famous 1972 essay \"More Is Different.\" \"I have no idea what he said,\" Gell-Mann replied disdainfully. (Gell-Mann liked to call Anderson's field \"squalid-state physics.\") I explained Anderson's idea that complex phenomena such as life and consciousness require their own theories; you cannot reduce them to physics.\n\n\"You can! You can!\" Gell-Mann cried. \"Did you read what I wrote about this? I devoted two or three chapters to this!\" Biological phenomena, he acknowledged, obviously cannot be easily deduced from fundamental physical principles, but that does not mean organisms are ruled by their own laws operating independently of the laws of physics. \"I founded a whole institute to try to react against excessive reductionism,\" Gell-Mann said, \"but reductionism in principle hasn't been proved wrong.\"\n\nGell-Mann rejected the possibility--raised by Stuart Kauffman and others--that there might be a still-undiscovered force of nature that organizes matter into ever-more complex forms in spite of the supposedly inexorable increase of entropy. This issue, too, is settled, Gell-Mann said. The universe began in a \"wound-up\" state far from thermal equilibrium. As it winds down, disorder increases, on average, throughout the system, but there can be local violations of that tendency.\n\n\"It's a tendency, and there are lots and lot of eddies in that process,\" said. \"That's very different from saying complexity increases. The envelope of complexity grows, expands. It's obvious from these other considerations it doesn't need another new law, however!\"\n\nThe universe creates what Gell-Mann calls \"frozen accidents\"--stars, galaxies, planets, stones, trees, humans--complex structures that serve as a foundation for the emergence of still more complex structures.\n\n\"As a general rule, more complex life forms emerge, more complex computer programs, more complex astronomical objects emerge in the course of non-adaptive stellar and galactic evolution and so on. But! If we look very, very, very far into the future, maybe it won't be true any more!\" Eons from now the era of complexity could end, and the universe could degenerate into \"photons and neutrinos and junk like that and not a lot of individuality.\" Entropy will get us after all.\n\n\"What I'm trying to oppose is a certain tendency toward obscurantism and mystification,\" Gell-Mann continued. He emphasized that there is still much to be understood about complex systems. \"There's a huge amount of wonderful research going on. What I say is that there is no evidence that we need some--I don't know how else to say it--something else!\"\n\nGell-Mann, as he said \"something else,\" wore a huge sardonic grin, as if he could scarcely contain his amusement at the foolishness of those who might disagree with him.\n\nGell-Mann noted that \"the last refuge of the obscurantists and mystifiers is self-awareness, consciousness.\" Humans are obviously more intelligent and self-aware than other animals, but they are not qualitatively different. \"Again, it's a phenomenon that appears at a certain level of complexity and presumably is emergent from the fundamental laws plus an awful lot of historical circumstances. Roger Penrose has written two foolish books based on the long discredited fallacy that Godel's theorem has something to do with consciousness requiring\"--pause--\"something else.\"\n\nParticle physics, Gell-Mann said, still represents science's best hope of discovering profound new principles of nature. Gell-Mann believed superstring theory would probably be confirmed as a unified theory of all fundamental forces early in the next millennium.\n\nBut would such a far-fetched theory--with its extra dimensions and infinitesimal stringy particles--ever really be accepted? After I asked this question, Gell-Mann stared at me, as if I'd just confessed to belief in angels. \"You're looking at science in this weird way, as if it were a matter of an opinion poll,\" Gell-Mann said. \"The world is a certain way, and opinion polls have nothing to do with it! They do exert pressures on the scientific enterprise, but the ultimate selection pressure comes from comparison with the world.\" He urged me to ignore \"crazy criticisms of superstring theory.\"\n\nGell-Mann also had no problem with theories that posit the existence of other universes; in fact, he is a proponent of the many-worlds interpretation of quantum mechanics. The goal of physics, he said, should be to determine whether our particular cosmos is probable or improbable. \"If it turns out that we're in a very improbable universe,\" Gell-Mann admitted, \"it'll look funny.\" But physicists can always fall back on the anthropic principle, he said, to explain why we happen to find ourselves in this particular universe.\n\nIs science finite or infinite? For once, Gell-Mann did not have a pre-packaged answer. \"That's a very difficult question,\" he replied soberly. \"I can't say.\" His view of how complexity emerges from fundamental laws, he said, \"still leaves open the question of whether the whole scientific enterprise is open-ended. After all, the scientific enterprise can also concern itself with all sorts of details.\"\n\nDetails.\n\nOne reason why Gell-Mann is so insufferable is that he is almost always right. His assertion that research on complex systems will not yield something else—a profound new principle of nature—will probably prove to be correct. Gell-Mann errs—dare one say it?—only in his judgment of string theory, which will never be as empirically validated and hence accepted as, say, quark theory.\n\n*One final story about Gell-Mann: After our meal together in New York in 1991, I hired a limousine to take us to the airport, where Gell-Mann was catching a plane. Before we parted, he fretted that he did not have enough money for a taxi after his plane landed; if I could give him $40 in cash, he'd write me a check. As Gell-Mann handed me the check, he suggested that I not cash it, since his signature would probably be quite valuable. I cashed the check but kept a photocopy.\n\nPhoto: Wikimedia Commons, http://commons.wikimedia.org/wiki/File:Murray_Gell-Mann_at_Lection.JPG.","title":"From My Archives: Quark Inventor Murray Gell-Mann Doubts Science Will Discover \"Something Else\"","origin":"Cross-Check","link":"https://blogs.scientificamerican.com/cross-check/from-my-archives-quark-inventor-murray-gell-mann-doubts-science-will-discover-e2809csomething-elsee2809d/"},{"authors":"Benjamin Storrow, E&E News","pub_date":"May 22, 2019","abstract":"Elizabeth Caruso is a town official in Caratunk, Maine, a community of 70 people about an hour’s drive south of the Canadian border. Like many people in this part of rural New England, her livelihood is tied to tourism.\n\nThat’s why she’s leading the opposition to a 145-mile transmission line that would cut a 150-foot-wide path through the forest around Caratunk. She worries the proposal would splinter the Maine woods and the businesses that rely on it.\n\nShe and her husband run a guiding business. In the summers, they take clients fishing on the ponds dotting the region and ferry hikers on the Appalachian Trail across the Kennebec River. Fall brings hunters, and snowmobilers make the pilgrimage to Coburn Mountain in the winter.\n\nA power line would change that, she said.\n\nThe so-called New England Clean Energy Connect, or NECEC, would link Hydro-Québec in the north to energy-hungry Massachusetts in the south, supplying nearly a fifth of the Bay State’s annual electricity consumption with low-carbon power. The project stems from a 2016 Massachusetts law aiming to slash the state’s emissions by requiring utilities to purchase large amounts of low-carbon energy.\n\nLike many in this part of Maine, Caruso is skeptical of the project’s purported climate benefits. She believes Massachusetts selected Hydro-Québec’s proposal for political reasons and noted the utility has refused to submit to cross-examination during regulatory proceedings underway in Massachusetts and Maine. In her mind, the region would be better served by supporting local solar projects.\n\n“No one is saying we don’t want clean energy, and no one is saying we just don’t want this in our backyard,” Caruso said. “But I know there is no proven benefit to ask Maine to sacrifice that to the depths of what they’re asking Maine to give.”\n\nShe is hardly alone. Massachusetts’ attempt to import more Canadian hydropower has been blocked by environmental groups, energy companies and communities opposed to the transmission lines linking the state to Quebec. The New England Clean Energy Connect is only now under consideration because a project through New Hampshire was rejected by regulators there.\n\nThe debate around transmission lines often revolves around the impact on scenic landscapes, wildlife concerns and the potential disruption to the wholesale electricity market that unites the six New England states. But it also hints at the larger question of whether imports of Canadian hydropower are needed to decarbonize the region’s electric grid.\n\nMassachusetts Gov. Charlie Baker (R) has argued the hydropower will help supplant the natural gas generation that today accounts for 50% of New England’s electricity generation, building the foundation for a low-carbon electric grid and easing concerns over wintertime grid reliability, when the region’s limited natural gas pipelines must serve power plants and heating demand.\n\nBut Baker has felt pushback from Attorney General Maura Healey (D), who argues that the proposed contracts between Hydro-Québec and the state’s three electric distribution utilities lack the guarantees to ensure the Canadian utility increases its imports, effectively forcing Massachusetts consumers to pay for electricity that does not decrease emissions.\n\nIt is a similar story farther north in Maine, where the proposed transmission project has produced a fissure between Gov. Janet Mills (D) and Democratic lawmakers who control the Legislature. Mills maintains that the new line will cut emissions, lower electricity bills and create jobs. Democratic lawmakers worry the project will exact a heavy environmental toll and hamper renewable development, all while failing to deliver the promised carbon reductions.\n\nThey have been joined by environmental groups like the Sierra Club and Natural Resources Council of Maine, which warn that Hydro-Québec could backfill its domestic consumption with imported fossil fuel generation while selling more exports to New England. That could prompt overall emissions to rise.\n\n“We’re talking about investing billions of dollars in infrastructure that can cause significant environmental damage without the climate benefit,” said Sue Ely, an attorney at the Natural Resources Council of Maine.\n\nOf the utility’s promised carbon reductions, she said, “We should be skeptical, and we should ask for proof.”\n\nOthers dismiss those arguments, saying the possibility of an emissions spike is greatly exaggerated. Imported fossil fuels accounted for 0.04% of Hydro-Québec’s power supply in 2017, and any increase of fossil fuels from neighboring markets would be subject to a financial penalty given Quebec’s participation in California’s cap-and-trade system, they say (Climatewire, Nov. 28, 2018).\n\nHydro-Québec also has significant financial incentives to sell as much electricity as it can to the Northeast, which pays some of the highest electricity rates in the United States.\n\nBefore they approved the transmission line last month, Maine utility regulators wrote that the Canadian utility, “as a rational economic actor, will seek to maximize profits, and therefore will use whatever water it has available to generate energy for the NECEC rather than using the NECEC to divert energy from existing markets in New England.”\n\nLimits of offshore wind\n\nNew transmission lines were built to carry electricity away from four dams on the Romaine River in northeast Quebec. Hydro Québec\n\nThe outlines of a similar debate are forming in New York City. Mayor Bill de Blasio announced last month the city was entering negotiations with Hydro-Québec to supply 1,000 megawatts of electricity to a 330-mile transmission line linking Queens with Quebec.\n\nCity officials frame the move as part of a wider attempt to quickly decarbonize America’s largest metropolis, displacing the fossil fuel generation that now provides 70% of the city’s power. That figure could grow after Indian Point Energy Center, a 2,144-MW nuclear plant outside the city, closes in 2021.\n\n“Hydropower, we believe, can be delivered into the city in the next five years,” said Mark Chambers, who leads the mayor’s Office of Sustainability. “Offshore wind is eight years out. We’re going to need that power, too. The faster we reduce our emissions, the better the outcomes are. This is a time game, as well.”\n\nYet the proposal is starting to garner pushback from New York energy producers. Upstate New York has large potential for wind and existing hydro and nuclear generation. But transmission constraints around the city means there is little prospect for getting it there. A new transmission line filled with hydropower imports from Quebec only exacerbates the problem, said Gavin Donohue, president and CEO of the Independent Power Producers of New York Inc., a trade group representing power plant owners.\n\n“You’re further inhibiting the in-state renewable developers and hindering their ability to compete,” Donohue said.\n\nThe tension over Canadian hydropower highlights the difficult decisions facing the Northeast as states look to decarbonize, climate analysts say.\n\nThere is little prospect of replacing Indian Point or the retiring 690-MW Pilgrim Nuclear Power Station south of Boston with new nuclear facilities. Carbon capture and sequestration at gas plants isn’t even a possibility in New England, which lacks the geologic formations needed to store carbon underground. And utility-scale renewable generation faces a series of constraints, including a lack of transmission, scarce open space and, in the case of solar, the region’s northern climate.\n\nThe Northeast is pinning much of its hope on offshore wind. The industry shows significant promise, but there are limits to what it can provide. Current state targets for offshore wind construction in New England and New York represent roughly 16% and 20% of installed power plant capacity in each region, respectively.\n\nAt the same time, more low-carbon electricity is likely required. Transportation and buildings are the largest and second-largest sources of greenhouse gas emissions in the Northeast today. Most climate experts believe widespread electrification will be needed in both sectors to make meaningful carbon reductions in line with the region’s climate targets.\n\nThe National Renewable Energy Laboratory estimates electricity demand in New York and New England could increase by as much as 71% and 67%, respectively, in 2050 if widespread electrification takes hold. That projection does not account for energy efficiency measures and assumes only moderate technological advancements, but it does underline the challenge facing Northeastern climate hawks as they search for more low-carbon electrons.\n\n“From where I sit it’s hard to see a reason to take Canadian hydro off the table, given the relatively small set of options available,” said John Larsen, a former Obama administration official who leads power sector research at the Rhodium Group, an economic consulting firm. “It doesn’t mean hydro is a silver bullet or an answer to everyone’s prayers. That said, it has unique attributes that make it useful in a low-carbon bulk power system.”\n\nHydro-Québec has long been a major energy supplier to the Northeast. In 2018, it exported almost 17 terawatt-hours of electricity to New England, about 14% of the region’s annual power consumption, and 8.6 TWh to New York.\n\nTracking electrons\n\nNortheastern states have increasingly looked north in recent years, as they contemplated ways to green their power sectors. In many respects, the current push can be traced back to Massachusetts, where lawmakers passed a bill in 2016 requiring the state’s three electric distribution utilities to buy 1,200 MW of low-carbon power, which could come from some combination of hydro, wind and solar.\n\nState regulators received 46 bid packages to supply the contract. They ultimately selected Hydro-Québec—twice. After New Hampshire regulators rejected a proposal to build a transmission line through the state, Massachusetts officials selected the Canadian power company’s second bid: a $950 million power line to be built through Maine with a subsidiary of Avangrid Inc., Central Maine Power. The contracts call on Hydro-Québec to deliver 9.45 TWh annually for 20 years.\n\nIn theory, that should represent a substantial boost in hydropower imports to New England. But critics say a closer look at the proposed contracts between Hydro-Québec and Eversource Energy, National Grid PLC and Unitil Corp.—the three Massachusetts utilities—shows that the opposite is possible.\n\nHydro-Québec would pay penalties if its imports fell below a historic baseline. The problem is the baseline was set so low that the utility could sell less energy to New England and still satisfy its contractual obligations to Massachusetts, said Dean Murphy, an analyst at the Brattle Group who testified as an expert witness on behalf of the Massachusetts attorney general in regulatory proceedings before the state’s Department of Public Utilities.\n\nThat would contradict the purpose of Massachusetts’ 2016 legislation, which sought to foster clean energy development, Murphy said. He argued for increasing the baselines to ensure the state gets what it pays for.\n\n“The proposed contracts,” he said in testimony to the DPU, “would allow most (and potentially all) of the contract energy delivered to substitute for historical deliveries.”\n\nThe DPU is expected to rule on Hydro-Québec’s contracts later this year.\n\nOfficials at the Canadian power company dismiss those arguments, arguing it has a significant financial investment to boost its exports to the Northeast. In recent years, it added 5,000 MW of new power generation in an attempt to enhance its export capacity. And the utility is attempting to free up more electricity for export through a series of energy efficiency initiatives. It estimates energy efficiency measures have already saved 9 TWh in recent years, or roughly the amount it has proposed selling to Massachusetts. It argues the main constraint to additional exports to the Northeast is new transmission lines.\n\n“We want to sell more energy into one of our best markets, and which one is that? It’s New England,” said Gary Sutherland, a company spokesman. “Today New England is already about half of our sales.”\n\nBut in a series of interviews with E&E News, Hydro-Québec officials declined to answer questions about how much power the utility expects to export to the United States if the new transmission lines are built, ultimately saying its projections are confidential.\n\nSuch answers have provoked unease, even among those who believe more Canadian hydro is needed to help meet the region’s climate goals. The Acadia Center is one of several environmental groups that have advocated for injecting more electricity from Hydro-Québec’s existing dams into the Northeast’s power grid. In Maine, the group even offered qualified support of the New England Clean Energy Connect.\n\nAt the same time, the Acadia Center has argued that Massachusetts regulators should amend Hydro-Québec’s contracts with the state’s power companies, echoing the concerns of the attorney general and arguing for better tracking that would enable Massachusetts to verify that the energy is coming from the utility’s dams. That would ensure the power is actually carbon-free, the group says.\n\n“The combination of this sort of lax contract language around the baseline in combination with lack of actual tracking that every other eligible bidder to this contract would have had to undergo, it’s just not a level playing field,” said Deborah Donovan, Massachusetts director at the Acadia Center. “It is the price of entry for any other generation in [a regional portfolio standard] market.”\n\nShe added: “We don’t have 20 years to miss the boat here. We literally do not.”\n\nReprinted from Climatewire with permission from E&E News. E&E provides daily coverage of essential energy and environmental news atwww.eenews.net.","title":"Effort to Trade Gas for Hydropower in Northeast Meets Resistance","origin":"Energy","image":"https://static.scientificamerican.com/sciam/cache/file/B76D41EB-20D4-4690-AB89919AEE8BC80E_source.jpg?w=590&h=800&C3372ED5-4197-45A2-A3F4D36C015C270A","link":"https://www.scientificamerican.com/article/effort-to-trade-gas-for-hydropower-in-northeast-meets-resistance/"},{"authors":"John Fialka, E&E News","pub_date":"May 23, 2019","abstract":"BOULDER, Colo.—Scientists, governments and environmental groups are focused on what may be the first global detective story about emissions. It’s an effort to determine how many rogue businesses may have violated the Montreal Protocol by selling insulation containing a banned chemical that is 5,000 times more powerful as a global warmer than carbon dioxide.\n\nChina has mounted an investigation of more than 1,000 companies to see which Chinese firms violated international law by using trichlorofluoromethane—also known as CFC-11—in plastic foam insulation. Investigators say it’s possible that thousands of tons of the insulation was made for homes and other buildings and distributed throughout the country.\n\nThe chemical was banned by the 1987 treaty because it’s in the family of substances described by scientists as the “main culprits” in the depletion of the Earth’s atmospheric ozone layer, which protects people from the sun’s cancer-causing ultraviolet radiation. The chemical can also harm crops.\n\nThe Montreal Protocol was hailed as one of the world’s most powerful legal tools against future environmental harm. Its timetable called for a total ban of CFC-11 by 2010, but Stephen Montzka, a research scientist here at NOAA’s Global Monitoring Division, found something strange in samples of the atmosphere he was testing in 2014.\n\nNOAA’s data had showed a steady decline of the banned gas until about 2013, but the new samples collected in flasks showed what he called suspicious statistical “bumps and wiggles” on a weekly basis. “I couldn’t believe somebody was not paying attention to this,” he recalled in an interview. “Or did I make a mistake in the lab?”\n\nMistakes were certainly possible. There are at least three different ways to identify the invisible, odorless gas, and Montzka checked them all. They showed that the tiny percentage of CFC-11 in his samples was still declining, but the rate of decline had slowed down. In some samples, it appeared to be increasing.\n\nBut where in the world were the increases coming from?\n\nIt took Montzka and other investigators three years to narrow it down. There are two global atmospheric monitoring networks in the world. NOAA, a unit of the Department of Commerce, has one. A second, larger one is run by agencies and institutions from an international group of countries including the United States. It’s called the Advanced Global Atmospheric Gases Experiment (AGAGE).\n\nMontzka went there to check his findings. Ray Weiss, a geochemist at Scripps Institution of Oceanography at San Diego, and one of the leaders of the AGAGE network of 50 global observation stations, is happy that he did. “Steve gets very high marks for looking at his data,” explained Weiss.\n\nEvidence of the strange slowdown in the decline of CFC-11 had also begun to appear in the AGAGE data. “Steve looked at it more closely than we did,” explained Weiss. The two investigators and their teams began putting their pieces of this vast puzzle together.\n\nOne problem they wrestled with is there are gaps in their networks. Both systems rely on remote observation points where the atmosphere is thoroughly mixed. That allows them to get a global average of chemicals in the atmosphere without being overwhelmed by pollutants from industrial areas.\n\nMontzka came to suspect the increases were coming from the Northern Hemisphere. He had data from Hawaii that pointed to Asia as the most likely source.\n\nScientists at both networks used models to backtrack weather patterns that carried CFC-11 to their observation posts. By May 2018, Montzka had enough evidence to publish a paper in Nature, one of the world’s leading science journals.\n\nIt sounded an alarm.\n\nSomeone appeared to be making new supplies of CFC-11. By Montzka’s estimate, emissions had risen to 67,000 metric tons per year. It was more than just a couple of factories’ worth of production and enough to delay the recovery of the ozone layer, he estimated, by as much as a decade.\n\nCritics of the finding argued that the chemical, formerly used as a refrigerant, among other things, could be leaking from old cooling systems. Others speculated that CFC-11 emissions were leaking from previously made insulation stocks in buildings or stored in warehouses.\n\nA nonprofit in Washington, D.C., the Environmental Investigation Agency, which specializes in undercover investigations, mounted one quickly in China. They visited a number of companies and found employees in 18 of them who explained they had decided to revert to using CFC-11 to make insulation foam and panels because it was a better and cheaper substance to use.\n\n“It was very common knowledge that this was a banned substance,” explained Avipsa Mahapatra, the leader of EIA’s climate campaign. “Clearly there were connections with local enforcement agencies in some places.” She recalled how people at some local businesses bragged to EIA’s sleuths that they had warning calls from municipalities when government inspectors were about to make a visit.\n\nChina was having a building boom. There was high demand for the insulation, and the risks “were very low,” Mahapatra explained. Some companies rented factories to make insulating foam using CFC-11. “Others were offering to sell it on the internet.”\n\nOne result was “Blowing It,” an EIA report released in July 2018 that helped provoke a national investigation of over 1,172 Chinese companies. There have been a few reported arrests and tons of confiscated CFC-11, but Mahapatra, who recently returned from Beijing, explains that government officials told her the investigation has been complicated. “I don’t think we have the final numbers yet,” she said.\n\nAnother result was a statement from the China Plastics Processing Industry Association, which said its members would reject the use of illegal chemicals.\n\nMontzka and Weiss are among the 32 scientists who co-authored a new Nature article, released yesterday. It says they tracked at least 40% to 60% of the global CFC-11 emissions to two of China’s northeastern provinces: Shandong and Hebei. The study finds “no evidence” of a significant increase in emissions from other Asian countries or other regions of the world where there are observation posts that might detect them.\n\nThe working group that is monitoring the problem for the Montreal Protocol has met twice recently, and Montzka said he’s convinced that Chinese investigators “are eager to get at this.”\n\nWeiss agreed but noted that the world’s knowledge of how far the CFC-11 problem has spread is limited to what the global networks can detect and measure. At the moment, scientists can observe the eastern part of China, western Japan, the Korean Peninsula, and parts of North America, Europe and southern Australia.\n\nHe noted that there are large swaths of the world that are unobserved. That’s a problem, he thinks, that could haunt global leaders in the future. It may eventually appear in the accounting process needed to determine which countries are curbing their emissions to meet the Paris Agreement’s goal of limiting global temperatures to 1.5 degrees Celsius by midcentury.\n\n“What’s going to happen when some nations fall short?” Weiss asked. “There’s going to be finger-pointing. Some will say, well, we did our share.”\n\nWorld leaders haven’t devoted enough resources to the observation systems that eventually pinpointed the illegal CFC-11 emissions, he believes. Without more funding, there won’t be a truly global view of the prohibited chemicals, he warns.\n\n“We’ve done it for nuclear proliferation,” Weiss added, noting that there are 80 observation stations and some 300 seismographs poised to pinpoint clandestine nuclear weapons tests anywhere around the world. “We ought to be doing the same thing for ozone and climate change.”\n\nReprinted from Climatewire with permission from E&E News. E&E provides daily coverage of essential energy and environmental news atwww.eenews.net.","title":"Mystery Solved: Warming Superpollutant Tracked to China","origin":"Environment","image":"https://static.scientificamerican.com/sciam/cache/file/8B3F163D-F2C8-40DB-95717A8B9CB1EAEC_source.jpg?w=590&h=800&6750A416-EF08-445C-957B3BE379FC4BA3","link":"https://www.scientificamerican.com/article/mystery-solved-warming-superpollutant-tracked-to-china/"},{"authors":"Amy Maxmen, Nature magazine","pub_date":"May 23, 2019","abstract":"When Serggio Lanata moved to San Francisco in 2013, he was stunned by its sprawling tent cities. “Homelessness was everywhere I looked,” he says. Lanata, a neurologist at the University of California, San Francisco (UCSF), was also struck by similarities in the behaviour of some older homeless people and patients he had treated for dementia in the clinic. Now, years later, he is embarking on a study that will examine homeless adults for early signs of Alzheimer’s disease and other degenerative brain disorders to better understand the interplay between these conditions and life on the street.\n\nThe work, which is set to begin next month, ties into an ongoing effort by researchers at UCSF to understand the biological effects of homelessness in older people. Since 2013, a team led by Margot Kushel, director of the university’s Center for Vulnerable Populations, has followed a group of about 350 older homeless adults in Oakland, California, to determine why this group ages in hyper-speed. Although the participants’ average age is 57, they experience strokes, falls, visual impairment and urinary incontinence at rates typical of US residents in their late 70s and 80s.\n\nThe research has drawn attention from politicians, economists and health-care providers across the country who are struggling to help the homeless and reduce their numbers. Although homelessness is a global problem, the situation in California is particularly acute. Nearly 70% of the 130,000 people without homes in the state are considered to be ‘unsheltered’, living on the streets or in locations unfit for human habitation, compared with just 5% in New York City. In the San Francisco Bay Area—California’s wealthy technology hub, which includes Silicon Valley—roughly 28,200 people are homeless.\n\nThe United States’ homeless population is also greying: rising housing prices in many areas have increased the rate of homelessness among ‘baby boomers’ born between 1954 and 1964. But many hospitals, police and homeless shelters are unprepared to deal with the special needs of an ageing homeless population. “I hear from shelter providers, ‘Gosh, we are set up for people who use drugs but we have no idea how to manage dementia’,” Kushel says. By understanding how homelessness can accelerate ageing, her team hopes to identify ways to curb suffering and save governments money.\n\n“This crisis is upon us,” says Dennis Culhane, a social scientist at the University of Pennsylvania in Philadelphia. “A lot of money will be spent on this population. We can draw upon Margot’s data and learn how to spend that money wisely—or else we’ll just spend and still have lots of human misery.”\n\nHe and his colleagues estimate that Los Angeles, California, will spend $621 million annually on emergency medical care, nursing home beds and shelters for homeless people over the age of 55 between 2019 and 2030. Their analysis suggests that the city could reduce its spending by $33 million per year if it provided homes to elderly people who lack them.\n\nA closer look\n\nResearchers have known for decades that physical and mental health problems are prevalent among the homeless (see 'Declining health'). But there was little systematic research on the progression and causes of their ailments in 2013, when Kushel launched a study on the life trajectories of older homeless adults in the Bay Area. Since then, 42 of the initial 350 participants have died—mainly from cancer, heart attacks and diabetes. (Earlier this year, the study enrolled another 100 people to compensate for the loss of original participants.)\n\nKushel and her colleagues got a boost on 1 May, when philanthropists Marc and Lynne Benioff announced that they had donated US$30 million to create a research initiative at UCSF on homelessness. Marc Benioff, who founded the San Francisco-based computing company Salesforce, says the money will support research to explore the causes of homelessness and identify ways to prevent it.\n\nLanata’s study, which is set to begin next month, will look for signs of debilitating brain conditions—such as dementia of the frontal and temporal lobes, which can cause behavioural changes—in at least 20 homeless adults. He and his colleagues will conduct neurological exams, which might include brain scans, on participants to learn how homelessness influences these brain disorders. People living on the streets might face several factors that can contribute to neurological disease, Lanata says, such as lack of sleep, exposure to polluted air near highways, poorly controlled diabetes, high blood pressure and alcohol abuse.\n\nBy asking study participants about their personal histories, he also hopes to learn whether neurological issues might have helped to put them on the street—perhaps by impairing their ability to work or seek government assistance. That would make sense to him, given his experience treating people with some types of dementia. “If those patients didn’t have strong family support, they would be homeless, since no one could or would care for them,” Lanata says. “They can be hard to handle.”\n\nAnd Kushel has begun a new phase of her ongoing study, which will explore how the sudden stress of homelessness might trigger or exacerbate existing conditions. Many of the people in her study were over the age of 50 when they became homeless.\n\nNearly half of the participants exhibit signs of extreme loneliness, which has been linked to poor outcomes in people with cancer and other diseases. One-quarter of those in the study meet the criteria for cognitive impairment, compared with less than 10% among people over the age of 70 in the United States more generally. And in a paper in the press, Kushel and her colleagues found that 10% of participants report being physically or sexually assaulted at least every six months.\n\nAn increasing toll\n\nAlthough Culhane and other health economists have already begun to use Kushel’s findings to project how much it costs to care for the indigent, it is not clear whether politicians or the public will accept such suggestions.\n\nCalifornia Governor Gavin Newsom included $500 million for shelters and other support facilities in his proposed $209 billion state budget for 2019–20. But in late March, San Francisco residents rapidly met their goal of raising more than $100,000 to block the construction of a homeless shelter in a wealthy, waterfront neighbourhood. And although city voters approved a plan in November 2018 to fund services for the homeless by taxing the San Francisco’s biggest companies, business groups are challenging the policy in court.\n\nCoco Auerswald, a public-health researcher at the University of California, Berkeley, hopes that Kushel’s work and other studies of homelessness strike a moral nerve. “You judge a society on how it treats its most vulnerable,” she says. “My fear is that we will accept this as a state of affairs in our country.”\n\nThis article is reproduced with permission and was first published on May 21, 2019.","title":"What Are the Biological Consequences of Homelessness?","origin":"Biology","image":"https://static.scientificamerican.com/sciam/cache/file/50B72349-C13C-4B51-A2838E43E4A79DCD_source.jpg?w=590&h=800&AD88C5F4-1393-4243-8A21999699454FF1","link":"https://www.scientificamerican.com/article/what-are-the-biological-consequences-of-homelessness/"},{"authors":"Scott Barry Kaufman","pub_date":"May 22, 2019","abstract":"When it comes to intelligence, we all have bad days. Heck, we even have many bad moments, such as when we forget our car keys, forget a friend's name, or bomb an important test that we've taken a day after staying up all night worrying about it. Truth is, none of us-- including the world's smartest human-- is perfectly consistent in our cognitive functioning. Sometimes we are at our very best and feel like our brain is on fire, and at other times, we don't even recognize ourselves.\n\nAll of this sounds so obvious, but surprisingly the field of human intelligence has not had much to say on the topic. For over the past 120 years, the field has shed far more light on how we differ from each other in our patterns of cognitive functioning than how we each differ within ourselves over time.\n\nThis is curious considering that a person-centered approach has proved fruitful in other fields, such as medicine and neuroscience. Even within the study of human behavior there has been progress, from looking at how individual emotions fluctuate over time, to how individual personality traits such as introversion and openness to new experiences and even our morality fluctuates throughout the course of the day. It has become increasingly clear that the results from the traditional individual differences paradigm-- where we compare people to each other-- often does not apply at the person-specific level.\n\nIn only the past few years, intelligence researchers have been able to demonstrate that this is also true in the domain of human intelligence. For the past 120 years, the field just hasn't had the tools to view intelligence at such a level of granularity. With the adoption of newer technologies, however, researchers have begun to view an individual's intelligence at a more microscopic level, able to capture all sorts of fascinating variations-- across days, within days, and even moment-to-moment. It turns out that intelligence is changing all over the place all the time. Who knew?\n\nOf course, this was true well before these recent papers emerged, but we literally didn't have a way to think about how to measure intelligence at such a level until we got things like computer tablets that make it feasible to test people at a wide range of different timescales. As the Cambridge neuroscientist Rogier Kievit, one of the leaders of this new paradigm told me,\n\n\n\t\"I think about it as a cognitive microscope. It's like we put a bit of rain water under the microscope and looked at it and suddenly there are animals or tiny creatures moving around. It was there all along, but we just didn't have the tools to look at it. This is a whole new avenue into studying how people differ and how they change and which types of variability are bad and which ones are uniformly good.\"\n\n\nLet's take a deep dive into this exciting new view of intelligence.\n\nFluctuations in Intelligence\n\nIn the past few years, Florian Schmiedek, Martin Lövdén, and Ulman Lindenberger at the Max Planck Institute for Human Development have been leading the charge in understanding fluctuations in cognitive ability over time. Not only have they demonstrated that the cognitive functioning of most people fluctuates quite a bit throughout the day and across days, but that some people fluctuate quite a bit more than others. This applies to children in elementary school as well as adults in everyday life. Remember these findings the next time you panic that you might be getting dementia because you forgot your house keys. Just think about how many times you actually remembered your house key in the past month!\n\nIn my view, this research is revolutionary for a number of reasons. For one, this research shows that these cognitive fluctuations aren't simply the result of random noise or \"error variance\". They are systematic. Researchers have started to reveal some of the most important factors that have a systematic impact on fluctuations in intelligence, and this includes sleep quality and sleep duration, emotions, noise disturbance in the school classroom, cognitive fatigue, and poverty.*\n\nThe person-centered approach to intelligence is also groundbreaking because it allows us to tease out different profiles of variability which may have important implications on real-world functioning. For instance, one study by Florian Schmiedek and Judith Dirk at the Leibniz Institute for Research and Information in Education in Germany had 110 schoolchildren in grades 3 and 4 complete working memory tasks on smartphones 3 times a day in school and at home for 4 weeks. Those who have strong working memory performance are able to hold multiple bits of information in memory while simultaneously processing other information (such as comprehending the last sentence I wrote, which required a lot of working memory). Working memory is essential for learning and reasoning, and this is especially the case when it comes to complex on-the-spot problem-solving under timed conditions. In other words, school.\n\nWhile the researchers found overall significant fluctuations day-to-day and moment-to-moment, some children showed a lot more variability than other children. In fact, some children showed no systematic day-to-day variability whatsoever in their working memory performance. This had real-world implications, as more variable working memory performance was related to lower school achievement and lower scores on a fluid intelligence test which measured on-the-spot abstract reasoning.\n\nIn the same study study, the children also rated their momentary emotional states. Overall, working memory performance was lower on occasions when the child reported higher negative emotions, and there was no link between working memory performance and positive emotions. However-- and this is critical-- children differed in the degree to which they were affected by their environment.\n\nUsing the person-centered approach, the researchers were able to identify different groups of children. In line with the distinction between \"the orchid and the dandelion\", some children were sensitive to all emotional stimuli, showing a strong effective of both positive and negative emotions on their working memory performance (orchids), whereas others showed low sensitivity to their current affective state overall (dandelions). This new paradigm allows us to see more clearly than ever before that when it comes to the complex relationship between emotions and cognition, there is no one-size-fits-all approach.\n\nFinally, this research is important because it suggests that the much researched \"general factor of intelligence\" (g)-- the largest source of cognitive variation ever discovered in humans-- is much less prominent within people than between people. To be sure, over the past 120 years intelligence researchers have done a truly remarkable job cataloguing the structure of cognitive abilities that exists when you assess intelligence between people, and general intelligence does predict many important things in life.\n\nHowever, Schmiedek and his colleagues found that within-person structures of daily cognitive performance cannot be inferred from between-person structures. To demonstrate this, the researchers administered a wide range of cognitive tests to 101 young adults on 100 occasions over the course of 6 months. They found that each person had their own cognitive signature, with differing fluctuations across the different tasks over the span of 6 months. The research team then attempted to predict how well an individual would perform on one particular task on a certain day by their performance on the other eight tasks that were also done on each day. They found that this prediction worked much better if the prediction took into account the individual's highly idiosyncratic structure of daily fluctuations, rather than using the structure that describes average between-person differences in cognitive ability.\n\nAll of this is a fancy way of saying that if you really want to understand the complexities of a person's intelligence, we can do much better than simply looking at a person's overall IQ score based on their one-time intellectual deviation from other people who all took the test at different times in a sterile testing environment. This doesn't offer nearly as much information about the rich tapestry of an individual person's intellectual landscape as actually following them over time at different times of the day as they engage in a variety of different cognitive tasks in their everyday lives.\n\nPractical Implications and Future Directions\n\nThis new frontier in intelligence research opens up a lot of avenues. One avenue is the investigation of the long-run consequences and causes of variability. The longest timescale Schmiedek and his team has looked at is 6 months, which involved 100 different measurements for each person. What happens when we look at years, even decades, with thousands and thousands of different data points per person? What does the long arc of a person's intellectual life look like? What are the major life events that cause the biggest fluctuations in a person's life, and what impact do those fluctuations have on a life well lived?\n\nRogier Kievit-- who is currently applying for a grant to look at the impact of long-term fluctuations-- told me that he finds this line of research \"absolutely fascinating.\" Kievit isn't only interested in the antecedents and causes of cognitive fluctuations over long time scales, but is also curious as to which fluctuations can be beneficial, and which ones may be detrimental to performance. Kievit points out that some fluctuations can be a positive sign that a person is trying different strategies to solve a problem, whereas for others fluctuations can be an indication of floundering.\n\nThe implications may also be different for adults than young children. Low variability may be a positive sign for adults, whereas high variability among children can be more mixed, depending on the causes of the variability (is it due to exploration and smart strategies or blind trial-and-error?). Kievit is particularly excited by the increased attention on topics such as the \"microgenetics\" approach pioneered by Robert Siegler which examines change as it occurs at a very high temporal resolution. Such moment-to-moment fluctuations in abilities such as spatial working memory have already been captured in schoolchildren using smartphones. It'll be exciting to see how this plays out in the long-run for the child.\n\nI can envision a smart phone app someday that will allow you to do repeated assessments of your cognitive performance across a wide range of tasks over the course of months to determine what times of the day you are at peak cognitive performance, and for which cognitive abilities. This would be useful not only for adults to plan their workday, but also for children scheduling when to take which classes. What appears to be a \"dull\" child may have more to do with the time of day that assessment is being made, or a particular time in that child's life, than a reflection of their true intelligence.\n\nWhich leads me to another important implication of this research, which is high-stakes testing. Let's be clear: this research doesn't suggest that there is no such thing as intelligence-- of course differences in intelligence exist! Instead, it highlights that if we want to more fully and accurately understand a person's intellectual potential we must look at their individual intelligence over time. This is critical because many gifted and talented programs base their admissions on the result of a single-shot testing session. Likewise, many important college decisions are based on the result of a one-shot standardized test. Ideally, we'd allow students take a test many times over a year and submit their aggregate result, and college admissions officers would also be on the lookout for conditions that may have depressed a child's true score.\n\nI asked Schmiedek what avenue of research excites him the most using the person-centered approach, and he told me he is excited to conduct more research that takes into account social and emotional factors and uses that information to design interventions that can help people improve cognitive functioning. This avenue of research is also very exciting to me, as I believe it highlights the importance of viewing individuals as whole people, with not just cognitive potentials, but also motivations and passions, personality traits, rich life experiences, and daily fluctuations in the lived stream of life.\n\nYes, it's possible to take a single trait-- say, IQ-- and compare people to each other treating all else equal. But within individuals, all else is assuredly not equal. Our levels of engagement affect our intellectual potential, as do our personal long-term dreams and goals. This is why in my 2013 book Ungifted: Intelligence Redefined, I presented a theory of Personal Intelligence, which I defined as \"the dynamic interplay of abilities and engagement in pursuit of personal goals.\"At the end of the day, what individuals care the most about is not how their overall intellectual functioning compares to others, but how they can maximize their own unique capacities in the service of realizing a desired future image of themselves.\n\nI'm truly excited by this new frontier in intelligence research because it will allow us the opportunity to capture the complexities of an individual's potential to a much greater degree than we ever have before, and maybe one day we can use that information-- not to limit possibility-- but to make sure we are bringing out the best in everyone.\n\n--\n\n* Take cognitive fatigue. Hans Sievertsen and his colleagues looked at standardized test data for literally every single child who attended Danish public schools between 2009 and 2013. This comprised 2 million tests taken! They found that the time of day of the testing significantly impacted test scores, with the impact being particularly strong for low-performing students. Additionally, a 20- to 30- minute break every hour substantially improved average test scores. They calculated that the breaks are worth about $1,900 higher household income, almost 2 months of parental education, or 19 school days. The authors conclude that \"cognitive fatigue should be taken into consideration when deciding on the length of the school days and the frequency and duration of breaks.\"","title":"Toward a New Frontier in Human Intelligence: The Person-Centered Approach","origin":"Beautiful Minds","image":"https://static.scientificamerican.com/blogs/cache/file/58BEFE42-429A-4CD1-805D353CDA62D0EF_source.jpg?w=590&h=800&9936361B-EA0C-404E-B964E3DD46C31F40","link":"https://blogs.scientificamerican.com/beautiful-minds/toward-a-new-frontier-in-human-intelligence-the-person-centered-approach/"},{"authors":"Annie Sneed","pub_date":"May 22, 2019","abstract":"A study found that only a small percentage of bird beak shape variation is dependent on diet, with other factors like display and nest construction probably playing parts too.","title":"Bird Beak Shapes Depend on More Than Diet","origin":"Evolution 60-Second ScienceSubscribe:Apple iTunesRSS","image":"https://static.scientificamerican.com/sciam/cache/file/34F800CE-6127-49F0-9373C3D03A1AF056_source.jpg?w=590&h=800&0B6A4764-AA8A-4A5D-82CB30B5F92C8429","link":"https://www.scientificamerican.com/podcast/episode/bird-beak-shapes-depend-on-more-than-diet/"},{"authors":"Brian Switek","pub_date":"May 15, 2019","abstract":"We all know the story by now. What was overwhelmingly bad for the dinosaurs was ultimately good for mammals, the mass extinction of 66 million years ago clearing the deck for the Age of Beasts to finally come to fruition. Mammals no longer had to suffer under the feet of the “terrible lizards” and could finally make the world their own.\n\nOf course the tale’s not quite that simple. In recent years, the idea that Mesozoic mammals were all shrew-like insectivores has been toppled by an array of squirrel, beaver, and even aardvark-like mammals that proliferated through the Age of Dinosaurs. Not to mention that the origins of many familiar mammal groups – like primates – seem to pre-date the terrible K/Pg disaster. The event was certainly important, but how?\n\nTo answer that question, paleontologists Gemma Louise Benevento, Roger Benson, and Matt Friedman looked at differences in mammal jaw shape through time. What they were assessing was a sense of disparity – the number of different shapes mammal jaws took through time. And given that these shapes are related to diet, the details of the jaw can track how mammals created new niches for themselves through evolutionary time.\n\nAfter analyzing the jaws of 256 ancient beasts from the days of the earliest mammals in the Triassic through the rise of dominant beasts in the Eocene, long after the catastrophe of 66 million years ago, Benevento and colleagues found that the evolutionary breathing room really did make a difference to our greater family. Despite evolving various body types and ecological niches, mammal jaws through the Triassic, Jurassic, and Cretaceous were relatively similar when it came to how they were using their jaws.\n\nThe big change occurred after the end-Cretaceous mass extinction. Mammals of the following time period, the Paleocene, show a slightly wider range of jaw shapes and expressed a greater degree of different jaw functions than their Mesozoic predecessors. The trend kept going through the Eocene, which started about 56 million years ago, as mammal evolution flourished. Not only were there early members of familiar groups by this time – like early bats, primates, and elephants – but also now-extinct groups like the carnivorous hyaenodonts, rhino-like brontotheres, and more.\n\nWhat emerges, then, is a mammalian mosaic. Previous studies documented how mammal body size changed dramatically right after the end-Cretaceous extinction. Mammals evolved into a wider range of body sizes than ever before. But jaw shape and function is a different story.\n\nViewed as a whole, Benevento and coauthors write, mammal jaw shape didn’t really pick up much disparity until about 10 million years after the end-Cretaceous crisis. But when the researchers looked at therian mammals alone – that is, the group that contains placental and marsupial mammals – they found that this subgroup quickly responded to the ecological shakeup by evolving more jaw functions. The extinction effectively allowed them to take over ecological space previously occupied by more archaic forms of mammals, or, as the researchers write, “We tentatively argue therefore that therians were the primary beneficiaries of the K/Pg extinction event.”\n\nThere's still more to uncover. Much of what we know about the beginning of the Age of Mammals comes from western North America. Fossils from elsewhere in the world will be critical to examining and testing ideas about what happened. For now, though, the story goes beyond the dinosaurs. It wasn’t just that dinosaurs were keeping mammals suppressed. At a smaller scale, archaic mammals were affecting the evolution of the therians. It took the extinction of these older mammal forms, as well as the dinosaurs, to give the therians a change to flourish. If that asteroid impact never happened, it wouldn’t have just translated into extended dinosaurian dominion. The shape of mammalian evolution would have been dramatically different.","title":"Fossil Jaws a Sign of When Mammals Bounced Back","origin":"Laelaps","image":"https://static.scientificamerican.com/blogs/cache/file/9E9C9660-33D8-4CC6-8CF4F77A4DDE9F09_source.jpg?w=590&h=800&5722040B-3C7F-4E95-9F4D72D1E2B51635","link":"https://blogs.scientificamerican.com/laelaps/fossil-jaws-a-sign-of-when-mammals-bounced-back/"},{"authors":"Rachel Scheer","pub_date":"May 22, 2019","abstract":"Plastic pollution, including tiny bits known as microplastic, permeate the environment, posing a threat to human and planetary health. “Solving the Plastic Waste Problem,” the third “Science on the Hill” event bringing insights about key issues to the halls of Congress, will address important considerations for dealing with this pressing problem, including how solutions can boost businesses and lead to job growth.\n\nThe expert panel includes Morton A. Barlaz, Distinguished University Professor and Head of the Department of Civil, Construction and Environmental Engineering at North Carolina State University; April Crow, Senior Advisor to Circulate Capital, supporting the launch of the impact-focused investment management firm dedicated to financing innovation, companies and infrastructure that prevent the flow of plastic waste into the world's ocean while advancing the circular economy; and Kara Lavender Law, Research Professor at Sea Education Association, studying the sources, distribution, behavior and fate of plastic debris in the ocean. Mariette DiChristina, editor in chief of Scientific American, will moderate the panel.\n\nScientific American and Nature Research—part of Springer Nature—host the “Science on the Hill” event series, in collaboration with Congressman Jerry McNerney (CA-09). The first event, “Energy Solutions for a Sustainable Future,” occurred November 15, 2017. The second, “AI, Robotics and Your Health,” took place on June 14, 2018.\n\nFor further exploration into these important topics, Scientific American’s In-depth Report How Plastic Became a Plague and Springer Nature’s  Grand Challenges feature highlight relevant content. Follow the hashtag #ScienceOnTheHill to be kept abreast on the latest updates on the event, including live-tweeting and Instagram stories.","title":"Science on the Hill: Solving the Plastic Waste Problem","origin":"@ScientificAmerican","image":"https://static.scientificamerican.com/blogs/cache/file/DE4CF816-1C09-4862-8BEAA3EEDD73F08E_source.jpg?w=590&h=800&539EB73E-753A-400E-A455A9C6B3B196DE","link":"https://blogs.scientificamerican.com/at-scientific-american/science-on-the-hill-solving-the-plastic-waste-problem/"},{"authors":"Ian Graber-Stiehl","pub_date":"May 24, 2019","abstract":"Amid the continuing decline of pollinators worldwide, U.S. lawmakers recently revived a perennially struggling bill that aims to save these helpful species. However, pollinator loss is more complicated than many headlines suggest. And curbing it, some scientists say, requires more than just stricter pesticide regulation—a major focus of the bill.\n\nThis is the fifth iteration of the Save America’s Pollinators Act, which was introduced by Democratic Representatives Earl Blumenauer of Oregon and John Conyers Jr. of Michigan in 2013 but has never been put to a vote. It was initially intended to address colony collapse disorder—the sudden disappearance of millions of honey bees from commercial hives that was first widely noted around 2006, and which peaked nearly a decade ago. The bill’s latest form calls for a joint initiative by the U.S. Department of Agriculture, Environmental Protection Agency and Department of the Interior to monitor wild bee species, including bumble and mason bees. It also proposes changes to the EPA’s pesticide review process—namely, establishing a board of beekeepers, scientists and farmers who would review research on pesticides’ effects on pollinators before the chemicals could be approved for use on agricultural plants.\n\nThe bill—which is currently with a subcommittee that will decide if and when to move it toward a vote before the House of Representatives—could spur work to fill in gaps in scientists’ understanding of the health of both honeybee and wild bee species. But it appears unlikely to address the paucity of data on environmental threats to other wild pollinators—such as moths, butterflies, beetles, flies, wasps and bats.\n\nThe bill’s revival also raises a question that tends to make headlines at least once a year: Is there really a “beepocalypse,” or have honeybees recovered?\n\nThe collapse of commercial honeybee hives is definitely still an issue, according to Christina Grozinger, an entomology professor at the Pennsylvania State University Center for Pollinator Research. The USDA estimates 1.5 million hives were lost in the U.S. from January 2017 to January 2018. Grozinger says beekeepers are simply propagating new hives—splitting large hives into two, and ordering new queens to start new ones entirely—fast enough to keep pace with the losses.\n\nParticularly troubling to entomologist Dennis vanEngelsdorp, project director for the nonprofit Bee Informed Partnership (BIP), is that since 2006, summer losses have approached those occurring in winter—which had long been the deadliest time of year for bees because of the cold weather, minimal honey reserves and parasites. “Colonies are [now] dying at a constant rate, all year round,” VanEngelsdorp says. Honey bees are not in danger of extinction by a long shot, says University of Illinois entomologist May Berenbaum—“but beekeeping, as an industry, is still under threat.”\n\nCanary in the Beehive?\n\nHoneybees are often looked to as proxies for the status of pollinators as a whole, but that concept seems imperfect. Data from the largest federal survey on domestic honeybee mortality only date back to 2015; data on wild bees are far less comprehensive, and those on other wild pollinators are scarcer still. The factors that threaten honeybees also frequently differ from those threatening wild bees and other wild pollinators. For example, BIP beekeepers consistently cite parasitic varroa mites—which entomologists frequently list as honeybees’ greatest stressor—as a top cause of hive mortality. But these mites do not infect wild bumblebees or mason bees.\n\nPesticides, the chief issue that the Save America’s Pollinators Act hopes to address, appear to affect honeybees, wild bees and other wild pollinators all quite differently. Most research covering pesticides’ effects on pollinators has focused on those of specific chemicals (particularly on a type called neonicotinoids) on individual honeybees. In response to these pesticides, bees routinely develop impaired immune systems and become more susceptible to pathogens such as Nosema and deformed wing virus. But field studies, which have many more variables, suggest neonicotinoids have far milder effects on honeybees.\n\nUnlike wasps, bumblebees or solitary bee species, honeybees live in large hives—and these massive, malleable complexes can buffer them against stressors, Berenbaum says. As a colony, they share one “stomach”: a communal reserve of nectar and pollen harvested from kilometers around, from a mix of agricultural, wild, pesticide-treated and untreated flowers. Although every bee may be exposed to pesticides, the dose is diluted by nectar or pollen samples from millions of other flowers.\n\nIn contrast, bumblebees have fewer workers, smaller foraging ranges and less diverse food reserves. They also nest in the ground, where some pesticides can persist for months to years. Around treated areas, this translates to higher doses of neonicotinoids and fungicides—compounds that have both been shown to compromise bumblebee nesting behavior and reduce the production of both new workers and queens. Solitary bees live under similar, albeit more extreme, conditions than bumblebees, depending on the nectar they can individually gather from nearby areas. “You can lose 30 percent of the bees in a [honeybee] colony, and not have a huge effect on that colony’s health or productivity. However, if you kill solitary bees … you not only kill that [individual] but all her subsequent offspring,” vanEngelsdorp says.\n\nThe Save America’s Pollinators Act would collect more data on the health of wild bees—which would be a major asset to scientists, Berenbaum says. The act’s proposed pesticides review board would also generally consider native bees and other wild pollinators more in regulating the chemicals. \n\nMany species of bees make wax or food stores that can be tested for pesticide exposure, and the insects return to nests, making them easy to track. But other wild pollinators, such as moths, butterflies, wasps, flies and beetles, do not reliably return to a central nest or produce material that can be easily tested. As a result, there is comparatively little research on how neonicotinoids affect these insects. Some critics believe the proposed legislation would be unlikely to remedy the imbalance between the troves of research on honeybees and the comparatively scarce data on wild pollinators as a whole. And realistically, when the pesticide review board evaluates how chemicals affect pollinators, it will have to base many of its conclusions on research done on honey bees.\n\n“The Saving America’s Pollinators Act was developed in consultation with scientists, beekeepers and the environmental community to combat one of the most prominent threats to pollinator health,” Blumenauer says. “I’m always open to discussing improvements to this bill as it moves through the legislative process.”\n\nBut Berenbaum says the legislation will not fix problems she thinks far outstrip neonicotinoids, varroa and disease—namely climate change, habitat loss and the diminishing diversity of forage that pollinators have access to, as fields and native plants are replaced by monocultures such as lawns and agricultural areas. “There’s a natural human tendency to focus on what’s immediately fixable,” she says. But the more existential issues of the American lifestyle—that is something a single bill cannot address.","title":"New Law Would Help Bees—but Could Leave Other Pollinators out in the Cold","origin":"Conservation","image":"https://static.scientificamerican.com/sciam/cache/file/2C8DC03F-B3F9-47C3-BFE5E6B18B19627C_source.jpg?w=590&h=800&E2039096-B557-49FC-99D156DB4AFB92EB","link":"https://www.scientificamerican.com/article/new-law-would-help-bees-but-could-leave-other-pollinators-out-in-the-cold/"},{"authors":"Christopher Intagliata","pub_date":"May 23, 2019","abstract":"A new study suggests women's performance on math and verbal tasks increases as room temperature rises, up to about the mid 70s F. Christopher Intagliata reports.","title":"Icy Room Temperatures May Chill Productivity","origin":"Cognition 60-Second ScienceSubscribe:Apple iTunesRSS","image":"https://static.scientificamerican.com/sciam/cache/file/398435AE-B4B9-428D-A7027098302B262B_source.jpg?w=590&h=800&9369BA9A-12DC-421C-AD8995684507F8B8","link":"https://www.scientificamerican.com/podcast/episode/icy-room-temperatures-may-chill-productivity/"},{"authors":"Meera Subramanian, Nature magazine","pub_date":"May 22, 2019","abstract":"An influential panel of scientists voted this week to designate a new geologic epoch—the Anthropocene—to mark the profound ways that humans have altered the planet. That decision, by the 34-member Anthropocene Working Group (AWG), marks an important step toward formally defining a new slice of the geologic record, which has generated intense debate within the scientific community over the past few years.\n\nThe panel plans to submit a formal proposal for the new epoch by 2021 to the International Commission on Stratigraphy, which oversees the official geological time chart.\n\nTwenty-nine members of the Anthropocene Working Group (AWG) supported the Anthropocene designation and they also voted in favour of starting the new epoch in the mid-20th century, when a rapidly rising human population accelerated the pace of industrial production, use of agricultural chemicals and other human activities. At the same time, the first atomic bomb blasts littered the globe with radioactive debris that became embedded in sediments and glacial ice, becoming part of the geologic record.\n\n“The Anthropocene works as a geological unit of time, process and strata,” says Jan Zalasiewicz, chair of the AWG, who wasn’t confident of that conclusion when the AWG first began its work a decade ago. But the vote demonstrates that the group has mostly coalesced around the geological unit. “It is distinguishable. It is distinctive,” he says.\n\nThe results of the vote were not surprising, given that they solidified an informal vote that was taken in 2016 at the International Geological Congress in Cape Town, but the decision invigorates the race to find a geologic marker signalling the end of the Holocene epoch and the start of one shaped by human activity.\n\nThe AWG was established by the Subcommission on Quaternary Stratigraphy, which in turn is part of the International Commission on Stratigraphy. Having decided to go ahead with the new epoch, the group will now focus on identifying a definitive geologic marker or “golden spike”, which is technically called a Global boundary Stratotype Section and Point (GSSP).\n\nThe group is considering ten candidate sites from around the globe, from a cave in northern Italy to corals in the Great Barrier Reef to a lake in China. Next week, many of the scientists involved will gather in Berlin to coordinate the next two years of research. They hope to identify a single site to include in their formal proposal. They must also define the type of physical evidence in the sedimentary record that represents the start of the epoch. The group is considering whether to choose the radionuclides that came from atomic bomb detonations from 1945 until the Limited Nuclear Test Ban Treaty of 1963, says Zalasiewicz.\n\nOnce the AWG makes its formal proposal, it would be considered by several more layers of the International Commission on Stratigraphy. And if it makes it past that group, final ratification would come from the executive committee of the International Union of Geological Sciences.\n\nFour members of the AWG voted against the idea of designating the Anthropocene as a new epoch. They objected to the group’s efforts to find a single clear signal that can be found globally in the geological record, as opposed to acknowledging the progressive impacts of humans on the world, starting with agriculture in prehistoric times. “The stratigraphic evidence overwhelmingly indicates a time-transgressive Anthropocene with multiple beginnings rather than a single moment of origin,” says Matt Edgeworth, an archaeologist at the University of Leicester and a member of the AWG. Naming a new epoch based on the radionuclide signal alone, he says, “impedes rather than facilitates scientific understanding of human involvement in Earth system change.”\n\nThis article is reproduced with permission and was first published on May 21, 2019.","title":"Anthropocene Now: Influential Panel Votes to Recognize Earth’s New Epoch","origin":"Policy & Ethics","image":"https://static.scientificamerican.com/sciam/cache/file/A09985EE-9D17-4F05-A41FE0517FFC02CB_source.jpg?w=590&h=800&766DB0A7-16B0-4FA3-8296E9DFB5FC7DD4","link":"https://www.scientificamerican.com/article/anthropocene-now-influential-panel-votes-to-recognize-earths-new-epoch/"},{"authors":"Caleb A. Scharf","pub_date":"May 21, 2019","abstract":"Mars is a dusty place. The first time humans got a probe into orbit around the planet in 1971 - the Mariner 9 mission - it had to wait weeks for a global dust storm to die down before the surface could be seen.\n\nMore recently the solar powered rovers Spirit and Opportunity not only had their light collecting capacity reduced by a coating of martian dust, they also ended up relying on the natural cleaning effects of the dust devils that whirl across Mars's surface. \n\nNASA's Insight lander, situated at low northerly latitudes has also experienced a build-up of dust on its solar panels. We can see this clearly in a pair of 'before and after' selfie-images.\n\nThis one was taken December 6th 2018:\n\n\n\tCredit: NASA, JPL-Caltech\n\n\nAnd this one taken on May 6th 2019:\n\n\n\tCredit: NASA, JPL-Caltech\n\n\nAlthough this coating has reduced the power captured by the lander by about 30%, that's still well within operating requirements. But naturally the mission controllers and engineers would be happier for a little natural dust-cleaning to take place. \n\nWhile the lander doesn't seem to have had a very powerful dust-lifting devil pass across it yet, there has been evidence of some pickup in local winds measured by Insight's instruments and possibly a gentler bit of dust devil sweeping. Some of that increase in winds and pressure drops seem to have correlated with a few percent increase in power generation, suggesting the removal of a small amount of dust from the solar panels. \n\nMars has passed its northern Spring Equinox, heading for summer. While northern summer on Mars is not as hot or disruptive as the southern summer, it may well be that Insight gets its hoped for spring cleaning.","title":"Spring Cleaning on Mars","origin":"Life, Unbounded","image":"https://static.scientificamerican.com/blogs/cache/file/1C02B7AD-87DD-40D4-A963DB48C703BAD1_source.jpg?w=590&h=800&0B76EBAE-A9A1-4F1F-9D1BB1E076AE9F6F","link":"https://blogs.scientificamerican.com/life-unbounded/spring-cleaning-on-mars/"},{"authors":"Imre Bartos, Szabolcs Márka","pub_date":"May 23, 2019","abstract":"The first detection of a neutron-star collision took 60 years after Margaret Burbidge and colleagues realized that the heaviest elements in the universe must be created in violent cosmic explosions. Gravitational waves observed from the collision by LIGO and Virgo in 2017 led to a worldwide hunt for the cosmic flash produced by the neutron star debris. The findings elevated neutron stars to be leading candidates to create Burbidge’s elements.\n\nSome of the elements from Burbidge’s process are radioactive; they steadily decay after the initial explosion that created them. These elements act as omnipresent cosmic clocks that carry information about their origin. We found that such clocks, present in the solar system, point to a neutron star collision that occurred a mere 1,000 light-years from what is today the solar system, a distance that is 1 percent the size of the Milky Way. This single event is responsible for the creation of 0.3 percent of some of the heaviest elements found on Earth today, including gold, uranium and iodine.\n\nMatter deposited by this single event is ubiquitous. Every human has an eyelash worth of matter in them from this collision, mostly in the form of iodine. A gold wedding ring contains 10 milligrams. A Tesla Model 3 has five grams. A nuclear reactor has 200 kilograms. Of the gold ever refined by humanity, 600 tons are from this single collision.\n\nNeutron stars are ultracompact dead stars, formed when massive stars collapse under their own gravitational pull. They weigh as much as our sun, but their size is that of New York City. They are in effect a gigantic atomic nucleus, made up primarily of neutrons, hence their name.\n\nScientists long suspected that neutron stars occasionally collide with each other in the universe, even though definite proof was elusive. Freeman Dyson suggested in the early 1960s that two neutron stars may be closely orbiting each other; he was interested in the extraction of energy from such a system in humanity’s distant future. Russell Hulse and Joe Taylor were the first to detect two close-by neutron stars, in the 1970s. These neutron stars are set to collide in 300 million years; their distance is continuously decreasing as a result of the emission of gravitational waves.\n\nThe first hint of neutron-star collisions came as a serendipitous side effect of the cold war. The United States launched a network of military satellites, called Vela, to monitor the Soviet Union’s compliance with a nuclear test ban treaty during the 1960s. Instead of rogue detonations on Earth, the satellites detected mysterious signals from outer space. Some of these cosmic flashes of energetic photons, called gamma rays, later turned out to be telltale signs of colliding neutron stars billions of light-years away.\n\nIn 2017, the LIGO detectors in the U.S. and the Virgo detector in Europe observed a gravitational wave signal that was identified as a neutron star collision 130 million light-years from Earth, proving that such collisions do happen in the universe. This was a distant event, but the fact that LIGO and Virgo detected it after monitoring the sky for only about four months meant that neutron stars regularly collide. In our Milky Way galaxy, there is one collision every 100,000 years. If we had millions of years to wait, we would eventually encounter a collision by chance not too far from our solar system.\n\nWhen and where neutron stars collide has particular importance for the birth of the solar system. Before the sun and the planets were formed, matter that eventually made them was in the form of a primordial cloud made of gas and dust. This cloud was many times greater than the solar system today, casting a giant cosmic net that collected atoms traveling between stars in the so-called interstellar medium. Some of the matter accumulated in this presolar cloud came from neutron star collisions, including most of the gold, platinum, uranium and other heavy elements we find on Earth today.\n\nTo reconstruct what happened, we can turn to radioactive isotopes that were produced by neutron stars and were deposited in the presolar cloud. While many of these isotopes have long since decayed, their presence in the early solar system is recorded in meteorites that were formed soon after the presolar cloud collapsed and began to form the sun and the planets.\n\nThe information encoded in radioactive isotopes is revealing. We found that the relative quantities of several short-lived isotopes with half-lives less than 100 million years is typical if they originate from neutron stars, but they are inconsistent with another kind of cosmic explosion: a supernova, produced by massive dying stars. Supernova explosions are 1,000 times more frequent in the Milky Way than neutron star collisions.\n\nThis means that the shortest-lived isotopes would be regularly replenished if produced by supernovae, making them certain to be present at the time of the solar system’s formation. For the rarer neutron star collisions, the shortest-lived isotopes are depleted soon after a merger, and stay depleted until the next. This means that it is probable that at the time of the solar system’s formation, this isotope will be depleted. The observed abundances of the short-lived curium-247 and iodine-129 isotopes in the early solar system show this depletion, ruling out supernovae.\n\nAt the time of Burbidge’s original work and in the decades that followed, supernovae were considered to be the main candidates for sources of the heaviest elements. This paradigm has been challenged by multiple lines of evidence, including the optical signal observed from the neutron star collision detected by LIGO and Virgo, and our work.\n\nRadioactive isotopes also point to a collision near the presolar cloud. Comparing the measured quantities to numerical simulations of the Milky Way, we found that a single collision likely produced a substantial fraction of the short-lived elements of the early solar system. For example, curium, whose longest-lived isotope has a half-life of 15.6 million years, was mainly produced by a single collision.\n\nElements that come from neutron stars play important roles in our lives. They include valuable metals, such as gold and platinum, or elements essential to making modern electronic devices. Some of them, such as iodine, are critical to life itself. Had the rate of collisions been different, or had Earth been at a different part of the Milky Way, the abundance of neutron-star matter on Earth, and with it our environment, may have been very different.\n\nWith the advent of new types of observatories, such as those capable of detecting gravitational waves or neutrinos, our exploration of the universe increasingly relies on combining information from different cosmic messengers. In this new field called multimessenger astrophysics, radioactive elements found in the solar system are a valuable addition. There is much to learn about the most extreme cosmic events and how they shaped our environment here on Earth.","title":"Where (Some of) Earth’s Gold Came From","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/8BD4678F-7E9B-473E-A6FFB6F9794B7DD5_source.jpg?w=590&h=800&1A09DB92-AD9B-4A56-89B3129599B79406","link":"https://blogs.scientificamerican.com/observations/where-some-of-earths-gold-came-from/"},{"authors":"Bernardo Kastrup","pub_date":"May 24, 2019","abstract":"One of the weirdest theoretical implications of quantum mechanics is that different observers can give different—though equally valid—accounts of the same sequence of events. As highlighted by physicist Carlo Rovelli in his relational quantum mechanics (RQM), this means that there should be no absolute, observer-independent physical quantities. All physical quantities—the whole physical universe—must be relative to the observer. The notion that we all share the same physical environment must, therefore, be an illusion.\n\nSuch a counterintuitive prediction—which seems to flirt dangerously with solipsism—has been clamoring for experimental verification for decades. But only recently has technology advanced far enough to allow for it. So now, at last, Massimiliano Proietti and collaborators at Heriot-Watt University, in the U.K., seem to have confirmed RQM; as predicted by quantum mechanics, there may well be no objective physical world.\n\nYet, our perceptions of the world beyond ourselves are quite consistent across observers: if you were to sit next to me right now, we would describe my study in very similar, mutually consistent ways. Clearly, observers must share an environment of some sort, even if such an environment is not physical—i.e., not describable by physical quantities.\n\nPossible solutions to this dilemma have been proposed. For instance, writing for this magazine last year, I maintained that physical quantities describe merely our perceptions and are, therefore, relative to each of us as observers. What is really out there, underlying our perceptions, is constituted not by physical but by transpersonal mental states instead. Perceived physicality is merely a representation of that surrounding mental environment, brought into being by an act of observation.\n\nThis isn’t a new view. In fact, it is very old. For instance, already in the early 19th century, Arthur Schopenhauer argued that the physical world of discrete objects in spacetime is merely a subjective representation in the mind of an observer. What is really out there is what Schopenhauer called the “Will”: transpersonal mental states with a volitional character, which transcend our ability to sense or measure. It is the volitional character of these states that explains the universe’s evolution according to causal chains; the universe moves and changes because it is compelled to do so by the patterns of its own underlying willing.\n\nDespite the objections one might have to Schopenhauer’s ideas, they do seem to make sense of RQM’s counterintuitive predictions: physics was developed to describe perceptual states alone, not endogenous mental states such as volition. For this reason, physical descriptions are always observer-dependent; they don’t capture the world as it is in itself, but merely how it presents itself to each of us, given our respective point of view within the environment. Make no mistake: there still is a common environment of transpersonal volitional states, in which we are all immersed; it’s just that this environment is not what physics directly describes.\n\nMaking sense of RQM by inferring that our surrounding environment is essentially mental—a view called “objective idealism”—avoids solipsism. However, it carries with it a seemingly difficult problem: if what is really out there are transpersonal volitional states, then why do seeing or hearing feel so different from desiring or fearing? If my perceptions represent underlying states akin to desire and fear, why do I see forms and colors instead?\n\nIf only we could provide a compelling rationale for this qualitative transition, we would be able to leverage objective idealism to make sense of RQM and the latest experimental results. But can we? As it turns out, we very well can; even in more ways than one.\n\nOver the past several years, Donald Hoffman’s group at the University of California, Irvine, has shown that our perceptual apparatus hasn’t evolved to represent the world truthfully, as it is in itself; if we saw the world as it really is, we would be swiftly driven to extinction. Instead, we see the world in a way that favors our survival, not the accuracy of our representations. In Hoffman’s analogy, the contents of perception are like icons on a computer desktop: a set of visual metaphors that facilitate one’s job by illustrating the salient properties of files and applications, but which don’t portray these files and application as they really are.\n\nApproaching the problem from a different angle, Karl Friston and collaborators have shown that, if an organism is to represent the states of the external environment in order to properly navigate this environment, it must to so in an encoded, inferential manner. The reason is that, if the organism were to simply mirror the states of the external environment in its own internal states, it would not be able to maintain its structural integrity; its internal states would become too dispersed and the organism would dissolve into an entropic soup. Perceptual encoding is necessary for the organism to resist entropy and thus remain alive.\n\nWhat both of these lines of argument suggest is this: the screen of perception is much more akin to a dashboard than a window into the environment. It conveys relevant information about the environment in an indirect, encoded manner that helps us survive. The forms and colors we see, the sounds we hear, the flavors we taste are all like dials: they present to us, at a glance, information that correlates—in a manner fundamentally beyond our ability to cognize—with the mental states of the environment out there.\n\nInstead of having to feel the myriad mental states surrounding us—which would be akin to how a telepath would feel overwhelmed and disoriented in the middle of an agitated crowd—we encode them neatly in the pixels of the screen of perception.\n\nEvolution has provided each of us with a dashboard of dials that inform us about the environment we live in. But we don’t have a window to look directly at what is out there; all we have are the dials. The error we make is in mistaking the dials for the external environment itself.\n\nPhysics models and predicts the behavior of the dials. Although we are all immersed in a common environment, each of us interacts with it in a different way, from a different perspective. Therefore, we each gather different information about the environment, and so our respective dials may not always agree. This doesn’t mean that there is no common environment; it means only that this environment isn’t physical.\n\nFor as long as we insist that the world, as it is in itself, must have the forms and contours of the images on the screen of perception, quantum mechanics will continue to be paradoxical. For as long as we believe that physical theory models the shared environment underlying our perceptions—as opposed to the perceptions themselves—quantum mechanics will continue to be puzzling. As discussed much more extensively in my latest book, there is only one reasonable way out: to regard our perceptions as a dashboard of dials providing salient, though indirect, information about a mental universe out there.","title":"The Universe as Cosmic Dashboard","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/2236C5C9-39BD-4DD5-A80B1D6AC115594F_source.jpg?w=590&h=800&10F42076-970B-4151-BC3ECDFDD8FBD41B","link":"https://blogs.scientificamerican.com/observations/the-universe-as-cosmic-dashboard/"},{"authors":"Chelsea Harvey, E&E News","pub_date":"May 24, 2019","abstract":"The annual return of the Asian monsoons is one of nature’s great cycles of renewal. Each summer, the onset of the wet season brings much-needed rain to millions of people across the continent.\n\nBut scientists have noticed a puzzling trend in recent decades. Some of the monsoons, including the annual rains in India and parts of China, seem to be weakening over time, raising concerns about the long-term effects on water supplies and agriculture.\n\nIt’s the exact opposite of what should be expected in a warming world.\n\nOn a basic level, the monsoons are caused by a difference in temperature between the air over the Asian continent versus the surrounding oceans. This gradient changes with the seasons, and allows for an influx of warm, moist air to blow across the land during the hotter summer months. That brings stormy weather. As the climate warms, land masses are expected to heat up faster than surrounding waters, strengthening the monsoon season in the process.\n\nBut some are getting weaker, and now scientists think they know why.\n\nResearch increasingly suggests that rising air pollution in parts of Asia is powerful enough to alter the weather—sometimes in ways that work against the influence of global warming.\n\nThe result is a kind of tug of war between greenhouse gases and pollution particles in the atmosphere. And for now, when it comes to the annual monsoons, pollution seems to be winning.\n\nA recent study, published last month in the journal Geophysical Research Letters, is the latest to highlight the phenomenon. It focuses on the Asian summer monsoon, which brings annual rains to large regions of China.\n\nThe researchers, led by Yu Liu of the Chinese Academy of Sciences, compiled a large collection of data from tree rings in northern China. They revealed 448 years of the region’s climate history. Tree ring samples contain a variety of information about the conditions that occurred over the course of a tree’s life.\n\nThe tree rings showed that the Asian summer monsoon has been decreasing in strength for the past 80 years—the most substantial weakening observed in the entire 450-year record.\n\nThe researchers then used a series of climate model simulations to determine the causes. The models indicate that the growing influence of aerosols—tiny pollution particles in the atmosphere—are likely to blame. Without the air pollution, the models suggest, the strength of the monsoon should be growing as the climate warms.\n\nThe major mechanism likely involves the cooling influence of certain types of aerosols, particularly sulfate particles, according to study co-author Steven Leavitt of the University of Arizona.\n\n“The sulfate particles turn out to be quite reflective to sunlight,” he noted in an email to E&E News—meaning they beam sunlight away from the Earth, causing a cooling effect on the local climate. This cooling effect works against the influence of climate change, dampening the warming that’s occurring over the continent and causing the monsoon to weaken.\n\nThe research reaffirms an idea that many other studies have also suggested, according to climate and monsoon system expert Andrew Turner of the University of Reading in the United Kingdom.\n\n“Modelling studies consistently show that aerosol emissions over Asia, and in particular sulphate aerosol emissions, lead to reduced monsoon rainfall,” he noted in an email, adding that the effect isn’t limited to northern China.\n\nMultiple studies have pointed to a similar phenomenon affecting the South Asian summer monsoon, as well, which brings rain to millions of people across the Indian subcontinent. Observations suggest that the South Asian monsoon has also weakened in recent decades. And as with the rains in northern China, models indicate that aerosols are likely a major factor.\n\nThat’s not to say there aren’t remaining mysteries about the behavior of the Asian monsoons. The East Asian monsoon has exhibited some perplexing trends in recent years, according to atmospheric physicist Yi Ming of NOAA’s Geophysical Fluid Dynamics Laboratory.\n\nSome of Ming’s own modeling studies have suggested that the influence of aerosols over the Asian continent should be driving a drying trend over southern China, similar to northern China and the Indian subcontinent. But real-life observations show that it’s actually growing wetter, he said in an interview. Scientists are still working to understand the discrepancy, but many believe that certain natural climate variations may be part of the reason.\n\nOn the whole, though, research increasingly points to the acute influence of air pollution on monsoon systems affecting broad swaths of the Asian continent.\n\nA global phenomenon\n\nScientists are becoming more aware of the profound ways in which air pollution can affect global weather and climate patterns, and the ways in which they may work with or against the influence of climate change.\n\nDifferent types of pollutants may have different effects in the atmosphere—black carbon particles, for instance, can actually absorb heat and increase climate warming. But the cooling influence of particles like sulfates has proven among the most globally significant effects so far.\n\nRecent studies suggest that air pollution may be masking some of the influence of climate change, so to speak, and that the climate would be significantly warmer if it didn’t exist. One particularly jarring 2018 study estimated that eliminating all human aerosol emissions could cause the planet to warm by as much as an additional half to 1 degree Celsius.\n\nOther studies have suggested that changes in air pollution have had significant effects on other kinds of climate patterns—not just monsoons—in various parts of the world. One recent study, published earlier this month in Nature, found the likely signature of aerosols in a century of global drought records.\n\nAlso relying on tree ring data, the study found that the influence of human-caused global warming on droughts around the world has been clear for at least 100 years. But its influence temporarily declines for a few decades, starting around 1950. The researchers suggest that an increase in global air pollution during this time was probably counteracting some of the effects of climate change.\n\nOn a smaller scale, research suggests that some air pollution particles may also be able to affect the weather by altering the formation of clouds. That’s even more complicated. Depending on the types of particles and their concentrations in the air, studies have found that pollution can sometimes enhance rainfall and sometimes suppress it.\n\nStill, the effect on clouds has had some globally significant consequences. One oft-cited 2014 paper, for instance, found that pollution from Asia may be strengthening storms in the Pacific Ocean—including weather systems that eventually make it all the way to North America—largely by altering the formation and structure of clouds.\n\nOn the other hand, some of the weakening trend in the Asian monsoons may actually be linked to the opposite process, according to Wenju Cai of Australia’s Commonwealth Scientific and Industrial Research Organisation, one of the new paper’s co-authors. Some types of aerosol particles over the continent may actually work to decrease the size of water droplets in the storm clouds, which he notes is “not conducive to rainfall.”\n\nThat’s one of the complicated things about air pollution—there’s a wide variety of different particles released into the atmosphere, and they don’t all behave in the same ways. That makes modeling their effects on already complicated aspects of the Earth system, like weather and climate, a big challenge.\n\nWhat may be an equally important question is what will happen to global weather and climate patterns if air pollution goes away.\n\nDespite the variety of different particles and their behaviors, it’s clear that the climate-cooling properties of some of the most common aerosols remain among the most significant global consequences of air pollution around the world. Currently, that effect is working at odds with the progression of global warming.\n\nBut aerosols only last a short time in the atmosphere compared with carbon dioxide and other greenhouse gases, and they disappear quickly when emissions are halted. Some scientists have pointed out that as efforts to clean up the air become more successful, it could be followed by rapid climate warming. And that extra warming could bring about a spate of unforeseen changes in regional weather patterns, as well.\n\nRecent patterns in Atlantic hurricanes may hold some hints about the potential future of the Asian monsoons, Ming suggested.\n\nWhile climate change is expected to increase the strength of tropical cyclones, some research has also suggested that recent reductions in air pollution from North America and Europe may have removed some of the cooling influence of aerosols over the Atlantic Ocean. This cooling effect may have been suppressing storms during much of the 20th century, and recent clean-up efforts could partly explain why hurricanes seem to be growing stronger over the last few decades.\n\n“In that sense, this is also like an anecdote, or somewhat preview, of what’s to come over East Asia,” Ming suggested.\n\nWidespread concern about air quality across Asia, particularly China and India, may eventually result in similarly successful efforts to reduce pollution. When that happens, the weakening trend in the monsoons may begin to reverse itself, particularly as the climate continues to warm.\n\nThat might seem like a good thing for water availability across the continent. But there are two sides to every coin, Ming noted. Monsoon season is often associated with sudden severe storms, as opposed to just continuous rainfall. If the monsoons strengthen, there may be an increase in flooding and storm-related damage.\n\nResearchers have frequently emphasized that it’s important to improve air quality despite the potential climate side effects. But they would like to project the outcomes on warming before they occur, so there’s time to prepare. And that starts with science aimed at understanding how air pollution is affecting the climate system now, and in what ways it’s working with or against the continued progression of climate change.\n\n“If our past understanding is right, that means you will have this unmasking of the aerosol effect—so that means everything will be reversed,” Ming said of the monsoons. “So that is not a tug of war anymore between aerosols and greenhouse gases. They will all be working in concert toward the same direction.”\n\nReprinted from Climatewire with permission from E&E News. E&E provides daily coverage of essential energy and environmental news atwww.eenews.net.","title":"“Tug-of-War” between Air Pollution and CO2 Masks Warming’s Impacts","origin":"Environment","image":"https://static.scientificamerican.com/sciam/cache/file/EA12E3CC-C863-4D6B-B5D39115B24F48C1_source.jpg?w=590&h=800&6434D5E8-6A8C-41C1-80F79218E6736DB3","link":"https://www.scientificamerican.com/article/tug-of-war-between-air-pollution-and-co2-masks-warmings-impacts/"},{"authors":"Steven Strogatz, Steve Mirsky","pub_date":"May 23, 2019","abstract":"Cornell University applied mathematics professor Steven Strogatz talks about his new book Infinite Powers: How Calculus Reveals the Secrets of the Universe.","title":"Secrets of the Universe Revealed!","origin":"Math Science TalkSubscribe:Apple iTunesRSS","image":"https://static.scientificamerican.com/sciam/cache/file/0F2332CB-08D8-40C5-90F5472406AD10CE_source.jpg?w=590&h=800&A0B752C0-F786-4EAA-A6F3FBE487A91F7E","link":"https://www.scientificamerican.com/podcast/episode/secrets-of-the-universe-revealed/"},{"authors":"Jennifer Frazer","pub_date":"May 23, 2019","abstract":"In the late 1990s, word began to spread of something paleontologists had long searched for: a fossil bed more tantalizingly close to the beginning of animal life than any yet seen.\n\nUnearthed by phosphate miners in southern China, the microfossils of the Doushantuo formation just predate -- or are the earliest known members of -- the Ediacaran biota, the first bizarre animal life. The Ediacaran itself predates the 550 million-year-old Cambrian Explosion when most modern animal groups emerged.\n\nIn this bed were what appeared to be animal embryos.\n\n20 years and reams of papers later, we still don’t really know what the heck they were. A 2017 review in the Journal of the Geological Society concluded that although they have many animal-like characteristics, none of these features are exclusive to animals. Debate continues.\n\nOther possible Doushantuo finds reviewed by the paper include the first sponge and possible red algae. But nothing except their beauty and marine origin are definitive.\n\nHere are a few:\n\n\n\tCredit: Cunningham et al. 2017\n\n\nThese fossils bear an unmistakable resemblance to animal embryos. Although the authors of the review agree it is likely the embryo-like fossils represent a single group of organisms, and their characteristics – an ornate envelope, Y-shaped cell junctions, and a particular form of cell division called palintomic -- don’t rule out an animal origin, they also don’t definitively support it.\n\n\n\tCredit: Cunningham et al. 2017\n\n\nSome have suggested these stacks of coin-like objects are now-extinct tabulate coral. But corals abandon old chambers, and all of the cells in these forms appear occupied by some sort of biological structures. The authors conclude there’s no good evidence that they were animals of any kind.\n\n\n\tCredit: Cunningham et al. 2017\n\n\nG, H, and I all remain mysterious. Some have suggested I is the embryo of a cnidarian, a group that includes coral and jellyfish.\n\nJ is a suspected red alga, but the authors feel its non-flattened shape seems at odds with that idea (most algae have flattened blades like leaves to maximize the illumination of their cells). K is an acritarch, the frequently spiny biological equivalent of a UFO. L is a suspected sponge. But other than looking like one, scientists have been unable to find any characteristics in the fossil that are definitively sponge-worthy.\n\nThe sculpted, exquisitely precise forms of all these fossils belie their frustrating taxonomic ambiguity. After 600 million years in the ground patiently waiting to be found, they are still waiting to be conclusively identified.\n\nReference\n\nCunningham, John A., Kelly Vargas, Zongjun Yin, Stefan Bengtson, and Philip CJ Donoghue. \"The Weng'an Biota (Doushantuo Formation): an Ediacaran window on soft-bodied and multicellular microorganisms.\" Journal of the Geological Society 174, no. 5 (2017): 793-802.","title":"What the Heck Are These Fossils?","origin":"The Artful Amoeba","image":"https://static.scientificamerican.com/blogs/cache/file/C99EFF21-90FF-46C5-B6749DEFD718A948_source.jpg?w=590&h=800&84B2097E-FDDF-4554-B5C7FF208CFC8061","link":"https://blogs.scientificamerican.com/artful-amoeba/what-the-heck-are-these-fossils/"},{"authors":"Ian Graber-Stiehl","pub_date":"May 24, 2019","abstract":"Amid the continuing decline of pollinators worldwide, U.S. lawmakers recently revived a perennially struggling bill that aims to save these helpful species. However, pollinator loss is more complicated than many headlines suggest. And curbing it, some scientists say, requires more than just stricter pesticide regulation—a major focus of the bill.\n\nThis is the fifth iteration of the Save America’s Pollinators Act, which was introduced by Democratic Representatives Earl Blumenauer of Oregon and John Conyers Jr. of Michigan in 2013 but has never been put to a vote. It was initially intended to address colony collapse disorder—the sudden disappearance of millions of honey bees from commercial hives that was first widely noted around 2006, and which peaked nearly a decade ago. The bill’s latest form calls for a joint initiative by the U.S. Department of Agriculture, Environmental Protection Agency and Department of the Interior to monitor wild bee species, including bumble and mason bees. It also proposes changes to the EPA’s pesticide review process—namely, establishing a board of beekeepers, scientists and farmers who would review research on pesticides’ effects on pollinators before the chemicals could be approved for use on agricultural plants.\n\nThe bill—which is currently with a subcommittee that will decide if and when to move it toward a vote before the House of Representatives—could spur work to fill in gaps in scientists’ understanding of the health of both honeybee and wild bee species. But it appears unlikely to address the paucity of data on environmental threats to other wild pollinators—such as moths, butterflies, beetles, flies, wasps and bats.\n\nThe bill’s revival also raises a question that tends to make headlines at least once a year: Is there really a “beepocalypse,” or have honeybees recovered?\n\nThe collapse of commercial honeybee hives is definitely still an issue, according to Christina Grozinger, an entomology professor at the Pennsylvania State University Center for Pollinator Research. The USDA estimates 1.5 million hives were lost in the U.S. from January 2017 to January 2018. Grozinger says beekeepers are simply propagating new hives—splitting large hives into two, and ordering new queens to start new ones entirely—fast enough to keep pace with the losses.\n\nParticularly troubling to entomologist Dennis vanEngelsdorp, project director for the nonprofit Bee Informed Partnership (BIP), is that since 2006, summer losses have approached those occurring in winter—which had long been the deadliest time of year for bees because of the cold weather, minimal honey reserves and parasites. “Colonies are [now] dying at a constant rate, all year round,” VanEngelsdorp says. Honey bees are not in danger of extinction by a long shot, says University of Illinois entomologist May Berenbaum—“but beekeeping, as an industry, is still under threat.”\n\nCanary in the Beehive?\n\nHoneybees are often looked to as proxies for the status of pollinators as a whole, but that concept seems imperfect. Data from the largest federal survey on domestic honeybee mortality only date back to 2015; data on wild bees are far less comprehensive, and those on other wild pollinators are scarcer still. The factors that threaten honeybees also frequently differ from those threatening wild bees and other wild pollinators. For example, BIP beekeepers consistently cite parasitic varroa mites—which entomologists frequently list as honeybees’ greatest stressor—as a top cause of hive mortality. But these mites do not infect wild bumblebees or mason bees.\n\nPesticides, the chief issue that the Save America’s Pollinators Act hopes to address, appear to affect honeybees, wild bees and other wild pollinators all quite differently. Most research covering pesticides’ effects on pollinators has focused on those of specific chemicals (particularly on a type called neonicotinoids) on individual honeybees. In response to these pesticides, bees routinely develop impaired immune systems and become more susceptible to pathogens such as Nosema and deformed wing virus. But field studies, which have many more variables, suggest neonicotinoids have far milder effects on honeybees.\n\nUnlike wasps, bumblebees or solitary bee species, honeybees live in large hives—and these massive, malleable complexes can buffer them against stressors, Berenbaum says. As a colony, they share one “stomach”: a communal reserve of nectar and pollen harvested from kilometers around, from a mix of agricultural, wild, pesticide-treated and untreated flowers. Although every bee may be exposed to pesticides, the dose is diluted by nectar or pollen samples from millions of other flowers.\n\nIn contrast, bumblebees have fewer workers, smaller foraging ranges and less diverse food reserves. They also nest in the ground, where some pesticides can persist for months to years. Around treated areas, this translates to higher doses of neonicotinoids and fungicides—compounds that have both been shown to compromise bumblebee nesting behavior and reduce the production of both new workers and queens. Solitary bees live under similar, albeit more extreme, conditions than bumblebees, depending on the nectar they can individually gather from nearby areas. “You can lose 30 percent of the bees in a [honeybee] colony, and not have a huge effect on that colony’s health or productivity. However, if you kill solitary bees … you not only kill that [individual] but all her subsequent offspring,” vanEngelsdorp says.\n\nThe Save America’s Pollinators Act would collect more data on the health of wild bees—which would be a major asset to scientists, Berenbaum says. The act’s proposed pesticides review board would also generally consider native bees and other wild pollinators more in regulating the chemicals. \n\nMany species of bees make wax or food stores that can be tested for pesticide exposure, and the insects return to nests, making them easy to track. But other wild pollinators, such as moths, butterflies, wasps, flies and beetles, do not reliably return to a central nest or produce material that can be easily tested. As a result, there is comparatively little research on how neonicotinoids affect these insects. Some critics believe the proposed legislation would be unlikely to remedy the imbalance between the troves of research on honeybees and the comparatively scarce data on wild pollinators as a whole. And realistically, when the pesticide review board evaluates how chemicals affect pollinators, it will have to base many of its conclusions on research done on honey bees.\n\n“The Saving America’s Pollinators Act was developed in consultation with scientists, beekeepers and the environmental community to combat one of the most prominent threats to pollinator health,” Blumenauer says. “I’m always open to discussing improvements to this bill as it moves through the legislative process.”\n\nBut Berenbaum says the legislation will not fix problems she thinks far outstrip neonicotinoids, varroa and disease—namely climate change, habitat loss and the diminishing diversity of forage that pollinators have access to, as fields and native plants are replaced by monocultures such as lawns and agricultural areas. “There’s a natural human tendency to focus on what’s immediately fixable,” she says. But the more existential issues of the American lifestyle—that is something a single bill cannot address.","title":"New Law Would Help Bees—but Could Leave Other Pollinators Out in the Cold","origin":"Conservation","image":"https://static.scientificamerican.com/sciam/cache/file/2C8DC03F-B3F9-47C3-BFE5E6B18B19627C_source.jpg?w=590&h=800&2E27ACA5-C67E-4C55-A0B5F111C47435AC","link":"https://www.scientificamerican.com/article/new-law-would-help-bees-but-could-leave-other-pollinators-out-in-the-cold/"},{"authors":"Daniel B. Poneman","pub_date":"May 24, 2019","abstract":"Sixty-five years ago, President Eisenhower took the first concrete steps toward implementing his “Atoms for Peace” initiative, presenting Soviet leaders with a detailed outline of the safety and nonproliferation rules that should guide the peaceful development of civilian nuclear energy.\n\nThree more years of determined U.S.-led diplomacy culminated in the establishment of the International Atomic Energy Agency, which continues to be pivotal in maintaining, monitoring and enforcing global nonproliferation safeguards—so that, in Ike’s words, “this greatest of destructive forces can be developed into a great boon, for the benefit of all mankind.”\n\nThe existential threat of nuclear annihilation did not go away when the Cold War ended, and now we face a second existential threat from climate change. In the face of these twin threats, American nuclear leadership is as critical in 2019 as it was in 1954. \n\nNuclear energy is the largest source of carbon-free energy in the U.S. by a huge margin and it has a major role to play in confronting the global climate challenge. But we must also be vigilant about the prospect of nuclear weapons falling into the hands of terrorists or rogue regimes. \n\nThe threat of nuclear proliferation abroad should not lead us to abandon nuclear energy at home. Indeed, American nuclear leadership has always been critical to guiding the safe, responsible use of civilian nuclear energy around the world. \n\nFor example, a number of American companies are developing advanced generation-reactor technologies that offer a host of safety and nonproliferation advantages. These advanced designs would have “walk away” safety, meaning they do not need any backup power or external cooling systems in the event of an accident. And since many of the new reactor designs would rarely if ever need to be refueled, the risk of diversion of fuel from uranium-enrichment or plutonium-reprocessing plants to a bomb program would be greatly diminished. \n\nThe U.S. should lead the way in the development of these reactors so they can be deployed at home and abroad over the next decade. As a growing number of countries around the world turn to nuclear power as a source of carbon-free electricity, it is strongly in our interest that they do so with safe, American-made technology. Countries that adopt the new U.S. reactor designs will also be subject to U.S. nonproliferation requirements, which are second to none. \n\nWe must also confront the challenge posed by countries like North Korea, which has nuclear weapons, and Iran, which has sought to develop them. There is no substitute for tough diplomacy, backed by a unified international community willing to exercise its leverage—through sanctions or ultimately military means, if necessary—to persuade these nations to give up their weapons in a transparent and verifiable way. Here again, America’s technical expertise in building, operating and fueling reactors informs and strengthens our ability to design enforceable nonproliferation agreements and effective verification measures to detect and respond to violations. \n\nAmerican leadership in nuclear technologies is equally important when it comes to the climate challenge. It has been three years since the Paris Climate Agreement and the world is already falling far short of its collective commitments to reduce carbon emissions. Even if all nations achieved 100 percent of the reductions they pledged in Paris, the world would not come anywhere near the goal of limiting temperature rise to 2 degrees Celsius over preindustrial levels, much less the 1.5-degree target that scientists say we must achieve if we are, for example, to save the earth’s coral reefs. Projected increases in renewable power and plans to invest in carbon-capture technologies, efficiency measures, reforestation and other steps are important but will not get us there.\n\nThat is why the International Energy Agency has concluded that meeting the goal of 2 degrees C will require doubling nuclear power’s contribution to global energy consumption by mid-century. Late last year the Intergovernmental Panel on Climate Change reached a similar conclusion: in most scenarios consistent with the target of 1.5 degrees C, nuclear energy would have to more than double.\n\nIn the end, we do not need statistical models to know that nuclear must be on the table. Common sense tells us that if we are facing an existential crisis, every available zero-carbon technology must be called into service. Scaling the mountain in front of us is daunting enough without tying one hand behind our back. \n\nThe 98 reactors in our nuclear fleet are the workhorse of the clean-energy sector. They provide one fifth of our electricity. Unfortunately, over the past few years six reactors have been prematurely shut down, and another 12 are set to close in the next seven years.\n\nThe problem is that the rules governing wholesale electricity markets do not allow the unique advantages of nuclear power to be reflected in the wholesale price, effectively putting new and existing nuclear plants at a disadvantage. These rules were written decades ago to deliver some things we want (low prices and excess capacity to meet spikes in demand) but not other things we want (clean air, lower carbon emissions and grid reliability). \n\nNuclear plants are not only emissions-free and carbon-free, they are by far the most reliable assets in our power generation mix, operating 93 percent of the time—even during extreme weather events when some fossil fuel plants may be forced to shut down or curtail their operations. Under current rules, electricity markets are not allowed to value these attributes, even though they are clearly valuable. \n\nRepublicans and Democrats in states like Illinois, New York and New Jersey have taken action to establish “zero-emission credits” so the markets better reflect the value of carbon-free energy like nuclear and renewables. But state solutions are an imperfect substitute for what should be federal, nationwide action to reform these markets. \n\nPreserving existing reactors may not sound exciting, but it is a critical first step if we take the climate challenge seriously. Consider that for every reactor that prematurely shuts down, our carbon dioxide emissions rise by about 5.8 million metric tons per year. According to the Environmental Protection Agency’s Greenhouse Equivalencies Calculator, that equals the emissions from burning more than 648 million gallons of gasoline—the equivalent of filling up an NFL stadium with gasoline and setting it on fire. To offset those carbon emissions, we would need to plant over 95 million trees. Or we could install solar panels on one million homes and figure out a cost-effective way of storing the electricity so it is available day and night. \n\nBut that is just to break even and does not move us past the starting line. Instead of swapping one source of zero-emission power for another, wouldn’t it be better to combine all available sources of low- and no-carbon energy to maximize our emissions reductions? The only way to do that is through a public-private partnership. \n\nThis kind of partnership can succeed only over a sustained period, which requires a strong foundation of support across a broad political spectrum. The good news about nuclear energy is that those who care about climate change may support it on environmental grounds while those who care about U.S. global influence may support it for other reasons. Remember that apart from generating power to light homes, drive industrial manufacturing and reduce carbon emissions, the U.S. needs a robust nuclear industry to support its national security. This includes building, operating, sustaining and fueling our aircraft carriers and fast-attack and nuclear-armed submarines. \n\nIn the 1950s, Admiral Hyman Rickover’s redoubtable efforts to establish a nuclear navy led directly to a commercial nuclear power industry in the U.S., beginning with the Shippingport reactor in 1957. Today the Pentagon’s need for reliable power can help drive demand for nuclear energy and defray its costs. \n\nIt is telling that despite the polarized politics of the day, two bills promoting U.S. leadership in nuclear energy passed Congress last year and were signed into law (the Nuclear Energy Innovation Capabilities Act, or NEICA, and the Nuclear Energy Innovation and Modernization Act, or NEIMA). \n\nTogether, public and private partners can drive a new generation of smaller, cheaper, safer nuclear reactors that satisfy the world’s growing energy demands while lowering carbon emissions and reducing proliferation risks.","title":"We Can't Solve Climate Change Without Nuclear Power","origin":"Observations","image":"https://static.scientificamerican.com/blogs/cache/file/CE426778-71A5-4F6F-853ED61D9AF572C1_source.jpg?w=590&h=800&D5C61D8E-7D81-47B6-B96D5203D5735994","link":"https://blogs.scientificamerican.com/observations/we-cant-solve-climate-change-without-nuclear-power/"}]